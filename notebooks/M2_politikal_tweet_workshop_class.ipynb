{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q7bAAUtH8G0"
      },
      "outputs": [],
      "source": [
        "# Install the tweet-preprocessor library (used for cleaning and preprocessing tweets)\n",
        "!pip install tweet-preprocessor -q\n",
        "\n",
        "# Install the latest version of gensim (a library for topic modeling and document similarity analysis)\n",
        "!pip install -q -U gensim\n",
        "\n",
        "# Install pyLDAvis (a Python library for interactive topic model visualization)\n",
        "!pip install -q pyLDAvis\n",
        "\n",
        "# Force reinstall numpy to version 1.22.4. This is currently necessary to ensure compatibility or fix certain issues.\n",
        "!pip install --force-reinstall -q numpy==1.22.4\n",
        "\n",
        "# Install LIME (Local Interpretable Model-Agnostic Explanations), a library for explaining machine learning model predictions\n",
        "!pip install -q lime\n",
        "\n",
        "# Install or update the imbalanced-learn library, useful for dealing with imbalanced datasets\n",
        "!pip install -U imbalanced-learn -q # needs specific numpy. Can use to artificially make more customers from a low count group, and hence can be trained more accurately on the stuff we need\n",
        "\n",
        "!pip install gradio -U -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbDJSVD3H0Fm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import preprocessor as prepro # twitter prepro\n",
        "from tqdm.notebook import tqdm #progress bar # since or things are now gonna take longer, or if we're stuck in somethign or if it's still working\n",
        "\n",
        "import spacy #spacy for quick language prepro # Very good if you need to use language processing fast, not so much for neutral networks, hence dying a bit also probably.\n",
        "nlp = spacy.load('en_core_web_sm') #instantiating English module\n",
        "\n",
        "# sampling, splitting\n",
        "from imblearn.under_sampling import RandomUnderSampler # need to bring it to the same distribution\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# loading ML libraries\n",
        "from sklearn.pipeline import make_pipeline #pipeline creation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
        "from sklearn.linear_model import LogisticRegression #Logit model\n",
        "from sklearn.metrics import classification_report #that's self explanatory\n",
        "from sklearn.decomposition import TruncatedSVD #dimensionality reduction # dimensionality reduction well suited for sparce matrix\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import altair as alt #viz\n",
        "\n",
        "#explainability\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from collections import OrderedDict\n",
        "\n",
        "# topic modeling\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary # Import the dictionary builder\n",
        "from gensim.models import LdaMulticore # we'll use the faster multicore version of LDA\n",
        "\n",
        "# Import pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv5R8X2ZO2eH"
      },
      "outputs": [],
      "source": [
        "# getting rid of annoying warnings from ipykernel\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning) # because warnings are annoying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsQWaTU41ksj"
      },
      "outputs": [],
      "source": [
        "# prepro settings\n",
        "prepro.set_options(prepro.OPT.URL, prepro.OPT.NUMBER, prepro.OPT.RESERVED, prepro.OPT.MENTION) # twitter preprocessor and remove all these.\n",
        "# Could argue if you're losing information, but whatever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3-rBov_TJuV"
      },
      "outputs": [],
      "source": [
        "# open file\n",
        "data_pol = pd.read_json('https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pol_tweets.gz')\n",
        "data_tw = pd.read_json('https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pres_debate_2020.gz')\n",
        "# just care about the performance evaluation, we don't care about the causality of what is happening. Which you would care a lot about in statistics and colinearity and so on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo3mwRfAg-ky"
      },
      "outputs": [],
      "source": [
        "data_pol.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_Wj4PrZON4f"
      },
      "outputs": [],
      "source": [
        "data_pol['text'] = data_pol.text.str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BKBKbcFJ0m1"
      },
      "outputs": [],
      "source": [
        "data_tw.tweet.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97iCgpL4Mnyl"
      },
      "outputs": [],
      "source": [
        "data_tw = data_tw.tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvBJQomEMrtI"
      },
      "outputs": [],
      "source": [
        "data_tw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUhjywn2gBz0"
      },
      "outputs": [],
      "source": [
        "#basic cleanup only for tweets\n",
        "data_pol['text_clean'] = data_pol['text'].map(lambda t: prepro.clean(t)) # Anonymus functions, .map (expose them 1 by 1) and call them t lambda(don't have to define), prepro.clean(t)\n",
        "# can also do it with if and so on, + append but it's easier.\n",
        "data_pol['text_clean'] = data_pol['text_clean'].str.replace('#','') # str has a lot of features. String manipulations in panadas.\n",
        "data_pol['text_clean'] = data_pol['text_clean'].str.replace('rt','') # prepro.clean might've made it all same size, but otherwise do one for upper or lower caes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxk5G49lNQTW"
      },
      "outputs": [],
      "source": [
        "data_pol.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijwe4ks92cTn"
      },
      "outputs": [],
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm(total=len(data_pol['text_clean']),position=0, leave=True) # make a progessbar\n",
        "\n",
        "for text in nlp.pipe(data_pol['text_clean'], disable=[\"tagger\", \"parser\", \"ner\"]): # putting all our text into the loop, and skip tagger parser and ner since they're computationally intense\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text # lowercase lematized tokens\n",
        "         if token # if it's characters (and ont the ones below)\n",
        "         and not token.is_stop # don't want it to be stop words\n",
        "         and not token.is_punct] # and don't want it to be dots\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb8RI568dKqj"
      },
      "outputs": [],
      "source": [
        "def text_prepro(texts: pd.Series) -> list: # called typing, and then if something breaks, and this ensures that we need a certain type of data coming in and out\n",
        "    \"\"\"\n",
        "    Preprocess a series of texts.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: A pandas Series containing the text to be preprocessed.\n",
        "    - nlp: A spaCy NLP model.\n",
        "\n",
        "    Returns:\n",
        "    - A list of preprocessed texts.\n",
        "\n",
        "    Steps:\n",
        "    - Clean twitter-specific characters using a predefined 'prepro' method.\n",
        "    - Normalize the text by lowercasing and lemmatizing.\n",
        "    - Remove punctuations, stopwords, and non-alphabet characters.\n",
        "    \"\"\"\n",
        "# nlp can be used for if we have different languages\n",
        "    # Clean twitter-specific characters and other special characters\n",
        "\n",
        "    texts_cleaned = texts.str.lower()\n",
        "    texts_cleaned = texts.map(prepro.clean).str.replace('#', '')\n",
        "    texts_cleaned = texts_cleaned.str.replace('#', '')\n",
        "\n",
        "    # Initialize container for the cleaned texts\n",
        "    clean_container = []\n",
        "\n",
        "    # Use tqdm for a progress bar\n",
        "    pbar = tqdm(total=len(texts_cleaned), position=0, leave=True)\n",
        "\n",
        "    # Use spaCy's nlp.pipe for efficient text processing\n",
        "    for doc in nlp.pipe(texts_cleaned, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "        # Extract lemmatized tokens that are not punctuations, stopwords, or non-alphabetic\n",
        "        tokens = [token.lemma_.lower() for token in doc\n",
        "                  if token and not token.is_stop and not token.is_punct]\n",
        "\n",
        "        clean_container.append(\" \".join(tokens))\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    return clean_container\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJwvhVSBQZnu"
      },
      "outputs": [],
      "source": [
        "data_pol['text_clean'] = text_prepro(data_pol[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkpAIkYnQwD9"
      },
      "outputs": [],
      "source": [
        "data_df = pd.DataFrame({'label':data_pol['labels'], \"text\":data_pol[\"text_clean\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4MeMSlcRcPe"
      },
      "outputs": [],
      "source": [
        "data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMbJ9vQ-RRfs"
      },
      "outputs": [],
      "source": [
        "data_df.label.value_counts().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "objzDumxRbwM"
      },
      "outputs": [],
      "source": [
        "# Count and reset index\n",
        "data_chart = data_df.label.value_counts().reset_index().rename(columns={'index': 'Category', 'label': 'N Tweets'})\n",
        "\n",
        "# Replace numerical categories with textual descriptions\n",
        "data_chart['Category'] = data_chart['Category'].map({0: 'repuplican', 1: 'democrat'}) # basically just look at this for the groupings\n",
        "\n",
        "# Plot the chart\n",
        "chart = alt.Chart(data_chart).mark_bar(filled=True).encode(\n",
        "    alt.X('N Tweets:Q', title='N Tweets'),\n",
        "    alt.Y('Category:O', title='Category', sort='-x'),\n",
        "    color=alt.Color('Category:N', legend=alt.Legend(title=\"Label Types\"), scale=alt.Scale(\n",
        "        domain=['repuplican', 'democrat'],\n",
        "        range=['red', 'green']\n",
        "    ))\n",
        ")\n",
        "\n",
        "chart # highly imbalanced dataset, will be hard to teach the model to recognice and separate hatespeech from offensive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCnYdY4ASGUz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51Miz9_kpref"
      },
      "outputs": [],
      "source": [
        "# fixing sample imbalance\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "data_df_res, y_res = rus.fit_resample(data_df, data_df['label']) # for numerical data you can use better models. Making fake data would work better (for the different classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoJRCOfn-YuU"
      },
      "outputs": [],
      "source": [
        "data_df_res['label'].value_counts() # still enough to train a model # losing a bunch of data, but proabably getting better results in the end # 20 and 80% are on the edge, but usually it's fine even if it's 40-60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myvcNFNQf4cA"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into the Training set and Test set (since we have a new output variable) # can also use an eval set around 10% and only in the end we use test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_df_res['text'], y_res, test_size = 0.4, random_state = 42) # best learning material is basically just the documentation (in relation to the imbalanced dataset but also other things we do)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKC9cONAPCq3"
      },
      "outputs": [],
      "source": [
        "#instantiate models and \"bundle up as pipeline\"\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "svd = TruncatedSVD(n_components = 100) # squeeze the factors into about 100 factors\n",
        "cls_xg = XGBClassifier()\n",
        "\n",
        "pipe_xg = make_pipeline(tfidf, svd, cls_xg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7H4C77rPatZ"
      },
      "outputs": [],
      "source": [
        "pipe_xg.fit(X_train,y_train) # fit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6nRa0zkeY0Y"
      },
      "outputs": [],
      "source": [
        "# pipe_xg.fit(X_train,y_train) # fit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alBRiy43eYtW"
      },
      "outputs": [],
      "source": [
        "# evaluate model performance on training set\n",
        "\n",
        "y_eval = pipe_xg.predict(X_train)\n",
        "report = classification_report(y_train, y_eval)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTI_wpLWzYVm"
      },
      "outputs": [],
      "source": [
        "# evaluate model performance on training set\n",
        "\n",
        "y_eval = pipe_xg.predict(X_test)\n",
        "report = classification_report(y_test, y_eval)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0BWsX3taEFc"
      },
      "outputs": [],
      "source": [
        "# prepro.clean2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FynW1KqaBku"
      },
      "outputs": [],
      "source": [
        "cleaned_twitter_data = data_tw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYWUNTSch3KI"
      },
      "outputs": [],
      "source": [
        "# predict\n",
        "programmed_tw = text_prepro(data_tw)\n",
        "new_tw = pipe_xg.predict(programmed_tw) # make with xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8T-ELhJokps"
      },
      "outputs": [],
      "source": [
        "data_tw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFzwS3oUQmJq"
      },
      "outputs": [],
      "source": [
        "# Let's explain the result:\n",
        "\n",
        "class_names = [\"repuplican\", \"democrat\"]\n",
        "\n",
        "explainer = LimeTextExplainer(class_names = class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtB2B_yRQwUF"
      },
      "outputs": [],
      "source": [
        "exp = explainer.explain_instance(programmed_tw[3], pipe_xg.predict_proba, num_features = 10, top_labels=3) # technically we skipped the preprocessing. How many words does the model need to explain = num_features = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAkpiRdhWKwJ"
      },
      "outputs": [],
      "source": [
        "exp.show_in_notebook(text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ63iO0WWEBR"
      },
      "outputs": [],
      "source": [
        "exp = explainer.explain_instance(data_tw[3], pipe_xg.predict_proba, num_features = 10, top_labels=3) # technically we skipped the preprocessing. How many words does the model need to explain = num_features = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTJw0TF4QxBG"
      },
      "outputs": [],
      "source": [
        "exp.show_in_notebook(text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_CkOGEypqM2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWPtkKuQodlP"
      },
      "outputs": [],
      "source": [
        "cleaned_twitter_data2 = pd.DataFrame(cleaned_twitter_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3yo1DdsnmD_"
      },
      "outputs": [],
      "source": [
        "\n",
        "cleaned_twitter_data2 [\"label\"] = new_tw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAOe6m7Vpexs"
      },
      "outputs": [],
      "source": [
        "cleaned_twitter_data2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_5XKibSpuH3"
      },
      "outputs": [],
      "source": [
        "tw_demo = cleaned_twitter_data2[cleaned_twitter_data2['label']== 1]\n",
        "tw_repu = cleaned_twitter_data2[cleaned_twitter_data2['label']== 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogSyOLhdqDgW"
      },
      "outputs": [],
      "source": [
        "print(tw_demo.head())\n",
        "print(tw_repu.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJbZG8WsSdpr"
      },
      "outputs": [],
      "source": [
        "# preprocess texts (we need tokens)\n",
        "tokensrepu = []\n",
        "\n",
        "for text in nlp.pipe(tw_repu['tweet'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in text\n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV']\n",
        "              and not token.is_stop\n",
        "              and not token.is_punct]\n",
        "  tokensrepu.append(proj_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZuqqjamqj2m"
      },
      "outputs": [],
      "source": [
        "# preprocess texts (we need tokens)\n",
        "tokensdemo = []\n",
        "\n",
        "for text in nlp.pipe(tw_demo['tweet'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in text\n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV']\n",
        "              and not token.is_stop\n",
        "              and not token.is_punct]\n",
        "  tokensdemo.append(proj_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmeyfcwDa7i1"
      },
      "outputs": [],
      "source": [
        "tw_repu['tokens'] = tokensrepu\n",
        "tw_demo['tokens'] = tokensdemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxBNVSFwgrKs"
      },
      "outputs": [],
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionaryr = Dictionary(tw_repu['tokens']) # from gensin (look earlier)\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionaryr.filter_extremes(no_below=5, no_above=0.3, keep_n=1000) # no_above, so don't keep anything that occours more than 50% and don't keep anyhting that happens less than 5\n",
        "# construct corpus using this dictionary\n",
        "corpusr = [dictionary.doc2bow(doc) for doc in tw_repu['tokens']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR-I1RThhBSd"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "lda_modelr = LdaMulticore(corpusr, id2word=dictionaryr, num_topics=10, workers = 4, passes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9Qn8sdZrkul"
      },
      "outputs": [],
      "source": [
        "lda_modelr.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVT8N92GsBvu"
      },
      "outputs": [],
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionaryd = Dictionary(tw_demo['tokens']) # from gensin (look earlier)\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionaryd.filter_extremes(no_below=5, no_above=0.3, keep_n=1000) # no_above, so don't keep anything that occours more than 50% and don't keep anyhting that happens less than 5\n",
        "# construct corpus using this dictionary\n",
        "corpusd = [dictionaryd.doc2bow(doc) for doc in tw_demo['tokens']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iIT-GbisBvw"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "lda_modeld = LdaMulticore(corpusd, id2word=dictionaryd, num_topics=10, workers = 4, passes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVkAoHKisBvw"
      },
      "outputs": [],
      "source": [
        "lda_modeld.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81KTmWw7hCqQ"
      },
      "outputs": [],
      "source": [
        "# Let's try to visualize\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfGSNcH5hl_S"
      },
      "outputs": [],
      "source": [
        " # Let's Visualize\n",
        "pyLDAvis.display(lda_display) # left is basically a PCA map size represents how many documents are in tha topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gAT5aDeiSwJ"
      },
      "outputs": [],
      "source": [
        "def text_prepro2(texts: pd.Series) -> list: # called typing, and then if something breaks, and this ensures that we need a certain type of data coming in and out\n",
        "    \"\"\"\n",
        "    Preprocess a series of texts.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: A pandas Series containing the text to be preprocessed.\n",
        "    - nlp: A spaCy NLP model.\n",
        "\n",
        "    Returns:\n",
        "    - A list of preprocessed texts.\n",
        "\n",
        "    Steps:\n",
        "    - Clean twitter-specific characters using a predefined 'prepro' method.\n",
        "    - Normalize the text by lowercasing and lemmatizing.\n",
        "    - Remove punctuations, stopwords, and non-alphabet characters.\n",
        "    \"\"\"\n",
        "# nlp can be used for if we have different languages\n",
        "    # Clean twitter-specific characters and other special characters\n",
        "\n",
        "    texts_cleaned = texts.str.lower()\n",
        "    texts_cleaned = texts.map(prepro.clean).str.replace('#', '')\n",
        "    texts_cleaned = texts_cleaned.str.replace('#', '')\n",
        "\n",
        "    # Initialize container for the cleaned texts\n",
        "    clean_container = []\n",
        "\n",
        "\n",
        "    # Use spaCy's nlp.pipe for efficient text processing\n",
        "    for doc in nlp.pipe(texts_cleaned, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "        # Extract lemmatized tokens that are not punctuations, stopwords, or non-alphabetic\n",
        "        tokens = [token.lemma_.lower() for token in doc\n",
        "                  if token and not token.is_stop and not token.is_punct]\n",
        "\n",
        "        clean_container.append(\" \".join(tokens))\n",
        "\n",
        "\n",
        "\n",
        "    return clean_container\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhdK5wMCfNs4"
      },
      "outputs": [],
      "source": [
        "def predictpolitical(placetext):\n",
        "  text_rdy = []\n",
        "  text_rdy = text_prepro2(pd.Series(placetext))\n",
        "  result = pipe_xg.predict(text_rdy) # can also use predict pobability\n",
        "  result2= pipe_xg.predict_proba(text_rdy)\n",
        "  if result == 1:\n",
        "    return \"Democrat, \" \"probability = \" + str(result2[0][1].round(2))\n",
        "  if result == 0:\n",
        "    return \"Republican \" \"probability = \" + str(result2[0][0].round(2))\n",
        "\n",
        "\n",
        "predictpolitical(\"Hello you little person\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYZVfE6Yevla"
      },
      "outputs": [],
      "source": [
        "# Create a Gradio interface with custom names for categorical values and radio for all features\n",
        "interface = gr.Interface(                  # call the interface of gradio\n",
        "    fn=predictpolitical,                       # define the function it should use\n",
        "    inputs=[gr.Textbox(label = \"Insert text to find any political lean\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Find political lean\",\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
