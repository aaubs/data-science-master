{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M3_3_LanceDB_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://lancedb.github.io/lancedb/assets/ecosystem-illustration.png)"
      ],
      "metadata": {
        "id": "zTzN-m4_hUo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LanceDB is an open-source database for vector-search built with persistent storage, which greatly simplifies retrieval, filtering and management of embeddings."
      ],
      "metadata": {
        "id": "zx241F9hhr5V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGzZTq1bhSfM",
        "outputId": "29d80ae8-f429-4332-fa18-919c588f727a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.9/111.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install lancedb --q\n",
        "!pip install pypdf --q\n",
        "# !pip install -qqq chromadb==0.4.10 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lancedb\n",
        "\n",
        "uri = \"/content/data/sample-lancedb\"\n",
        "db = lancedb.connect(uri)"
      ],
      "metadata": {
        "id": "3QPenRzrhX99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = db.create_table(\"my_table\",\n",
        "                        data=[{\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n",
        "                              {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0}])"
      ],
      "metadata": {
        "id": "QFr9jZuUhgCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = table.search([100, 100]).limit(2).to_list()"
      ],
      "metadata": {
        "id": "MFjYLUYjhh9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJeCOKwdhik1",
        "outputId": "00193c74-e48a-4080-96bb-9addb24cae4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'vector': [5.900000095367432, 26.5],\n",
              "  'item': 'bar',\n",
              "  'price': 20.0,\n",
              "  '_distance': 14257.0595703125},\n",
              " {'vector': [3.0999999046325684, 4.099999904632568],\n",
              "  'item': 'foo',\n",
              "  'price': 10.0,\n",
              "  '_distance': 18586.421875}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a Vector Database for Documents"
      ],
      "metadata": {
        "id": "EwMy0VxPGog7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGbb32o1xArw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ef2040-8213-48b2-c367-22fbf38d4e63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "loader = PyPDFLoader(\"/content/attention.pdf\")\n",
        "\n",
        "docs = loader.load()\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF1OOWyZyiy4"
      },
      "source": [
        "The Markdown file we're loading is the original Attention paper: \"Attention is all you need!\". Let's see how we can use the RecursiveCharacterTextSplitter to split the document into smaller chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc_ER7Nyx7FG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca7d0e2-6572-4ec3-d064-c68373dfc85a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEP3dr5Jyk6K"
      },
      "source": [
        "Splitting the document into chunks is required due to the limited number of tokens a LLM can look at once (4096 for Llama 2). Next, we'll use the HuggingFaceEmbeddings class to create embeddings for the chunks:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lancedb\n",
        "from langchain.vectorstores import LanceDB\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "uri = \"/content/data/paper-lancedb-\"\n",
        "db = lancedb.connect(uri)"
      ],
      "metadata": {
        "id": "DFL6v_CVDFUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use HuggingFace embeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "Y90jw_mLB9R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = db.create_table(\n",
        "    \"paper_table\",\n",
        "    data=[\n",
        "        {\n",
        "            \"vector\": embeddings.embed_query(\"Hello World\"),\n",
        "            \"text\": \"Hello World\",\n",
        "            \"id\": \"1\",\n",
        "        }\n",
        "    ],\n",
        "    mode=\"overwrite\",\n",
        ")\n",
        "\n",
        "docsearch = LanceDB.from_documents(texts[5:20], embeddings, connection=table)"
      ],
      "metadata": {
        "id": "5WDEsH5wFDAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = docsearch.as_retriever(search_kwargs={'k': 2})"
      ],
      "metadata": {
        "id": "_ydqZeToFjsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvcMnV02EuLZ",
        "outputId": "be60d27f-7299-4d43-8f10-0ec68d566835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['LanceDB', 'HuggingFaceEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.lancedb.LanceDB object at 0x7c85e66c0190>, search_type='similarity', search_kwargs={'k': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "A3bjGnjjFQUl",
        "outputId": "824092b6-dfd0-431d-d2fd-a820cbf6aa64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = table.search(embeddings.embed_query(texts[0].page_content)).limit(2).to_list()"
      ],
      "metadata": {
        "id": "f0aEnSm1FNlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnRSARRhFlyh",
        "outputId": "67346efb-4aa4-461a-eaf9-6695fc224408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['vector', 'text', 'id', '_distance'])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Create a LanceDB for Two Papers and Load Each into a Table"
      ],
      "metadata": {
        "id": "0j9bhHa8HUHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChromaDB"
      ],
      "metadata": {
        "id": "l8Ro7fPmHLGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://images.datacamp.com/image/upload/v1693482377/image4_7b6910cd7c.png)"
      ],
      "metadata": {
        "id": "PZo8JjuJGkkT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0BeHxeGypah"
      },
      "source": [
        "In the spirit of using free tools, we're also using free embeddings hosted by HuggingFace. We'll use Chroma database to store/cache the embeddings and make it easy to search them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPi0DWfzBaJ"
      },
      "source": [
        "To combine the LLM with the database, we'll use the RetrievalQA chain:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq chromadb==0.4.10 --progress-bar off"
      ],
      "metadata": {
        "id": "vGOZDRrwF5fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9Zjb7JiymgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6443f85c-d736-40fc-daf7-915e892cc510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.2 Model Variations\n",
            "To evaluate the importance of different components of the Transformer, we varied our base model\n",
            "in different ways, measuring the change in performance on English-to-German translation on the\n",
            "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n",
        "results = db.similarity_search(\"Transformer models\", k=2)\n",
        "print(results[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Hzk8WlF28c",
        "outputId": "614fbe92-5e64-4921-c809-da99f883ab2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'page': 7, 'source': '/content/attention.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Create a ChromaDB for Two Papers and Load Each into a collection"
      ],
      "metadata": {
        "id": "y0BUI-hhKM1N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a8ZB-o3AJ64J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}