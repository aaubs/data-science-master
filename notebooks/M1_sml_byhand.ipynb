{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMYcec9snSWA7rb567xYxIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M1_sml_byhand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH0HrrG_fpaC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Digression: SML by hand\n",
        "\n",
        "Well, we might not do it anymore after this exercise, but still it might be worth to review how to code up a simple SML model by hand."
      ],
      "metadata": {
        "id": "RPA_IO-dO0rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression. by hand\n",
        "Based on adapted example from [here](https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy)."
      ],
      "metadata": {
        "id": "OSWkzbfMQvH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "3 # Generate random dataset\n",
        "from sklearn import datasets\n",
        "X, y = datasets.make_regression(\n",
        "        n_samples=500, n_features=1, noise=15, random_state=4)"
      ],
      "metadata": {
        "id": "SWCiy6ZOd4zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reminder: The general equation for a linear line is:\n",
        "\n",
        "y = b0 + b1*X\n",
        "\n",
        "* X is numeric, single-valued. Here b1 and b0 represent the gradient and y-intercept (or bias).\n",
        "* These are unknowns, and varying values of these can generate different lines. In machine learning, X is dependent on the data, and so are the y values.\n",
        "We only have control over b0 and b1, that act as our model parameters.\n",
        "* We aim to find optimal values of these two parameters, that generate a line that minimizes the difference between predicted and actual y values.\n",
        "\n",
        "But how do we know the optimal values of our bias and weight values? Well, we don’t. But we can iteratively find it out using Gradient Descent. We start with random values and change them slightly for multiple steps until we get close to the optimal values.\n",
        "\n",
        "First, let us initialize Linear Regression, and we will go over the optimization process in greater detail later."
      ],
      "metadata": {
        "id": "d8JoM-A1SkGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing\n",
        "\n",
        "Reminder: The general equation for a linear line is:\n",
        "\n",
        "y = b0 + b1*X\n",
        "\n",
        "* X is numeric, single-valued. Here b1 and b0 represent the gradient and y-intercept (or bias).\n",
        "* These are unknowns, and varying values of these can generate different lines. In machine learning, X is dependent on the data, and so are the y values.\n",
        "We only have control over b0 and b1, that act as our model parameters.\n",
        "* We aim to find optimal values of these two parameters, that generate a line that minimizes the difference between predicted and actual y values.\n",
        "\n",
        "But how do we know the optimal values of our bias and weight values? Well, we don’t. But we can iteratively find it out using Gradient Descent. We start with random values and change them slightly for multiple steps until we get close to the optimal values.\n",
        "\n",
        "First, let us initialize Linear Regression, and we will go over the optimization process in greater detail later.\n",
        "\n",
        "```python\n",
        "class LinearRegression:\n",
        "    def __init__(self, lr: int = 0.01, n_iters: int = 1000) -> None:\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "```\n",
        "\n",
        "* We use a learning rate and number of iterations hyperparameters, that will be explained later.\n",
        "* The weights and biases are set to None because the number of weight parameters depends on the input features within the data.\n",
        "* We do not have access to the data yet, so we initialize them to None for now."
      ],
      "metadata": {
        "id": "gerciF_4Rsw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LinearRegression object\n",
        "model = LinearRegression(lr=0.01, n_iters=1000)"
      ],
      "metadata": {
        "id": "eX5KEFufdp4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "cVRljjZYeMVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Fit Method\n",
        "In the fit method, we are provided with data and their associated values. We can now use these, to initialize our weights, and then train the model to find optimal weights.\n",
        "\n",
        "```python\n",
        "def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape     # X shape [N, f]\n",
        "        self.weights = np.random.rand(num_features)  # W shape [f, 1]\n",
        "        self.bias = 0\n",
        "```\n",
        "\n",
        "* Most elements of ML are usually performed using ```sklearn``` which offers a uniform API.\n",
        "* New algorithms developed outside the ```sklearn``` framework will usually use the same established notation which makes it easy to switch to switch or use them in combination with e.g. tools for performance evaluation."
      ],
      "metadata": {
        "id": "06VEuTE6WOi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predicting Y Values\n",
        "\n",
        "We use the line equation discussed above to calculate predicted y values. However, instead of an iterative approach to sum all values, we can follow a vectorized approach for faster computation. Given that the weights and X values are NumPy arrays, we can use matrix multiplication to get predictions.\n",
        "\n",
        "X has shape (num_samples, num_features) and weights have shape (num_features, ). We want the predictions to be of shape (num_samples, ) matching the original y values. Therefore we can multiply X with weights, or (num_samples, num_features) x (num_features, ) to obtain predictions of shape (num_samples, ).\n",
        "\n",
        "The bias value is added at the end of each prediction. This can simply be implemented in a single line.\n"
      ],
      "metadata": {
        "id": "0AxLhKgBWtrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred shape should be N, 1\n",
        "y_pred = np.dot(X, self.weights) + self.bias"
      ],
      "metadata": {
        "id": "K5ncRGZ8WzaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* However, are these predictions correct? Obviously not. We are using randomly initialized values for the weights and bias, so the predictions will also be random.\n",
        "* How do we get the optimal values? **Gradient Descent** (little digression), which will be an optimization (minimize RMSE) exercise."
      ],
      "metadata": {
        "id": "yCgMMX2EXl2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient descent"
      ],
      "metadata": {
        "id": "p9oXgN5qZRLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* For our predictions to be as close to original targets as possible, we now try to minimize this function. The loss function will be minimum, where the gradient is zero. As we can only optimize our weights and bias values, we take the partial derivates of the MSE function with respect to weights and bias values.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png)\n",
        "\n",
        "* We take the gradient (= derivative) with respect to each weight value and then move them to the opposite of the gradient.\n",
        "* This pushes the the loss towards minimum. If the gradient is positive, so we decrease the weight (and vice versa).\n",
        "* This pushes the J(W) or loss towards the minimum value.\n",
        "* The learning rate (or alpha) controls the incremental steps. We only make a small change in the value, for stable movement towards the minimum.\n",
        "\n",
        "\n",
        "* If we simplify the derivate equation using basic algebraic manipulation, this becomes very simple to implement.\n",
        "* For the derivate, we implement this using two lines of code:"
      ],
      "metadata": {
        "id": "flS7xjBCZS5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X -> [ N, f ]\n",
        "# y_pred -> [ N ]\n",
        "# dw -> [ f ]\n",
        "dw = (1 / num_samples) * np.dot(X.T, y_pred - y)\n",
        "db = (1 / num_samples) * np.sum(y_pred - y)"
      ],
      "metadata": {
        "id": "i5NyAbavaOhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* dw is again of shape (num_features, ) So we have a separate derivate value for each weight. We optimize them separately. db has a single value.\n",
        "* To optimize the values now, we move the values in the opposite direction of the gradient using basic subtraction."
      ],
      "metadata": {
        "id": "Nku9Cv3SaTkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.weights = self.weights - self.lr * dw\n",
        "self.bias = self.bias - self.lr * db"
      ],
      "metadata": {
        "id": "klKsBwqOXkLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Again, this is only a single step.\n",
        "* We only make a small change to the randomly initialized values. We now repeatedly perform the same steps, to converge towards a minimum.\n",
        "* The complete loop is as follows:"
      ],
      "metadata": {
        "id": "jAsH99t-aup6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(self.n_iters):\n",
        "\n",
        "            # y_pred shape should be N, 1\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # X -> [N,f]\n",
        "            # y_pred -> [N]\n",
        "            # dw -> [f]\n",
        "            dw = (1 / num_samples) * np.dot(X.T, y_pred - y)\n",
        "            db = (1 / num_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.weights = self.weights - self.lr * dw\n",
        "            self.bias = self.bias - self.lr * db"
      ],
      "metadata": {
        "id": "xSDyxJkka2Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction\n",
        "* We predict the same way as we did during training.\n",
        "* However, now we have the optimal set of weights and biases.\n",
        "* The predicted values should now be close to the original values.\n",
        "\n"
      ],
      "metadata": {
        "id": "aLEEDXUJbD1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias"
      ],
      "metadata": {
        "id": "jZBz3OsMbD_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Investigate"
      ],
      "metadata": {
        "id": "J8qYcKNQbjSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self, lr: int = 0.01, n_iters: int = 1000) -> None:\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape     # X shape [N, f]\n",
        "        self.weights = np.random.rand(num_features)  # W shape [f, 1]\n",
        "        self.bias = 0\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "\n",
        "            # y_pred shape should be N, 1\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # X -> [N,f]\n",
        "            # y_pred -> [N]\n",
        "            # dw -> [f]\n",
        "            dw = (1 / num_samples) * np.dot(X.T, y_pred - y)\n",
        "            db = (1 / num_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.weights = self.weights - self.lr * dw\n",
        "            self.bias = self.bias - self.lr * db\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias"
      ],
      "metadata": {
        "id": "bKIthoxpavHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LinearRegression object\n",
        "model = LinearRegression(lr=0.01, n_iters=1000)"
      ],
      "metadata": {
        "id": "_weqxB0BWkMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the example data\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "zhsfTfV6WkQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ym-N5dGUWkTu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}