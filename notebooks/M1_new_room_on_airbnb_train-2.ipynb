{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otTcIWiRz3fB"
      },
      "source": [
        "# Airbnb Price Prediction in Copenhagen\n",
        "\n",
        "This notebook demonstrates the process of building a machine learning model to predict Airbnb prices in Copenhagen. We'll go through data loading, preprocessing, feature engineering, model training, and evaluation, with a focus on handling imbalanced data, imputation, and model explainability.\n",
        "\n",
        "## Setup and Data Loading\n",
        "\n",
        "First, let's install the required libraries and import the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-rB5uaWz3fG"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost -q\n",
        "!pip install shap -q\n",
        "!pip install imbalanced-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from xgboost import XGBRegressor\n",
        "import shap\n",
        "import joblib\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2022-06-24/data/listings.csv.gz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KUtN7LFz3fI"
      },
      "source": [
        "## Data Preprocessing and Exploratory Data Analysis\n",
        "\n",
        "Let's clean the data and prepare it for analysis. We'll also perform some initial exploratory data analysis to understand our dataset better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBVVyGZLz3fJ"
      },
      "outputs": [],
      "source": [
        "# Display initial information about the dataset\n",
        "data.info()\n",
        "\n",
        "# Clean price column\n",
        "data['price'] = data.price.str.replace('$', '').str.replace(',', '').astype('float')\n",
        "\n",
        "# Convert instant_bookable to boolean\n",
        "data['instant_bookable'] = data['instant_bookable'].map({'f': False, 't': True})\n",
        "\n",
        "# Filter data\n",
        "data = data[data.room_type.isin(['Private room', 'Entire home/apt'])]\n",
        "data = data[data.number_of_reviews_l30d >= 1]\n",
        "data = data[data.review_scores_rating >= 4]\n",
        "\n",
        "# Remove outliers\n",
        "data['price_z'] = (data['price'] - data['price'].mean()) / data['price'].std(ddof=0)\n",
        "data = data[data.price_z.abs() < 2]\n",
        "\n",
        "print(\"Data shape after initial filtering:\", data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhCWP6VJz3fJ"
      },
      "source": [
        "### Explanation of Filtering Steps\n",
        "\n",
        "1. **Room Type Filter**: We focus on 'Private room' and 'Entire home/apt' as these are the most common and relevant types for typical Airbnb users. This helps us create a more focused model.\n",
        "\n",
        "2. **Recent Reviews Filter**: By keeping listings with at least one review in the last 30 days, we ensure our data represents active and current listings, improving the relevance of our predictions.\n",
        "\n",
        "3. **High Rating Filter**: Filtering for listings with ratings of 4 or above helps us focus on quality accommodations, which are more likely to have consistent pricing patterns.\n",
        "\n",
        "4. **Outlier Removal**: We remove price outliers using z-scores. This step is crucial because extreme prices (very low or very high) can disproportionately influence our model, leading to poor generalization. By removing listings with z-scores greater than 2 (roughly the top and bottom 2.5% of prices), we focus on the more typical price range.\n",
        "\n",
        "These filtering steps help us create a cleaner, more relevant dataset, which should lead to a more accurate and useful predictive model.\n",
        "\n",
        "## Feature Selection\n",
        "\n",
        "Now, let's select the features we'll use for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC0og7c8z3fJ"
      },
      "outputs": [],
      "source": [
        "# Select features\n",
        "selected_features = ['neighbourhood_cleansed', 'room_type', 'instant_bookable', 'accommodates', 'bedrooms', 'beds', 'minimum_nights_avg_ntm', 'price']\n",
        "df = data[selected_features]\n",
        "\n",
        "# Display information about selected features\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tQTu7nFz3fK"
      },
      "source": [
        "### Explanation of Feature Selection\n",
        "\n",
        "We've chosen these features based on their likely impact on price:\n",
        "\n",
        "- `neighbourhood_cleansed`: Location is often a key factor in accommodation pricing.\n",
        "- `room_type`: The type of space (entire home vs. private room) significantly affects price.\n",
        "- `instant_bookable`: This feature might influence pricing strategy.\n",
        "- `accommodates`: The number of guests a listing can host is usually reflected in its price.\n",
        "- `bedrooms` and `beds`: These features directly relate to the size and capacity of the listing.\n",
        "- `minimum_nights_avg_ntm`: Minimum stay requirements can affect pricing strategies.\n",
        "\n",
        "This selection balances the need for relevant information with the desire to keep our model relatively simple and interpretable.\n",
        "\n",
        "## Handling Missing Data\n",
        "\n",
        "Imputation is the process of replacing missing data with substituted values. Here, we'll use K-Nearest Neighbors (KNN) imputation to fill in missing values for 'bedrooms' and 'beds' based on similar listings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRLfusljz3fK"
      },
      "outputs": [],
      "source": [
        "# Identify columns with missing values\n",
        "columns_to_impute = ['bedrooms', 'beds']\n",
        "\n",
        "# Select relevant features for KNN imputation\n",
        "knn_data = df[['bedrooms', 'beds', 'accommodates', 'minimum_nights_avg_ntm']]\n",
        "\n",
        "# Initialize and apply KNN Imputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "knn_imputed = imputer.fit_transform(knn_data)\n",
        "\n",
        "# Update the original dataframe with imputed values\n",
        "df[columns_to_impute] = knn_imputed[:, :2]\n",
        "\n",
        "print(\"Missing values after imputation:\")\n",
        "print(df[columns_to_impute].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Rxn5SCz3fK"
      },
      "source": [
        "### Explanation of KNN Imputation\n",
        "\n",
        "We use KNN imputation because it can capture complex relationships between features. For each listing with missing data:\n",
        "\n",
        "1. It finds the 5 most similar listings based on the other features.\n",
        "2. It then uses the average value from these neighbors to fill in the missing data.\n",
        "\n",
        "This method is more sophisticated than simple mean or median imputation, as it takes into account the characteristics of each listing when estimating missing values.\n",
        "\n",
        "## Handling Imbalanced Data\n",
        "\n",
        "Although we're dealing with a regression problem, the 'room_type' feature is categorical and potentially imbalanced. We'll use random under-sampling to balance this feature, which can help prevent bias in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC3hex6hz3fL"
      },
      "outputs": [],
      "source": [
        "# Check initial distribution of room types\n",
        "print(\"Initial room type distribution:\")\n",
        "print(df['room_type'].value_counts())\n",
        "\n",
        "# Initialize RandomUnderSampler\n",
        "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# Apply resampling using 'room_type' as a balancing feature\n",
        "X_resampled, _ = rus.fit_resample(df, df['room_type'])\n",
        "\n",
        "print(\"\\nRoom type distribution after balancing:\")\n",
        "print(X_resampled['room_type'].value_counts())\n",
        "\n",
        "# Update our dataframe (or not - since this was just an example)\n",
        "# df = X_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxkWmxVyz3fL"
      },
      "source": [
        "### Explanation of Balancing\n",
        "\n",
        "Even in regression tasks, balanced categorical features can be important:\n",
        "\n",
        "1. It prevents the model from being biased towards the majority class.\n",
        "2. It ensures the model learns equally from all types of listings.\n",
        "3. It can lead to better generalization, especially for minority class predictions.\n",
        "\n",
        "By using random under-sampling, we reduce the number of samples in the majority class to match the minority class. This technique is simple yet effective, though it does mean we'll be working with a smaller dataset.\n",
        "\n",
        "## Feature Engineering\n",
        "\n",
        "We'll now encode categorical variables and scale numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84ghLOJlz3fL"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "ohe = OneHotEncoder() # be careful - output is a sparse matrix\n",
        "cat_features = ['neighbourhood_cleansed', 'room_type']\n",
        "X_cat = pd.DataFrame(ohe.fit_transform(X[cat_features]).todense(), columns=ohe.get_feature_names_out(cat_features))\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "num_features = ['instant_bookable', 'accommodates', 'bedrooms', 'beds', 'minimum_nights_avg_ntm']\n",
        "X_num = pd.DataFrame(scaler.fit_transform(X[num_features]), columns=num_features)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical features\n",
        "X_processed = pd.concat([X_num, X_cat], axis=1)\n",
        "\n",
        "print(\"Shape of processed features:\", X_processed.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0e2kbhXz3fL"
      },
      "source": [
        "### Explanation of Feature Engineering\n",
        "\n",
        "1. **One-Hot Encoding**: We use this for categorical variables like 'neighbourhood_cleansed' and 'room_type'. It creates binary columns for each category, allowing the model to learn separate weights for each category.\n",
        "\n",
        "2. **Standard Scaling**: We apply this to numerical features. It standardizes the features to have zero mean and unit variance, which is important for many machine learning algorithms, including XGBoost. This ensures all features are on a similar scale and prevents features with larger magnitudes from dominating the model training.\n",
        "\n",
        "## Model Training and Evaluation\n",
        "\n",
        "We'll use XGBoost for our regression task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHFMdBEPz3fL"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "model_xgb = XGBRegressor(random_state=42)\n",
        "model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, model_xgb.predict(X_train)))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, model_xgb.predict(X_test)))\n",
        "\n",
        "print(f\"Train RMSE: {train_rmse:.2f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8hZ9ptRz3fM"
      },
      "source": [
        "## Model Explainability\n",
        "\n",
        "Understanding how our model makes predictions is crucial. We'll use SHAP (SHapley Additive exPlanations) values to interpret our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6uoGjuvz3fM"
      },
      "outputs": [],
      "source": [
        "# Create a SHAP explainer\n",
        "explainer = shap.TreeExplainer(model_xgb)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Plot feature importances\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "\n",
        "# Plot SHAP values\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# Plot dependence for the most important feature\n",
        "most_important_feature = X_test.columns[np.argmax(np.abs(shap_values).mean(0))]\n",
        "shap.dependence_plot(most_important_feature, shap_values, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values[1,:], X_test.iloc[1,:])"
      ],
      "metadata": {
        "id": "3k8WtE8412Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zof6SQ7Tz3fM"
      },
      "source": [
        "### Explanation of SHAP Values\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) values help us understand the impact of each feature on individual predictions:\n",
        "\n",
        "1. They show how much each feature contributes, positively or negatively, to the prediction for each instance.\n",
        "2. They can reveal complex interactions between features.\n",
        "3. They provide both global interpretability (overall feature importance) and local interpretability (feature impact on individual predictions).\n",
        "\n",
        "Understanding these values can help hosts and users alike in comprehending the factors influencing Airbnb prices in Copenhagen.\n",
        "\n",
        "## Save the Model and Preprocessing Objects\n",
        "\n",
        "Finally, let's save our model and preprocessing objects for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVqzak-pz3fM"
      },
      "outputs": [],
      "source": [
        "joblib.dump(model_xgb, 'model_xgb.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "joblib.dump(ohe, 'ohe.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRdHRy6dz3fM"
      },
      "source": [
        "This notebook demonstrates the end-to-end process of building a price prediction model for Airbnb listings in Copenhagen. We covered data preprocessing, handling missing data through imputation, addressing class imbalance, feature engineering, model training, and interpretation using SHAP values. The model and preprocessing objects are saved for future use or deployment."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}