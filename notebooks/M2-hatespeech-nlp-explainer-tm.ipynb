{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlrUhxBZdLanqERGuyLzWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M2-hatespeech-nlp-explainer-tm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigger warning: This notebook contains words or language that are considered profane, vulgar, or offensive by some."
      ],
      "metadata": {
        "id": "31w7INhskhWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor -q\n",
        "\n",
        "# Installing Gensim and PyLDAvis\n",
        "!pip install -qq -U gensim\n",
        "!pip install -qq pyLDAvis\n",
        "!pip install --force-reinstall -q numpy==1.22.4"
      ],
      "metadata": {
        "id": "a7ClxNmEW8-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explainability (why did the model say it's hate speech)\n",
        "!pip install eli5"
      ],
      "metadata": {
        "id": "Yfx3zk8hOcRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMGBKNqTRcNu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import preprocessor as prepro # twitter prepro\n",
        "import tqdm #progress bar\n",
        "\n",
        "import spacy #spacy for quick language prepro\n",
        "nlp = spacy.load('en_core_web_sm') #instantiating English module\n",
        "\n",
        "# sampling, splitting\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# loading ML libraries\n",
        "from sklearn.pipeline import make_pipeline #pipeline creation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
        "from sklearn.linear_model import LogisticRegression #Logit model\n",
        "from sklearn.metrics import classification_report #that's self explanatory\n",
        "from sklearn.decomposition import TruncatedSVD #dimensionality reduction\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import altair as alt #viz\n",
        "\n",
        "#explainability\n",
        "import eli5\n",
        "from eli5.lime import TextExplainer\n",
        "\n",
        "# topic modeling\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary # Import the dictionary builder\n",
        "from gensim.models import LdaMulticore # we'll use the faster multicore version of LDA\n",
        "\n",
        "# Import pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "%matplotlib inline\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsQWaTU41ksj"
      },
      "source": [
        "# prepro settings\n",
        "prepro.set_options(prepro.OPT.URL, prepro.OPT.NUMBER, prepro.OPT.RESERVED, prepro.OPT.MENTION, prepro.OPT.SMILEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3-rBov_TJuV"
      },
      "source": [
        "data = pd.read_csv('https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/twitter_hate.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUhjywn2gBz0"
      },
      "source": [
        "data['text_clean'] = data['tweet'].map(lambda t: prepro.clean(t))\n",
        "data['text_clean'] = data['text_clean'].str.replace('#','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy basics"
      ],
      "metadata": {
        "id": "9QmceSunZ_Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a sentence about how much i would like to have a break in spanish they call it pausa\""
      ],
      "metadata": {
        "id": "rvye6QPVU5tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = text.split(' ')"
      ],
      "metadata": {
        "id": "kjvHcGNfVAr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(text_list)"
      ],
      "metadata": {
        "id": "21ebkHILVJnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_spacy = nlp(text)"
      ],
      "metadata": {
        "id": "tPBBKrUDVZPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(word.lemma_, word.pos_) for word in text_spacy if not word.is_stop]"
      ],
      "metadata": {
        "id": "Ww9zZrnQVdmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_spacy.vector"
      ],
      "metadata": {
        "id": "Z5ZldFmpWxmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_spacy = nlp('Aalborg Univ. is a nice place where Donaldo Trump has never been.')"
      ],
      "metadata": {
        "id": "v4koNHO5a9zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(token, token.label_) for token in text_spacy.ents]"
      ],
      "metadata": {
        "id": "sj2VaiZebHHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "back to the project"
      ],
      "metadata": {
        "id": "-_r_uMJDaD9f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijwe4ks92cTn"
      },
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm.tqdm(total=len(data['text_clean']),position=0, leave=True)\n",
        "\n",
        "for text in nlp.pipe(data['text_clean'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text \n",
        "         if token.is_alpha \n",
        "         and not token.is_stop \n",
        "         and not token.is_punct]\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write everything into one function that can be re-used later\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  removes twitter stuff\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "  texts_clean = texts_clean.str.replace('#','')\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "Vb8RI568dKqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HURoUCsB2kt3"
      },
      "source": [
        "# apply all prepro-pipeline to texts\n",
        "data['text_clean'] = text_prepro(data['tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdAqGWwL12Y0"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKUEvg4Q1uFS"
      },
      "source": [
        "# renaming and reordering\n",
        "\n",
        "data_df = pd.DataFrame({'label':data['class'], 'text':data['text_clean']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_df.label.value_counts().reset_index()"
      ],
      "metadata": {
        "id": "iHympnHCdwJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt.Chart(data_df.label.value_counts().reset_index()).mark_bar(filled=True).encode(\n",
        "    alt.X('label:Q', title='N Tweets'),\n",
        "    alt.Y('index:N', title='Category')\n",
        ")"
      ],
      "metadata": {
        "id": "AQo2S7S785Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51Miz9_kpref"
      },
      "source": [
        "# fixing sample imbalance\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "data_df_res, y_res = rus.fit_resample(data_df, data_df['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df_res['label'].value_counts()"
      ],
      "metadata": {
        "id": "HoJRCOfn-YuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set (since we have a new output variable)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_df_res['text'], y_res, test_size = 0.4, random_state = 42)"
      ],
      "metadata": {
        "id": "myvcNFNQf4cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate models and \"bundle up as pipeline\"\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "cls = LogisticRegression()\n",
        "\n",
        "pipe = make_pipeline(tfidf, cls)"
      ],
      "metadata": {
        "id": "xPwp4olReY5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train,y_train) # fit model"
      ],
      "metadata": {
        "id": "y6nRa0zkeY0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model performance on training set\n",
        "\n",
        "y_eval = pipe.predict(X_train)\n",
        "report = classification_report(y_train, y_eval)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "alBRiy43eYtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run single prediction\n",
        "\n",
        "t1 = ['you stupid fag bitch']"
      ],
      "metadata": {
        "id": "rXQWo_xuhtQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess\n",
        "\n",
        "t1_p = text_prepro(pd.Series(t1)) # note, we need to pack text up as pd.Series "
      ],
      "metadata": {
        "id": "Zh94Jc4NhyCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "\n",
        "pipe.predict(t1_p)"
      ],
      "metadata": {
        "id": "sYWUNTSch3KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall weights (works only for linear models)\n",
        "eli5.show_weights(pipe, top=10, target_names=['hate','offensive','nothing'])"
      ],
      "metadata": {
        "id": "WN-oI_JwinYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explain one prediction\n",
        "eli5.show_prediction(pipe[1], t1_p[0], vec=pipe[0],\n",
        "                     target_names=['hate','offensive','nothing'])"
      ],
      "metadata": {
        "id": "2TJSO7JLjwZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tweet'][100]"
      ],
      "metadata": {
        "id": "IxKdidW1mW25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['class'][100]"
      ],
      "metadata": {
        "id": "Zs3wH9DTmiJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eli5.show_prediction(pipe[1], data['text_clean'][100], vec=pipe[0],\n",
        "                     target_names=['hate','offensive','nothing'])"
      ],
      "metadata": {
        "id": "kw5JSFnglfS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try a complex (black-box) model"
      ],
      "metadata": {
        "id": "GADnYvq7PGJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate models and \"bundle up as pipeline\"\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "svd = TruncatedSVD(n_components = 100)\n",
        "cls_xg = XGBClassifier()\n",
        "\n",
        "pipe_xg = make_pipeline(tfidf, svd, cls_xg)"
      ],
      "metadata": {
        "id": "zKC9cONAPCq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_xg.fit(X_train,y_train) # fit model"
      ],
      "metadata": {
        "id": "W7H4C77rPatZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model performance on training set\n",
        "\n",
        "y_eval = pipe_xg.predict(X_train)\n",
        "report = classification_report(y_train, y_eval)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "YJ188lIqPhS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model performance on test set\n",
        "\n",
        "y_pred = pipe_xg.predict(X_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "xmQ1aG7-PnCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explain single prediction\n",
        "te = TextExplainer(random_state=42)\n",
        "te.fit(data['text_clean'][100], pipe_xg.predict_proba)\n",
        "te.show_prediction(target_names=['hate','offensive','nothing'])"
      ],
      "metadata": {
        "id": "Z6VgBGjLPzxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_xg = pipe_xg.predict(X_test)\n"
      ],
      "metadata": {
        "id": "V7Ip1sP7sD7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_evaluation =pd.DataFrame(X_test)"
      ],
      "metadata": {
        "id": "FIvoln8zsO4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_evaluation.index = range(len(X_test_evaluation))"
      ],
      "metadata": {
        "id": "rNwtZ03Kscow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_evaluation['y_test'] = list(y_test)"
      ],
      "metadata": {
        "id": "C8MZCUNLsiyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_evaluation['pred_xg'] = list(pred_xg)"
      ],
      "metadata": {
        "id": "EPWMjEFHsnuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_evaluation"
      ],
      "metadata": {
        "id": "1cDwACV-svjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(X_test_evaluation['y_test'],X_test_evaluation['pred_xg'], normalize='index')"
      ],
      "metadata": {
        "id": "btZCRU6Aszvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic model"
      ],
      "metadata": {
        "id": "MqBf02ghmQEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts (we need tokens)\n",
        "tokens = []\n",
        "\n",
        "for text in nlp.pipe(data['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in text \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "xZiq7pEAWes_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tokens'] = tokens"
      ],
      "metadata": {
        "id": "dWy_pCa0ge7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "1H4cdl4ehLPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like to know what things people are talking about when it is considerede \"hatespeech\""
      ],
      "metadata": {
        "id": "Po510P-1obpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_hate = data[data['class'] == 0]"
      ],
      "metadata": {
        "id": "DfdEXrJFgqgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(data_hate['tokens'])\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data_hate['tokens']]"
      ],
      "metadata": {
        "id": "uxBNVSFwgrKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=10, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "WR-I1RThhBSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to visualize\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "81KTmWw7hCqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Let's Visualize\n",
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "dfGSNcH5hl_S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}