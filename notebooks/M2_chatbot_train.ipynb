{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPoTJ3xfqUCOkb7PTNd2844",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M2_chatbot_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple FAQ chatbot\n",
        "\n",
        "![](https://source.unsplash.com/V5vqWC9gyEU)\n",
        "\n",
        "This is a variation of https://github.com/python-engineer/pytorch-chatbot, and a toy-example.\n",
        "For a more complex example check out (e.g.) https://github.com/MrJay10/banking-faq-bot. Datacamp has some courses on that topic, too.\n",
        "\n",
        "The data needed for building such a system is a collection from a company FAQ for instance with variations for each question-answer pair type. For examples with many q-a pairs like the banking-faq, you can consider similarity-based approaches, where input text is matched with most \"similar\" predifined questions and then the user picks upon request - \"did you mean xyz-question\"\n",
        "Alternatively, one could create paraphrased versions of questions for each question-answer pair to have more training examples. This can be done manually or \n",
        "\n",
        "The overall architecture is as follows:\n",
        "\n",
        "*   Given a free text/prompt, predict which question is asked (intent)\n",
        "*   Pick corresponding answer (e.g. random out of 2-3) to simulate dialogue\n",
        "\n",
        "In this notebook, we will first use TFIDF-Logit, then standard SpaCy vectors and finally Floret (new SpaCy vectors that combine a new efficient compression with fastText, that helps overcome typos by including subword-elements in the model)\n"
      ],
      "metadata": {
        "id": "wyxDgeviYAWi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O18yPl__dtpc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils...\n",
        "import json\n",
        "import requests\n",
        "import random"
      ],
      "metadata": {
        "id": "UG0NwwC9JS9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will be using the medium-sized spacy model here"
      ],
      "metadata": {
        "id": "Nth-mmRSaAue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! spacy download en_core_web_md --quiet"
      ],
      "metadata": {
        "id": "WUQklcMMOAqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy #spacy for quick language prepro\n",
        "nlp = spacy.load('en_core_web_md') #instantiating English module"
      ],
      "metadata": {
        "id": "ocLVwJT6eJfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline #pipeline creation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
        "from sklearn.linear_model import LogisticRegression #Logit model"
      ],
      "metadata": {
        "id": "XoLr_SehePR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we use spacy for preprocessing"
      ],
      "metadata": {
        "id": "VJzNOJjKaF6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a list/iterable of texts\n",
        "  removes twitter stuff\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  for text in nlp.pipe(texts, disable=[\"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text # lemmatize and lower\n",
        "          if token.is_alpha # remove numbers\n",
        "          and not token.is_punct] # remove punctoation\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "Cc87yUzfflkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is a json-file with questions and answers as well as \"intent-tags\"\n",
        "We can open it from local or grab it from remote with requests (only one option).\n",
        "The reasonn for using a json here is that variations of questions and answers are independent (number/linking; n-n). e.g. you can have 2 ways to ask and 4 ways to answer for a specific issue."
      ],
      "metadata": {
        "id": "CqR6QJ99aL9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#open loacal file\n",
        "# data = json.load(open('chatbot-convo.json','r'))"
      ],
      "metadata": {
        "id": "Afc5iEBTf7vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stream file from remote online\n",
        "r = requests.get('https://github.com/aaubs/ds-master/raw/main/data/chatbot-convo.json')\n",
        "data = json.loads(r.text)"
      ],
      "metadata": {
        "id": "avU96skeH818"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training, we need to get all questions and associated labels into tabular format. We iterate over all intents in the data and zip them with the intent-label into a list of tupels. Zip is a great function to bind 2 lists into a list of tupels `[a,b,c], [1,2,3] â†’ [(a,1), (b,2), (c,3)]`\n",
        "we multiply the label `l*[i['tag']]` to reach `len(i['patterns']) == len(l*[i['tag']])`"
      ],
      "metadata": {
        "id": "Od_NcosWayl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training = []\n",
        "\n",
        "for i in data['intents']:\n",
        "  l = len(data['intents'][0]['patterns'])\n",
        "  tuples = list(zip(i['patterns'], l*[i['tag']])) \n",
        "  training.extend(tuples)\n"
      ],
      "metadata": {
        "id": "nWaCHVx_gdyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = pd.DataFrame(training, columns=['txt','label'])"
      ],
      "metadata": {
        "id": "O7bGagjbgenf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.txt_p = text_prepro(training_df.txt)"
      ],
      "metadata": {
        "id": "uoisfhHSiBS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate models and \"bundle up as pipeline\"\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "cls = LogisticRegression()\n",
        "\n",
        "pipe = make_pipeline(tfidf, cls)"
      ],
      "metadata": {
        "id": "6ogumcNqiKDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(training_df.txt_p, training_df.label)"
      ],
      "metadata": {
        "id": "UjbAC947ilWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.predict(['tell me something funny'])"
      ],
      "metadata": {
        "id": "uHK4GHj2jJL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's give our bot a name\n",
        "bot_name = 'ðŸ’¬ Hugo'"
      ],
      "metadata": {
        "id": "TU6B3CZbk0Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input triggers reply, which preprocesses the text, asks the model for a prediction (which class of question?), then iterates over all possible intent-types and where it finds a match, it retrieves a random response from this class."
      ],
      "metadata": {
        "id": "Q34db35tbpio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reply(txt):\n",
        "  clean_text = text_prepro([txt])\n",
        "  tag = pipe.predict(clean_text)[0]\n",
        "  for intent in data['intents']:\n",
        "    if tag == intent[\"tag\"]:\n",
        "      print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n"
      ],
      "metadata": {
        "id": "8FyKHuP2jv_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply('Byes')"
      ],
      "metadata": {
        "id": "OD4vca4slXxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using SpaCy for vectorization\n",
        "Using \"traditional\" spacy embeddings. Medium web model.\n",
        "When using spacy, we rely on pretrained vectors i.e. spacy takes care of all preprocessing and vectorization internally. Thus, we can skip TFIDF and go directly to model training."
      ],
      "metadata": {
        "id": "3HNti4NsJZFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we use again logistic regression\n",
        "model_spacy = LogisticRegression()"
      ],
      "metadata": {
        "id": "_1E9nP1LIi0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.head(10)"
      ],
      "metadata": {
        "id": "fdZdevGKJr4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we grab the vectors for all texts and stack them into a matrix\n",
        "X_train = np.vstack([txt.vector for txt in nlp.pipe(training_df.txt, disable=[\"parser\", \"ner\"])])"
      ],
      "metadata": {
        "id": "BxKLgpKzJzLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick explaininng of the vectors (not really part of the code)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(X_train)[0]"
      ],
      "metadata": {
        "id": "D1EcQ14iKwMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "model_spacy.fit(X_train, training_df.label)"
      ],
      "metadata": {
        "id": "hEjzytCVKUhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot_name = 'ðŸ’¬ SpaCy Hugo'"
      ],
      "metadata": {
        "id": "xeJB2B94Kd9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reply_spacy(txt):\n",
        "  tag = model_spacy.predict([nlp(txt).vector])[0]\n",
        "  for intent in data['intents']:\n",
        "    if tag == intent[\"tag\"]:\n",
        "      print(f\"{bot_name}: {random.choice(intent['responses'])}\")"
      ],
      "metadata": {
        "id": "jy3uPsUyMLYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply_spacy('how can I pay?')"
      ],
      "metadata": {
        "id": "K0j-ew5kMXga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying out Floret ðŸŒ¸\n",
        "In 2017 Facebook introduced [fastText](https://fasttext.cc/) that includes sub-word elements in the model creation. In April 2022 Explorion.ai presented Blom Embeddings - an elegant way to reduce embessing size. Later In August 2022 they introduced [floret](https://explosion.ai/blog/floret-vectors), an approach to jon fastText with Bloom (not related to https://huggingface.co/bigscience/bloom)."
      ],
      "metadata": {
        "id": "hB3KPlztOqEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get floret installed\n",
        "! python -m pip install floret 'spacy~=3.4.0' --quiet"
      ],
      "metadata": {
        "id": "ZE40R9dNOXYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download floret vectors\n",
        "! wget -nc https://github.com/explosion/spacy-vectors-builder/releases/download/en-3.4.0/en_vectors_floret_md.floret.gz"
      ],
      "metadata": {
        "id": "CYu84PJHRyor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy initialize them internally\n",
        "! spacy init vectors en en_vectors_floret_md.floret.gz en_vectors_floret_md --mode floret"
      ],
      "metadata": {
        "id": "yOCS-EeQR3vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the spaCy pipeline with floret vectors\n",
        "nlp_fl = spacy.load(\"en_vectors_floret_md\")"
      ],
      "metadata": {
        "id": "5tPCNauPR7FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "how are these vectors different from standard spacy?\n",
        "the difference lies in OOV (out of vocabulary) words. Such words can occur where there are jargon or typos. Standard vecs in spacy contain a many 10k vocabularies. However, not everything can be covered"
      ],
      "metadata": {
        "id": "ps8-nijNc07o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_1 = nlp.vocab[\"univercities\"]\n",
        "word_2 = nlp.vocab[\"universities\"]\n",
        "\n",
        "word_1.similarity(word_2)"
      ],
      "metadata": {
        "id": "CCMEd3ncSHm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# typo --> 0-vector\n",
        "word_1.vector"
      ],
      "metadata": {
        "id": "XZc2MPnHSOmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Floret vectors will also rely on \n",
        "`<univ', 'unive', 'niver', 'iverc', 'verci', 'ercit', 'rciti', 'citie', 'ities', 'ties>` and get it closer to right..."
      ],
      "metadata": {
        "id": "ngU7v7wNdPED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_1 = nlp_fl.vocab[\"univercities\"]\n",
        "word_2 = nlp_fl.vocab[\"universities\"]\n",
        "\n",
        "word_1.similarity(word_2)"
      ],
      "metadata": {
        "id": "kikMFacVSa0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_1.vector"
      ],
      "metadata": {
        "id": "V9yY71axSg01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.vstack([txt.vector for txt in nlp_fl.pipe(training_df.txt, disable=[\"parser\", \"ner\"])])"
      ],
      "metadata": {
        "id": "CsBLufH5SvUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fl = LogisticRegression()"
      ],
      "metadata": {
        "id": "3BecPqZ3S8IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fl.fit(X_train, training_df.label)"
      ],
      "metadata": {
        "id": "ZRTVEuVcTBKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot_name = 'ðŸŒ¸ Floret Hugo'"
      ],
      "metadata": {
        "id": "RQSxFZnNTR6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reply_fl(txt):\n",
        "  tag = model_fl.predict([nlp_fl(txt).vector])[0]\n",
        "  for intent in data['intents']:\n",
        "    if tag == intent[\"tag\"]:\n",
        "      print(f\"{bot_name}: {random.choice(intent['responses'])}\")"
      ],
      "metadata": {
        "id": "5QmzneFaTR6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply_fl('when do I get stuff?')"
      ],
      "metadata": {
        "id": "z29jE39ATR6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Chatbot command-line style"
      ],
      "metadata": {
        "id": "Sy5V_zPG6XxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot_name = \"ðŸŒ¸ Floret Hugo\"\n",
        "your_name = \"ðŸŽƒ RJ\"\n",
        "\n",
        "\n",
        "print(\"Let's chat! (type 'quit' to exit)\")\n",
        "while True:\n",
        "    # sentence = \"do you use credit cards?\"\n",
        "    sentence = input(f\"{your_name}: \")\n",
        "    if sentence == \"quit\":\n",
        "        break\n",
        "    reply_fl(sentence)"
      ],
      "metadata": {
        "id": "DKH3OiPK6SBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chatbot app with streamlit:\n",
        "\n",
        "There is an extension which allows to implement a chatbot in ST:\n",
        "https://ai-yash-st-chat-exampleschatbot-fkuecs.streamlitapp.com/\n",
        "\n",
        "Example code can be found here:\n",
        "https://github.com/AI-Yash/st-chat/blob/main/examples/chatbot.py\n",
        "\n",
        "The bot in the example runs a rather fancy online deployed model by FB that actually evaluates the whole dialogue rather than individual sentences. Also, you would need to understand session states and a few other things to run that..."
      ],
      "metadata": {
        "id": "7WcxerG08MCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On vectors"
      ],
      "metadata": {
        "id": "6CAA315M-b43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait, hold on, where do such vectors come from? Well, they are pretrained form large text-collections in a \"self-supervised\" fashion. The idea comes from [Word2Vec](https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html) and related approaches, popular starting 2013. The idea here is to take text i.e. sentences and challenge a model to learn context, e.g. predict the \"masked\" next word from a sequence of words â‡’ Copenhagen is the [MASK] of Denmark."
      ],
      "metadata": {
        "id": "RQosMfrsdbOR"
      }
    }
  ]
}