{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgF7mq1hld10bLbsbWyiHp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M3_RNN_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The objective of this tutorial\n",
        "\n",
        "Our goal in this tutorial is to provide simple examples of the RNN model so that you can better understand its functionality and how it can be used in a domain.\n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "\n",
        "\n",
        "```\n",
        "- What is an RNN (Recurrent Neural Network)\n",
        "- How RNNs work and their structure\n",
        "- How RNNs can be used for tasks such as timeseries\n",
        "- How to implement an RNN in code using PyTorch\n",
        "- How to train and fine-tune an RNN for a specific task\n",
        "- Common challenges and pitfalls to avoid when working with RNNs\n",
        "```\n"
      ],
      "metadata": {
        "id": "5YsD4lOPEdmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A quick recap on Simple Neural Network (FeedForward)"
      ],
      "metadata": {
        "id": "ZQawEI4XUZ-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple neural network consists of three different parts namely Parameters, Linear, and None-Linear (Activation Function ) parts:"
      ],
      "metadata": {
        "id": "Mg36lXTUh26q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First, a weight is being applied to each input to an artificial neuron. \n",
        "2. Second, the inputs are multiplied by their weights, and then a bias is applied to the outcome. This is called the weighted sum. \n",
        "3. Third, the weighted sum is processed via an activation function, as a non-linear function."
      ],
      "metadata": {
        "id": "1IUeWB5KiCMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/13UpdymQx-C1tBKRnfD7eOg.gif\" width=\"500\"> "
      ],
      "metadata": {
        "id": "HBtJUGWIkxQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network can compare the outputs of its nodes with the desired values using a property known as the delta rule, allowing the network to alter its weights through training to create more accurate output values. This training and learning procedure results in gradient descent. \n",
        "\n",
        "The technique of updating weights in multi-layered perceptrons is virtually the same, however, the process is referred to as back-propagation. In such circumstances, the output values provided by the final layer are used to alter each hidden layer inside the network."
      ],
      "metadata": {
        "id": "evhlndvXkzzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems with a Simple Neural Network"
      ],
      "metadata": {
        "id": "Ivg_7TJWRZbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main shortcomings of traditional neural networks are:\n",
        "\n",
        "1. They can not handle sequential data\n",
        "2. They can not remember the sequence of the data, i.e order is not important\n",
        "3. Can not share parameters across the sequence\n",
        "4. They have a fixed input length\n",
        "\n",
        "Let’s have a brief look at these problems, then dig deeper into RNN."
      ],
      "metadata": {
        "id": "J2NNUpctRz82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. They can not handle sequential data"
      ],
      "metadata": {
        "id": "KfTNIi8UhsLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential data in RNN (Recurrent Neural Network) refers to any type of data where the observations have a temporal or sequential relationship. This could include time series data, where each observation is dependent on the previous one, or sequence data, where the order of the observations is important. In RNNs, this type of data is processed through the recurrent connections in the network, allowing the model to maintain and update an internal state based on the information in the sequence. This makes RNNs particularly well suited for tasks such as language modeling, speech recognition, and time series forecasting. There are some variations to the neural network’s configuration based on the shape of the input or output which you can see in the following:"
      ],
      "metadata": {
        "id": "BhxBLbBnkTtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/SeqData_RNN.png\" width=\"400\">\n",
        "\n"
      ],
      "metadata": {
        "id": "3k4r4FqTksyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. They can not remember the sequence of the data, i.e order is not important"
      ],
      "metadata": {
        "id": "QgCDwtjPbxRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second limitation of traditional neural networks is that they can not remember the sequence of the data, or the order is not important to them. Let’s understand this problem with an example which is shown in this figure (MIT 6.S191 Intro to Deep Learning)."
      ],
      "metadata": {
        "id": "r_nG9WWAb4f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.jpeg](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/Problems-with-Traditional-Neural-Network-2.jpeg)"
      ],
      "metadata": {
        "id": "SAJh08ejgB18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> *RNNs use feedback connections that allow information to be passed from one step of the sequence to the next, allowing the network to maintain and update an internal state that depends on the past input. This enables RNNs to capture and understand the dependencies and patterns in the sequence data, making them well suited for tasks such as natural language processing and time series analysis.*\n",
        "\n"
      ],
      "metadata": {
        "id": "Eyl8isKBg08O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. FeedForward Neural Network Can not share parameters across the sequence\n",
        "\n",
        "In traditional Feedforward Neural Networks (MLPs), each input is processed independently and there is no mechanism for sharing parameters across different inputs in a sequence. For example, let’s take the sentence **“what is your name? My name is Lasse”**. In an MLP, each word would be treated as a separate input and would be processed through separate hidden layers. There is no way for the network to share information across words in the sequence, such as information about the relationship between words or about common features that occur across different parts of the sequence. In this case, \"name\"'s parameters should have been shared and so the neural network should have been able to determine that \"name\"'s words are dependent in this sentence."
      ],
      "metadata": {
        "id": "5_a87DZzYAA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> *In contrast, Recurrent Neural Networks (RNNs) have a hidden state that is updated at each time step, allowing the network to maintain information about the sequence and share parameters across different time steps. This makes RNNs well-suited for processing sequential data and for tasks such as sequence classification, language modeling, and machine translation*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BYpgv8E9baFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore Recurrent Neural Networks (RNN), originally were designed to handle some of the shortcomings that traditional neural networks have when dealing with sequential data."
      ],
      "metadata": {
        "id": "5x-VbMMUiw6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a Recurrent Neural Network?"
      ],
      "metadata": {
        "id": "JIhY3uV3ipL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that the Simple Neural Network is unidirectional, which means it has a single direction, whereas the RNN, has loops inside it to persist the information over timestamp t. This looping preserves the information over the sequence."
      ],
      "metadata": {
        "id": "9EdoHDLRsC_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/FNN-RNN.png\" width=\"500\">"
      ],
      "metadata": {
        "id": "wYP4GSJpj4Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Structure of RNN"
      ],
      "metadata": {
        "id": "WRRo4fLIGilr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s dig deeper to understand what is happening under the hood. An RNN consists of four different parts:\n",
        "1. Parameters (This includes the weights and biases of the input-to-hidden layer, the hidden-to-hidden layer, and the hidden-to-output layer.)\n",
        "> *The hidden state is used to capture the information from the previous time steps, but this information is not relevant after the training process is finished. Therefore, resetting the hidden state parameters to zero ensures that the network starts with a clean slate for making predictions on new, unseen data.*\n",
        "\n",
        "2. The hidden state (also known as the context state)\n",
        "> *you can think of the hidden state as representing the \"memory\" of the network, which is updated at each time step and used to produce the output.*\n",
        "\n",
        "\n",
        "3. Non-Linear part (Activation Function (Tanh))\n",
        "> As you can see in the equation above, you feed in both input vector Xt and the previous state ht-1 into the function. Here you’ll have 2 separate weight matrices then apply the Non-linearity (tanh) to the sum of input Xt and previous state ht-1 after multiplication to these 2 weight matrices. \n",
        "<img src=\"https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/sequence-2.png\" width=\"300\">\n",
        "\n",
        "\n",
        "4. Linear part: Finally, you’ll have the output vector ŷt at the timestamp t.\n",
        "> <img src=\"https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/sequence-3.png\" width=\"150\"> \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zFlqx_2isQAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN code implementation"
      ],
      "metadata": {
        "id": "lyzMY5Q8sTWK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8eXPICSmvgdP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pylab as pl\n",
        "import torch.nn.init as init\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Recurrent Neural Networks (RNNs), the terms \"input size\", \"hidden size\", and \"number of outputs\" refer to the following:\n",
        "> Input size: Refers to the number of features in a single input sample. For example, if the input is a one-hot encoded word, the input size would be the number of unique words in the vocabulary.\n",
        "\n",
        "> Hidden size: Refers to the number of neurons in the hidden layer. The hidden state of the RNN at each time step is represented by this layer, which helps to capture information from the past time steps.\n",
        "\n",
        "> Number of outputs: Refers to the number of outputs generated by the RNN. This could be one output for a simple prediction problem, or multiple outputs for a multi-task prediction problem.\n",
        "\n",
        "Note that these hyperparameters need to be set prior to training the RNN and their choice can affect the model's performance."
      ],
      "metadata": {
        "id": "L5175gDHG1lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "hidden_size = 1\n",
        "output_size = 1\n",
        "\n",
        "x = Variable(torch.tensor([[0.1], [0.2], [0.3], [0.4]]).type(torch.FloatTensor), requires_grad=False)\n",
        "y = Variable(torch.tensor([[0.2], [0.3], [0.4], [0.5]]).type(torch.FloatTensor), requires_grad=False)"
      ],
      "metadata": {
        "id": "BI3NfaS_vyPi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"input to hidden\" weights are the connections or weights between the input layer and the hidden layer, and these connections allow the network to learn how to propagate information from the input to the hidden state.\n",
        "\n",
        "The \"hidden to output\" weights are the connections or weights between the hidden layer and the output layer, and these connections allow the network to learn how to produce the final output based on the hidden state."
      ],
      "metadata": {
        "id": "ZfNYUio8H_gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize the weights matrix\n",
        "# Initialize (the input to hidden) and (the hidden to hidden) weights matrix with the given input size, hidden size, and output size.\n",
        "Wi2h = torch.FloatTensor(input_size + hidden_size, hidden_size).type(torch.FloatTensor)\n",
        "Wh2h = torch.FloatTensor(hidden_size, output_size).type(torch.FloatTensor)\n",
        "\n",
        "# 2. Initialize the values of the weights matrix\n",
        "# Initialize the values of (the input to hidden) and (the hidden to hidden) weights with a normal distribution centered at 0.0 and with a standard deviation of 0.4.\n",
        "init.normal(Wi2h, 0.0, 0.4)\n",
        "init.normal(Wh2h, 0.0, 0.3)\n",
        "\n",
        "# 3. Create a PyTorch Variable of the weights matrix\n",
        "# Convert (the input to hidden) and (the hidden to hidden) weights to a PyTorch Variable with requires_grad=True, indicating that this tensor requires gradient computation.\n",
        "Wi2h = Variable(Wi2h, requires_grad = True)\n",
        "Wh2h = Variable(Wh2h, requires_grad = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e24P8JkoYrIY",
        "outputId": "4768cbb6-358d-43a1-83d3-a530ca209947"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-2c2f0c298745>:8: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  init.normal(Wi2h, 0.0, 0.4)\n",
            "<ipython-input-4-2c2f0c298745>:9: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  init.normal(Wh2h, 0.0, 0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The input to hidden weights matrix values: \\n', Wi2h)\n",
        "print('The hidden to hidden weights matrix value: \\n', Wh2h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nx2itVm1xV6",
        "outputId": "a64b1f1a-6218-4659-dd83-9258df8960e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input to hidden weights matrix values: \n",
            " tensor([[-0.4114],\n",
            "        [-0.2442]], requires_grad=True)\n",
            "The hidden to hidden weights matrix value: \n",
            " tensor([[0.1379]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The input to hidden weights matrix shape: ', Wi2h.shape)\n",
        "print('The hidden to hidden weights matrix shape: ', Wh2h.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czAyPsgsazQO",
        "outputId": "0baa7029-dc90-4df8-fba0-369981aceb0c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input to hidden weights matrix shape:  torch.Size([2, 1])\n",
            "The hidden to hidden weights matrix shape:  torch.Size([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward pass is the process of computing the output for a given input sequence. The forward pass starts by initializing the hidden state of the RNN with a zero vector or some other randomly generated values."
      ],
      "metadata": {
        "id": "hUyMWlphcdmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the forward pass we understood how the inputs and the hidden states interact with the weights and biases of the recurrent layers and how to use the information contained in the last hidden state to predict the next time step value.\n"
      ],
      "metadata": {
        "id": "AcOyCY_b_Ji4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Creating a BackForwardNetwork\n",
        "def backforward(input_size, context_state, Wi2h, Wh2h):\n",
        "   xh = torch.cat((input_size, context_state), 1)\n",
        "   context_state = torch.tanh(xh.mm(Wi2h))\n",
        "   out = context_state.mm(Wh2h)\n",
        "   return (out, context_state)"
      ],
      "metadata": {
        "id": "PueQsehWwesq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNs use feedback connections that allow information to be passed from one step of the sequence to the next, allowing the network to maintain and update an internal state that depends on the past input. This enables RNNs to capture and understand the dependencies and patterns in the sequence data, making them well suited for tasks such as natural language processing and time series analysis."
      ],
      "metadata": {
        "id": "gn7f3NVGwOt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/rnn_timestamps.gif\" width=\"700\">"
      ],
      "metadata": {
        "id": "fjIw-SzZBD4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The backward pass is just the application of the chain rule from the loss gradient with respect to the predictions until it becomes with respect to the parameters we want to optimize."
      ],
      "metadata": {
        "id": "ss-PWvFz_ZTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/understanding-gradient-descent.png\" width=\"400\">"
      ],
      "metadata": {
        "id": "Ecjc8TjoDy2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing Hyperparameters\n",
        "epochs = 300\n",
        "seq_length = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Loop over the number of epochs\n",
        "for i in range(epochs):\n",
        "   total_loss = 0\n",
        "   # Initialize the context state as a variable with shape (1, hidden_size) and set requires_grad to True\n",
        "   context_state = Variable(torch.zeros((1, hidden_size)).type(torch.FloatTensor), requires_grad = True)\n",
        "   for j in range(x.size(0)):\n",
        "      # Get the input and target at position j\n",
        "      input = x[j:(j+1)]\n",
        "      target = y[j:(j+1)]\n",
        "\n",
        "      # Run the forward pass\n",
        "      (pred, context_state) = backforward(input, context_state, Wi2h, Wh2h)\n",
        "\n",
        "      # 2. Network Evaluation\n",
        "      loss = (pred - target).pow(2).sum()/2\n",
        "      total_loss += loss\n",
        "\n",
        "      # 3. Gradient Calculation\n",
        "      loss.backward()\n",
        "\n",
        "      # 4. Back Propagation\n",
        "      Wi2h.data -= lr * Wi2h.grad.data\n",
        "      Wh2h.data -= lr * Wh2h.grad.data\n",
        "      Wi2h.grad.data.zero_()\n",
        "      Wh2h.grad.data.zero_()\n",
        "      context_state = Variable(context_state.data)\n",
        "  \n",
        "  \n",
        "   # Print the loss every 10 epochs\n",
        "   if i % 10 == 0:\n",
        "      print(\"Epoch: {} loss {}\".format(i, total_loss.data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm9nrCqEwhRX",
        "outputId": "6339c08b-fd23-4c56-b81c-6dc3d2f5e729"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 loss 0.28900742530822754\n",
            "Epoch: 10 loss 0.2695860266685486\n",
            "Epoch: 20 loss 0.25148341059684753\n",
            "Epoch: 30 loss 0.22564542293548584\n",
            "Epoch: 40 loss 0.18438762426376343\n",
            "Epoch: 50 loss 0.12606069445610046\n",
            "Epoch: 60 loss 0.06511279940605164\n",
            "Epoch: 70 loss 0.024737736210227013\n",
            "Epoch: 80 loss 0.008579584769904613\n",
            "Epoch: 90 loss 0.004249237943440676\n",
            "Epoch: 100 loss 0.0033112994860857725\n",
            "Epoch: 110 loss 0.0031000690069049597\n",
            "Epoch: 120 loss 0.003025184152647853\n",
            "Epoch: 130 loss 0.0029751500114798546\n",
            "Epoch: 140 loss 0.0029301573522388935\n",
            "Epoch: 150 loss 0.0028867090586572886\n",
            "Epoch: 160 loss 0.002844155766069889\n",
            "Epoch: 170 loss 0.0028023552149534225\n",
            "Epoch: 180 loss 0.0027612620033323765\n",
            "Epoch: 190 loss 0.0027208528481423855\n",
            "Epoch: 200 loss 0.0026811182033270597\n",
            "Epoch: 210 loss 0.0026420310605317354\n",
            "Epoch: 220 loss 0.0026035942137241364\n",
            "Epoch: 230 loss 0.0025657888036221266\n",
            "Epoch: 240 loss 0.002528602024540305\n",
            "Epoch: 250 loss 0.0024920266587287188\n",
            "Epoch: 260 loss 0.0024560478050261736\n",
            "Epoch: 270 loss 0.0024206575471907854\n",
            "Epoch: 280 loss 0.002385844709351659\n",
            "Epoch: 290 loss 0.002351601142436266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hidden state is used to capture the information from the previous time steps, but this information is not relevant after the training process is finished. Therefore, resetting the hidden state to zero ensures that the network starts with a clean slate for making predictions on new, unseen data.\n"
      ],
      "metadata": {
        "id": "9gCpg1IMx3cF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/rnn-shorttermmemory.gif\" width=\"400\">"
      ],
      "metadata": {
        "id": "Zv93vePiy-CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_state = Variable(torch.zeros((1, hidden_size)).type(torch.FloatTensor), requires_grad = False)\n",
        "predictions = []\n",
        "\n",
        "for i in range(x.size(0)):\n",
        "   input = x[i:i+1]\n",
        "   (pred, context_state) = backforward(input, context_state, Wi2h, Wh2h)\n",
        "   context_state = context_state\n",
        "   predictions.append(pred.data.numpy().ravel()[0])"
      ],
      "metadata": {
        "id": "8buG1LjToPZq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_time_steps = np.linspace(2, 10, x.shape[0])\n",
        "\n",
        "pl.scatter(data_time_steps[:], y.data.numpy(), s = 90, label = \"Actual\")\n",
        "pl.scatter(data_time_steps[:], predictions, label = \"Predicted\")\n",
        "pl.legend()\n",
        "pl.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "R9o1xLH5wkFB",
        "outputId": "0e96d93b-2348-4b9a-f290-ffbc1e2271b4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF6jKbXtxwgM",
        "outputId": "94f919dc-dc35-4da8-c9cc-f330552f9ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.13133748, 0.27376682, 0.40610337, 0.52211994]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asdTgR00wsIn",
        "outputId": "58498c70-65a4-464e-c193-6837cb7d6564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2000],\n",
              "        [0.3000],\n",
              "        [0.4000],\n",
              "        [0.5000]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which parameters have been trained and should be saved?"
      ],
      "metadata": {
        "id": "Qg9pMoqr2OE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"input to hidden\" as Wi2h and the \"hidden to hidden\" as Wh2h weights are learned during the training process through backpropagation and gradient descent optimization algorithms, and they determine the strength of the relationships between the input, hidden, and output layers."
      ],
      "metadata": {
        "id": "qVNe2og85Mxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Wi2h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QALKKMNw1l3j",
        "outputId": "b64e372c-1afe-4441-bc76-c38f94bd6e3b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.2335],\n",
              "        [-0.0039]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Wh2h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asfw-zXY1qiF",
        "outputId": "b12f5e41-caeb-486c-d548-38cd861ded03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.1433]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does PyTorch help to implement an RNN?\n"
      ],
      "metadata": {
        "id": "z1NQv-uxetdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement this code using PyTorch!"
      ],
      "metadata": {
        "id": "RK3YjT2uggx-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FHf9wC3T0R7Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pylab as pl\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input size, hidden size and number of outputs\n",
        "\n",
        "input_size = 1\n",
        "hidden_size = 6\n",
        "output_size = 1\n",
        "\n",
        "epochs = 1000\n",
        "seq_length = 2\n",
        "lr = 0.3\n",
        "\n",
        "\n",
        "x = Variable(torch.tensor([[0.1], [0.2], [0.3], [0.4]]).type(torch.FloatTensor), requires_grad=False)\n",
        "y = Variable(torch.tensor([[0.2], [0.3], [0.4], [0.5]]).type(torch.FloatTensor), requires_grad=False)"
      ],
      "metadata": {
        "id": "d_3Uyse-0R7Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Creating an RNN model\n",
        "rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "# Initialize the output layer\n",
        "fc = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "2qQdk16m2hyW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the parameters of the LSTM layer and linear layer\n",
        "params = list(rnn.parameters()) + list(fc.parameters())\n",
        "\n",
        "# Print the number of parameters\n",
        "print(\"Number of parameters:\", sum(p.numel() for p in params))\n",
        "\n",
        "# Print the shapes of the parameters\n",
        "for name, param in rnn.named_parameters():\n",
        "    print(\"Name: \", name)\n",
        "    print(\"shape: \", param.shape)\n",
        "    print(\"Weight: \", param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zWfVNQfEU3o",
        "outputId": "08f80deb-49a0-496b-f1f5-0bd09c9987cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 61\n",
            "Name:  weight_ih_l0\n",
            "shape:  torch.Size([6, 1])\n",
            "Weight:  tensor([[ 0.1095],\n",
            "        [-0.0109],\n",
            "        [ 0.3033],\n",
            "        [-0.1778],\n",
            "        [ 0.2507],\n",
            "        [ 0.2543]])\n",
            "Name:  weight_hh_l0\n",
            "shape:  torch.Size([6, 6])\n",
            "Weight:  tensor([[ 0.3807, -0.3095, -0.0959,  0.1095, -0.0634,  0.3137],\n",
            "        [ 0.3354, -0.3106, -0.2452, -0.2428, -0.3147,  0.3305],\n",
            "        [ 0.1869, -0.3027,  0.0682, -0.1869, -0.2878,  0.3585],\n",
            "        [ 0.1055,  0.3396, -0.0367, -0.2409,  0.3100,  0.2135],\n",
            "        [ 0.2202,  0.3938,  0.4066, -0.1723, -0.1880, -0.0971],\n",
            "        [ 0.2205,  0.2055,  0.2710, -0.2821, -0.2943,  0.2258]])\n",
            "Name:  bias_ih_l0\n",
            "shape:  torch.Size([6])\n",
            "Weight:  tensor([ 0.2960, -0.3088,  0.1785, -0.0809, -0.3981,  0.0550])\n",
            "Name:  bias_hh_l0\n",
            "shape:  torch.Size([6])\n",
            "Weight:  tensor([ 0.0357,  0.0168, -0.3519,  0.1929, -0.0561,  0.1041])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "   total_loss = 0\n",
        "   \n",
        "   # Initialize the hidden state\n",
        "   h0 = torch.zeros(1,  hidden_size)\n",
        "   for j in range(x.size(0)):\n",
        "      input = x[j:(j+1)]\n",
        "      target = y[j:(j+1)]\n",
        "\n",
        "      # Forward pass\n",
        "      y_pred, hn = rnn(input, h0)\n",
        "\n",
        "      # 2. Model Evaluation\n",
        "      y_pred = fc(hn.squeeze(0))\n",
        "      loss = criterion(y_pred.view(-1), target)\n",
        "    \n",
        "      # 3. Gradient Calculation\n",
        "      optimizer.zero_grad()\n",
        "      total_loss += loss\n",
        "      loss.backward()\n",
        "\n",
        "      # 4. Back Propagation\n",
        "      optimizer.step()\n",
        "   # display loss \n",
        "   if i % 10 == 0:\n",
        "      print(\"Epoch: {} loss {}\".format(i, total_loss.data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b577342-b1c3-47d8-bbba-351ff2b1e2ce",
        "id": "NmGOaqWb0R7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 loss 0.0629827082157135\n",
            "Epoch: 10 loss 0.053693998605012894\n",
            "Epoch: 20 loss 0.03975199908018112\n",
            "Epoch: 30 loss 0.029730292037129402\n",
            "Epoch: 40 loss 0.022304333746433258\n",
            "Epoch: 50 loss 0.016711654141545296\n",
            "Epoch: 60 loss 0.012474830262362957\n",
            "Epoch: 70 loss 0.009266851469874382\n",
            "Epoch: 80 loss 0.006847819313406944\n",
            "Epoch: 90 loss 0.005034207366406918\n",
            "Epoch: 100 loss 0.003683088580146432\n",
            "Epoch: 110 loss 0.002682790858671069\n",
            "Epoch: 120 loss 0.0019465357763692737\n",
            "Epoch: 130 loss 0.0014074660139158368\n",
            "Epoch: 140 loss 0.0010146060958504677\n",
            "Epoch: 150 loss 0.0007294547976925969\n",
            "Epoch: 160 loss 0.0005232124822214246\n",
            "Epoch: 170 loss 0.0003744936839211732\n",
            "Epoch: 180 loss 0.0002675412979442626\n",
            "Epoch: 190 loss 0.0001908064296003431\n",
            "Epoch: 200 loss 0.0001358667213935405\n",
            "Epoch: 210 loss 9.660812793299556e-05\n",
            "Epoch: 220 loss 6.860344728920609e-05\n",
            "Epoch: 230 loss 4.866190647589974e-05\n",
            "Epoch: 240 loss 3.4486256481613964e-05\n",
            "Epoch: 250 loss 2.442705590510741e-05\n",
            "Epoch: 260 loss 1.730179246806074e-05\n",
            "Epoch: 270 loss 1.2265370969544165e-05\n",
            "Epoch: 280 loss 8.71319389261771e-06\n",
            "Epoch: 290 loss 6.214591849129647e-06\n",
            "Epoch: 300 loss 4.4621870074479375e-06\n",
            "Epoch: 310 loss 3.2374282454838976e-06\n",
            "Epoch: 320 loss 2.3854561277403263e-06\n",
            "Epoch: 330 loss 1.79546032086364e-06\n",
            "Epoch: 340 loss 1.3898199995310279e-06\n",
            "Epoch: 350 loss 1.1131218116133823e-06\n",
            "Epoch: 360 loss 9.262158755518612e-07\n",
            "Epoch: 370 loss 8.018665766940103e-07\n",
            "Epoch: 380 loss 7.204758958323509e-07\n",
            "Epoch: 390 loss 6.68631855660351e-07\n",
            "Epoch: 400 loss 6.368059075612109e-07\n",
            "Epoch: 410 loss 6.184094445416122e-07\n",
            "Epoch: 420 loss 6.089235853323771e-07\n",
            "Epoch: 430 loss 6.051317313904292e-07\n",
            "Epoch: 440 loss 6.049024818821636e-07\n",
            "Epoch: 450 loss 6.06777575740125e-07\n",
            "Epoch: 460 loss 6.098040330471122e-07\n",
            "Epoch: 470 loss 6.134632712928578e-07\n",
            "Epoch: 480 loss 6.171388235998165e-07\n",
            "Epoch: 490 loss 6.206887519510929e-07\n",
            "Epoch: 500 loss 6.240044854166626e-07\n",
            "Epoch: 510 loss 6.270637413763325e-07\n",
            "Epoch: 520 loss 6.296085075518931e-07\n",
            "Epoch: 530 loss 6.318083478618064e-07\n",
            "Epoch: 540 loss 6.337374429676856e-07\n",
            "Epoch: 550 loss 6.353089361255115e-07\n",
            "Epoch: 560 loss 6.365621061377169e-07\n",
            "Epoch: 570 loss 6.376333203661488e-07\n",
            "Epoch: 580 loss 6.383517074937117e-07\n",
            "Epoch: 590 loss 6.388562496795203e-07\n",
            "Epoch: 600 loss 6.392370437424688e-07\n",
            "Epoch: 610 loss 6.394839147105813e-07\n",
            "Epoch: 620 loss 6.39635118204751e-07\n",
            "Epoch: 630 loss 6.396003868758271e-07\n",
            "Epoch: 640 loss 6.394952265509346e-07\n",
            "Epoch: 650 loss 6.392342584149446e-07\n",
            "Epoch: 660 loss 6.38961296317575e-07\n",
            "Epoch: 670 loss 6.386921427292691e-07\n",
            "Epoch: 680 loss 6.382457513609552e-07\n",
            "Epoch: 690 loss 6.378057264555537e-07\n",
            "Epoch: 700 loss 6.373370524670463e-07\n",
            "Epoch: 710 loss 6.367798732753727e-07\n",
            "Epoch: 720 loss 6.363601414705045e-07\n",
            "Epoch: 730 loss 6.358003474815632e-07\n",
            "Epoch: 740 loss 6.352248647090164e-07\n",
            "Epoch: 750 loss 6.34642674413044e-07\n",
            "Epoch: 760 loss 6.341316520774853e-07\n",
            "Epoch: 770 loss 6.334684599096363e-07\n",
            "Epoch: 780 loss 6.328459676296916e-07\n",
            "Epoch: 790 loss 6.323280103970319e-07\n",
            "Epoch: 800 loss 6.316492999758339e-07\n",
            "Epoch: 810 loss 6.311774995992891e-07\n",
            "Epoch: 820 loss 6.30577631000051e-07\n",
            "Epoch: 830 loss 6.299410415522289e-07\n",
            "Epoch: 840 loss 6.293503247434273e-07\n",
            "Epoch: 850 loss 6.287197038545855e-07\n",
            "Epoch: 860 loss 6.281157425291894e-07\n",
            "Epoch: 870 loss 6.273855319705035e-07\n",
            "Epoch: 880 loss 6.26846656359703e-07\n",
            "Epoch: 890 loss 6.262250167310413e-07\n",
            "Epoch: 900 loss 6.254022082430311e-07\n",
            "Epoch: 910 loss 6.248798172237002e-07\n",
            "Epoch: 920 loss 6.242985364224296e-07\n",
            "Epoch: 930 loss 6.237008847165271e-07\n",
            "Epoch: 940 loss 6.231465476957965e-07\n",
            "Epoch: 950 loss 6.225061497389106e-07\n",
            "Epoch: 960 loss 6.218829184945207e-07\n",
            "Epoch: 970 loss 6.212551397766219e-07\n",
            "Epoch: 980 loss 6.206143439158041e-07\n",
            "Epoch: 990 loss 6.199873610057693e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for i in range(x.size(0)):\n",
        "   input = x[i:i+1]\n",
        "   # Forward pass\n",
        "   out, hn = rnn(input, h0)\n",
        "   # Pass the hidden state through the output layer\n",
        "   y_pred = fc(hn.squeeze(0))\n",
        "   predictions.append(y_pred.data.numpy().ravel()[0])"
      ],
      "metadata": {
        "id": "knhdQPnB3-4S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_time_steps = np.linspace(2, 10, x.shape[0])\n",
        "\n",
        "pl.scatter(data_time_steps[:], y.data.numpy(), s = 90, label = \"Actual\")\n",
        "pl.scatter(data_time_steps[:], predictions, label = \"Predicted\")\n",
        "pl.legend()\n",
        "pl.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "9b03914a-c427-4ff5-847a-ba115eafcfec",
        "id": "Lt0EG_uf0R7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the parameters of the LSTM layer and linear layer\n",
        "params = list(rnn.parameters()) + list(fc.parameters())\n",
        "\n",
        "# Print the number of parameters\n",
        "print(\"Number of parameters:\", sum(p.numel() for p in params))\n",
        "\n",
        "# Print the shapes of the parameters\n",
        "for name, param in rnn.named_parameters():\n",
        "    print(\"Name:\", name)\n",
        "    print(\"Shape:\", param.shape)\n",
        "    print(\"Weight::\", param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWM1b9iKMkKe",
        "outputId": "ef670beb-f945-4fd5-f0dd-bcc41c21c6a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 61\n",
            "Name: weight_ih_l0\n",
            "Shape: torch.Size([6, 1])\n",
            "Weight:: tensor([[-0.3860],\n",
            "        [-0.5892],\n",
            "        [ 1.0938],\n",
            "        [-0.5653],\n",
            "        [-0.4973],\n",
            "        [ 0.5610]])\n",
            "Name: weight_hh_l0\n",
            "Shape: torch.Size([6, 6])\n",
            "Weight:: tensor([[ 0.3807, -0.3095, -0.0959,  0.1095, -0.0634,  0.3137],\n",
            "        [ 0.3354, -0.3106, -0.2452, -0.2428, -0.3147,  0.3305],\n",
            "        [ 0.1869, -0.3027,  0.0682, -0.1869, -0.2878,  0.3585],\n",
            "        [ 0.1055,  0.3396, -0.0367, -0.2409,  0.3100,  0.2135],\n",
            "        [ 0.2202,  0.3938,  0.4066, -0.1723, -0.1880, -0.0971],\n",
            "        [ 0.2205,  0.2055,  0.2710, -0.2821, -0.2943,  0.2258]])\n",
            "Name: bias_ih_l0\n",
            "Shape: torch.Size([6])\n",
            "Weight:: tensor([ 0.3633, -0.1651,  0.1196, -0.0354, -0.2758, -0.0334])\n",
            "Name: bias_hh_l0\n",
            "Shape: torch.Size([6])\n",
            "Weight:: tensor([ 0.1030,  0.1605, -0.4108,  0.2384,  0.0662,  0.0156])\n"
          ]
        }
      ]
    }
  ]
}