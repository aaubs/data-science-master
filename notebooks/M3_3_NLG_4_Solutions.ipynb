{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M3_3_NLG_4_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs in Harry Potter: Tom Riddle introduces himself to Harry Potter"
      ],
      "metadata": {
        "id": "40SgB408bPei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "\n",
        "<iframe width=\"966\" height=\"543\" src=\"https://www.youtube.com/embed/eh4b5zC0sB4\" title=\"Tom Riddle introduces himself to Harry Potter | Harry Potter and the Chamber of Secrets\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "mfWa-EMSBI_n",
        "outputId": "6fb09ce5-2ac0-4e49-f790-f64575a469c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"966\" height=\"543\" src=\"https://www.youtube.com/embed/eh4b5zC0sB4\" title=\"Tom Riddle introduces himself to Harry Potter | Harry Potter and the Chamber of Secrets\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The brief history of LLMs"
      ],
      "metadata": {
        "id": "-oUr9P2vdLAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYNNKmmi1ShV7xx76qtXww.png)"
      ],
      "metadata": {
        "id": "haVnsKCRa5I6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9H2ALEdG7BT"
      },
      "source": [
        "### Summary of Transformer Architecture in Foundation Models\n",
        "\n",
        "- **Introduction**: Introduced in 2017, transformers have advanced beyond Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks. Key in natural language processing and computer vision.\n",
        "\n",
        "- **Features**:\n",
        "  - **Self-Attention Mechanism**: Weighs significance of different input data parts.\n",
        "  - **Parallel Processing**: Enhances performance and scalability.\n",
        "  - **Bidirectionality**: Improves understanding of ambiguous words and coreferences.\n",
        "\n",
        "- **Components**:\n",
        "  - Original architecture: Encoder and Decoder.\n",
        "  - Variations: BERT (encoders only), GPT (decoders only).\n",
        "\n",
        "Transformers represent a significant leap in deep learning, enabling more efficient and effective data processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcZGualm4foL"
      },
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/TM_EvolutionaryTree.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZT04qxrNzR6"
      },
      "source": [
        "### Roles of Encoders and Decoders in Foundation Models: BERT and GPT\n",
        "The excerpt outlines the roles of encoders and decoders in foundation models, specifically focusing on BERT and GPT:\n",
        "- **Encoders and Decoders**:\n",
        "  - **BERT**: Utilizes only encoders.\n",
        "  - **GPT**: Exclusively employs decoders.\n",
        "  - Both models are proficient in understanding language, syntax, and semantics, with GPT's larger scale model (billions of parameters) excelling in these areas.\n",
        "\n",
        "- **Applications**:\n",
        "  - **BERT (Encoder)**: Ideal for classification (e.g., sentiment analysis), question-answering, summarization, and named entity recognition.\n",
        "  - **GPT (Decoder)**: Specializes in translation and content generation (e.g., stories).\n",
        "\n",
        "- **Outputs**:\n",
        "  - **BERT**: Produces embeddings representing words with context-specific attention information.\n",
        "  - **GPT**: Generates next-word predictions along with their probabilities.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT - Encoders\n",
        "\n",
        "- **Transformer Encoder Usage**:\n",
        "  - BERT employs the encoder part of the transformer architecture.\n",
        "  - It focuses on understanding semantic and syntactic language information.\n",
        "  - BERT's output consists of embeddings, rather than predicted next words.\n",
        "\n",
        "- **Application of Embeddings**:\n",
        "  - To use these embeddings, additional layers (e.g., for text classification or questions and answers) need to be added on top of BERT.\n",
        "\n",
        "- **Training Technique**:\n",
        "  - BERT utilizes a unique training method to circumvent the need for expensive labeled data.\n",
        "  - This involves using a technique called Self-Supervised Learning, particularly effective with large data volumes.\n",
        "\n",
        "- **Masked Language Model**:\n",
        "  - In BERTâ€™s training, sentences are altered by masking words.\n",
        "  - Example: **\"Sarah went to a restaurant to meet her friend that night.\"** becomes **\"Sarah went to a restaurant to meet her MASK that night.\"**\n",
        "  - These masks help BERT generate its own labeled data from originally unlabeled data.\n",
        "  - Each masked word prediction is informed by the other tokens in the sentence, which is processed by the encoder, eliminating the need for a decoder.\n",
        "\n"
      ],
      "metadata": {
        "id": "KD1mVKqcN_CI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT - Decoders\n",
        "\n",
        "- **Role in Language Processing**:\n",
        "  - Decoders are primarily used in generating next words in language tasks such as text translation or story generation.\n",
        "  - The outputs from decoders are words accompanied by their respective probabilities.\n",
        "\n",
        "- **Use of Attention Mechanisms**:\n",
        "  - Decoders implement the attention concept twice during the model training process.\n",
        "  - Initially, they use Masked Multi-Head Attention, similar to BERT's MASK concept. Here, only the initial words of a target sentence are shown to the model to facilitate learning without 'cheating'.\n",
        "  - Subsequently, the decoder employs Multi-Head Attention, akin to what is used in encoders.\n",
        "\n",
        "- **Interaction with Encoders in Transformer Models**:\n",
        "  - In models combining encoders and decoders, there's a technique where the encoder's output (keys and values) is fed into the decoders.\n",
        "  - Decoders use queries to find relevant keys, aiding in tasks like understanding and translating sentences, even with varying word counts and order.\n",
        "\n",
        "- **GPT's Unique Approach**:\n",
        "  - GPT deviates from this technique by using only a decoder.\n",
        "  - Trained on massive datasets (Large Language Model), GPT's knowledge is embedded in billions of parameters.\n",
        "  - This extensive training compensates for the absence of an encoder, embedding equivalent knowledge within the decoder.\n",
        "\n",
        "- **Evolution in ChatGPT**:\n",
        "  - ChatGPT has advanced these techniques, incorporating human-labeled data to mitigate issues like hate speech and abuse.\n",
        "  - It also uses Reinforcement Learning to enhance model quality, as seen in \"ChatGPT: Optimizing Language Models for Dialogue\".\n",
        "\n"
      ],
      "metadata": {
        "id": "5iRfDIprOwqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Smart Are They? Understanding the Scale of GPT-3 and GPT-4\n"
      ],
      "metadata": {
        "id": "J3ZgnpAvZ-_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Assumption                                  | Description                                                                                       |\n",
        "|---------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **Average Tokens per Book**                 | Estimated at 135,000 tokens per book, based on an average book length of 80,000 to 100,000 words.  |\n",
        "| **Average Reading Lifetime of an Individual** | Estimated at 510 books per lifetime, assuming a moderate reading habit of 5-12 books per year over 60 years. |\n",
        "| **Tokens per Word**                         | Estimated at 1.5 tokens per word, accounting for spaces and punctuation.                          |\n",
        "\n"
      ],
      "metadata": {
        "id": "xE_UeBjWZlQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Detail                             | GPT-3                                   | GPT-4                                   |\n",
        "|------------------------------------|-----------------------------------------|-----------------------------------------|\n",
        "| **Developed By**                   | OpenAI                                  | OpenAI                                  |\n",
        "| **Approximate Training Data Size** | 45 terabytes of text data               | Larger than GPT-3 (exact size unknown)  |\n",
        "| **Estimated Token Count**          | 300-400 billion tokens                  | Likely over 500 billion tokens          |\n",
        "| **Equivalent Number of Books**     | 2,222,222 - 2,962,963 books             | >3,703,704 books                        |\n",
        "| **Equivalent Knowledge of People** | 4,356 - 5,810 people                    | >7,263 people                           |\n"
      ],
      "metadata": {
        "id": "vxvz6Gw_ZjY7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j431XmcTOzCY"
      },
      "source": [
        "# Why adapt the language model?\n",
        "\n",
        "- LMs are trained in a task-agnostic way.\n",
        "- Downstream tasks can be very different from language modeling on the Pile.\n",
        "For example, consider the natural language inference (NLI) task (is the hypothesis entailed by the premise?):\n",
        "\n",
        "      Premise: I have never seen an apple that is not red.\n",
        "      Hypothesis: I have never seen an apple.\n",
        "      Correct output: Not entailment (the reverse direction would be entailment)\n",
        "\n",
        "- The format of such a task may not be very natural for the model.\n",
        "\n",
        "# Ways downstream tasks can be different\n",
        "\n",
        "- **Formatting**: for example, NLI takes in two sentences and compares them to produce a single binary output. This is different from generating the next token or filling in MASKs. Another example is the presence of MASK tokens in BERT training vs. no MASKs in downstream tasks.\n",
        "- **Topic shift**: the downstream task is focused on a new or very specific topic (e.g., medical records)\n",
        "- **Temporal shift**: the downstream task requires new knowledge that is unavailable during pre-training because 1) the knowledge is new (e.g., GPT3 was trained before Biden became President), 2) the knowledge for the downstream task is not publicly available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGxCwH7-L2q"
      },
      "source": [
        "\n",
        "# Optimizing Large Language Models\n",
        "\n",
        "There are several options to optimize Large Language Models:\n",
        "\n",
        "    Prompt engineering by providing samples (In-Context Learning)\n",
        "    Prompt Tuning\n",
        "    Fine-Tuning\n",
        "       - Classic fine-tuning by changing all weights\n",
        "       - Transfer Learning - PEFT fine-tuning by changing only a few weights\n",
        "       - Reinforcement Learning Human Feedback (RLHF)\n",
        "\n",
        "An important question is which of these options is the most effective one and which one can overwrite previous optimizations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Prompt Engineering, Prompt Tuning, and PEFT\n",
        "These techniques are essential for efficiently adapting large, pre-trained models like GPT or BERT to specialized tasks or domains, optimizing resource usage and reducing training time.\n",
        "\n",
        "\n",
        "1. **Prompt Engineering (In-Context Learning)**:\n",
        "   - **Definition**: Crafting input prompts to guide a Large Language Model (LLM) for desired outputs.\n",
        "   - **Application**: Uses natural language prompts to \"program\" the LLM, leveraging its contextual understanding.\n",
        "   - **Model Change**: No alteration to the model's parameters; relies on the model's existing knowledge and interpretive abilities.\n",
        "\n",
        "2. **Prompt Tuning**:\n",
        "   - **Difference from Prompt Engineering**: Involves appending a trainable tensor (prompt tokens) to the LLM's input embeddings.\n",
        "   - **Process**: Fine-tunes this tensor for a specific task and dataset, keeping other model parameters unchanged.\n",
        "   - **Example**: Adapting a general LLM for specific tasks like sentiment classification by adjusting prompt tokens.\n",
        "\n",
        "3. **Parameter-Efficient Fine-Tuning (PEFT)**:\n",
        "   - **Overview**: A set of techniques to enhance model performance on specific tasks or datasets by tuning a small subset of parameters.\n",
        "   - **Objective**: Targeted improvements without the need for full model retraining.\n",
        "   - **Relation to Prompt Tuning**: Prompt tuning is a subset of PEFT, focusing on fine-tuning specific parts of the model for task/domain adaptation.\n",
        "\n"
      ],
      "metadata": {
        "id": "5bJ78Ja2Urh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/PEFT_LLMs.png)"
      ],
      "metadata": {
        "id": "F2alv_qaRfHx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npsUW-pPKYp8"
      },
      "source": [
        "### Challenges\n",
        "\n",
        "Fine-tuning models can certainly help to get models to do what you want them to do. However, there are some potential issues:\n",
        "\n",
        "> - **Catastrophic forgetting**: This phenomenon describes a behavior when fine-tuning or prompts can overwrite the pre-trained model characteristics.\n",
        "> - **Overfitting**: If only a certain AI task has been fine-tuned, other tasks can suffer in terms of performance.\n",
        "\n",
        "In general, fine-tuning should be used wisely and best practices should be applied, for example, the quality of the data is more important than the quantity and multiple AI tasks should be fine-tuned at the same time vs after each other."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications\n",
        "\n",
        "There are four main platforms that can be used for LLMs' applications:\n"
      ],
      "metadata": {
        "id": "FYi_A6S-NHXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain\n",
        "\n",
        "- Overview: LangChain is a versatile framework designed to simplify the utilization of language models across various tasks. It serves as a seamless toolset for connecting different language capabilities.\n",
        "\n",
        "- Basic Usage: After installing LangChain, you can effortlessly import it into your Python script. To begin, initialize the LangChain class and use its methods to interact efficiently with GPT models, streamlining the process of applying language models to a wide array of applications.\n",
        "\n",
        "- Advanced Usage: For advanced users, LangChain offers extensive flexibility. You can customize the underlying language model, integrate external knowledge sources, and combine various language capabilities for tackling complex tasks. This advanced functionality empowers you to develop sophisticated applications harnessing the full potential of language models.\n",
        "\n",
        "### Llama2Index\n",
        "\n",
        "- Overview: Llama2Index is a potent tool designed for indexing and searching large datasets with language models. It simplifies the creation of indexes for your data and enables efficient data retrieval through natural language queries.\n",
        "\n",
        "- Indexing Data: To get started with Llama2Index, prepare your dataset and utilize the tool to create an index. This index serves as the foundation for seamlessly searching through your data using natural language queries, making your dataset easily accessible.\n",
        "\n",
        "- Usage: Once your data is indexed using Llama2Index, you gain the capability to run natural language queries. This enables you to retrieve relevant information from your dataset effortlessly, expanding accessibility to a broad user base.\n",
        "\n",
        "- Advanced Usage: Llama2Index is highly integratable, allowing seamless integration with other applications. This integration empowers them to provide natural language search capabilities, opening up possibilities for incorporating language-based search functionality into diverse software solutions.\n",
        "\n",
        "### Llama.cpp\n",
        "\n",
        "- Overview: Llama.cpp is a high-performance C++ framework tailored for language models, known for its efficiency and speed. It is an excellent choice for developing applications requiring interactions with language models.\n",
        "\n",
        "- Basic Usage: To start with Llama.cpp, you can create a straightforward C++ program that utilizes the framework to interact with a GPT model. This simplicity allows developers to harness the power of language models without unnecessary complexity.\n",
        "\n",
        "- Advanced Usage: For those seeking advanced capabilities, Llama.cpp provides features for optimizing performance in large-scale applications. Additionally, it offers the flexibility to integrate with other C++ projects, expanding the range of applications where language models can be employed effectively.\n",
        "\n",
        "### Cohere\n",
        "\n",
        "- Overview: Cohere is a comprehensive platform offering advanced natural language processing capabilities. It empowers users to leverage the potential of language models across a diverse range of applications. Cohere provides a set of tools and APIs for various language-related tasks, facilitating the development of intelligent and context-aware applications.\n",
        "\n",
        "- Basic Usage: Getting started with Cohere is straightforward. You can seamlessly integrate Cohere's APIs into your applications to perform tasks such as text analysis, sentiment analysis, and language understanding. Cohere's pre-trained models are readily available, enabling you to extract valuable insights from text data effortlessly.\n",
        "\n",
        "- Advanced Features: Cohere offers advanced features for users looking to customize and extend their natural language processing capabilities. You can fine-tune models to suit your specific tasks and integrate external data sources to enhance your applications' knowledge. Cohere's versatility makes it a valuable tool for both basic and complex language-related tasks, whether you are building chatbots, recommendation systems, or content analysis tools. Cohere empowers you to create intelligent and context-aware solutions that effectively understand and respond to human language.\n"
      ],
      "metadata": {
        "id": "IOH3EjAPI5Xj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTdkb3xeFKP"
      },
      "source": [
        "# LangChain\n",
        "\n",
        "    Build simple application with LangChain\n",
        "    Trace your application with LangSmith\n",
        "    Serve your application with LangServe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN5n9sr2eSJX"
      },
      "source": [
        "The simplest and most common chain contains three things:\n",
        "\n",
        "- **Model/Chat (LLM) Wrappers**: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
        "\n",
        "- **Prompt Template**: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
        "\n",
        "- **Memory**: Provides a construct for storing and retrieving messages during a conversation which can be either short term or long term.\n",
        "\n",
        "- **Indexes**: Help LLMs interact with documents by providing a way to structure them. LangChain provides Document Loaders to load documents, Text Splitters to split documents into smaller chunks, Vector Stores to store documents as embeddings, and Retrievers to fetch relevant documents.\n",
        "\n",
        "- **Chain**: Probably the most important component of LangChain is the Chain class. It's a wrapper around the LLM that allows you to create a chain of actions.\n",
        "\n",
        "- **Agents**:: Agents are the most powerful feature of LangChain. They allow you to combine LLMs with external data and tools.\n",
        "\n",
        "- **Callbacks**: Callbacks mechanism allows you to go back to different stages of your LLM application using â€˜callbacksâ€™ argument of the API. It is used for logging, monitoring, streaming etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDXn_5CQeZSU"
      },
      "source": [
        "In this guide we'll cover those three components individually, and then go over how to combine them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMQ38X_A0jbg"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Installing LangChain is easy. You can install it with pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYUgdL8xKXF6",
        "outputId": "5c138caa-9b2d-4096-dd7b-2008c792573d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\n",
            "Wall time: 5.01 Âµs\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xformers 0.0.21 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.33.2 requires tokenizers!=0.11.3,<0.14,>=0.11.1, but you have tokenizers 0.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%time\n",
        "!pip install -Uqqq pip --progress-bar off\n",
        "# !pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.33.2 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.21 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq tokenizers==0.14.0 --progress-bar off\n",
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off\n",
        "!pip install -qqq unstructured==0.10.16 --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyK7F2gr0m3v"
      },
      "source": [
        "Note that we're also installing a few other libraries that we'll be using in this tutorial.\n",
        "\n",
        "## Model (LLM) Wrappers\n",
        "\n",
        "Using Llama 2 is as easy as using any other HuggingFace model. We'll be using the HuggingFacePipeline wrapper (from LangChain) to make it even easier to use. To load the 13B version of the model, we'll use a GPTQ (Generative Pre-trained Transformer Quantization) version of the model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate --q"
      ],
      "metadata": {
        "id": "J06Xo0ItlEaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02956e0f-5258-4b7f-a5ee-2b4a2cef9033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEC0V1DcpG1U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "15b88a733a554e0b850bfa4d076f8641",
            "a9b1880c7a6947bb9ebdc73eb74b1b06",
            "27db62eaf9b34b8eb4c643d9f11c07d3",
            "64e2cd923610449bae1e683ebaaac631",
            "8b241c5621a64a1395bd368245f50dfe",
            "9b505fef3d2844e9bf6e0868b6fa2720",
            "4959cccd73324be8a8ca92693f0e51a2",
            "24c895718c0f450da6be33717053e620",
            "0bbb025b5041415eba8a5a64ed728898",
            "446d91f3847a493396799bc69bc22872",
            "8d9f93becd6040509ac57915b13c30fb",
            "c776dc0ece364e1c9103a99f1e8bf591",
            "23c3e97ef5cb4257a29d1f7def0cdc44",
            "024385c1620f402a95d9babe28d06e9e",
            "c2dcd1598a72483aa2cfdf10b9560e12",
            "3a1ffba80173451f8ffe306d901ce685",
            "b1dae59aa2b347dea02b1f830f0a50c1",
            "b8c3786bfe664a1f85999b81b73f9a8d",
            "3b8035fe5a9b425ca8986b561be9f237",
            "829711b54aed41949a5f3e70e7072fb9",
            "650c97600d22422abc62ca7c8014f8c8",
            "993bc2970f7749429abbce002addb123",
            "09a97a4cb8964628b66b5da51d55a802",
            "b9bbe68bc95c4151b47063db13719721",
            "1b847efdcc13489391f73173c65d64ed",
            "c4e908bb27a144e39cbcc11e2417f50f",
            "f13ebf2b85cd48f5acd974b9614d0e41",
            "66d8316a83d544df96b173a7f841f53f",
            "6c1fd60ce19348ab87ef8622a70d283d",
            "77a38082a7fc470ca638030004d3e4aa",
            "4ceb94508cd64e95ac767c0f69f8f291",
            "42ba4fbbc34e4a35b53ba94f20b133dc",
            "3ce9999e2c1b4353a7eb5ad9fdd461a3",
            "de26e861cfee49e296393519522db28d",
            "ed54e99907d5448da7aaa9771e069fcd",
            "c8f18f6111aa48058472e6cced391528",
            "4d9dc4b5f44c49d18fe5902df7b5b592",
            "9ffe717a554447328bd96dea518d162d",
            "d4589dd45a854cfc99125bbcd6278b62",
            "ebb889a1549346299d73bb3bda3546fd",
            "fc3403ddcc3a462090c310eed6f7af2a",
            "75c03cb5cd3544dfb01bd90eafdede2a",
            "4b848fcec75a43da8af57e53b2dcfe61",
            "da05dd86b7f3493e843f7c5b9bfac17c",
            "bd74a4e1c4da42bc854e197a6d0f45c3",
            "e1b58266ee3c41f68257fc1563daf5bf",
            "ad6d78bfac3d4b77a32c60b40e0373bd",
            "635d64f22c264f5488e964cbcd0a713d",
            "1590cb2f699441d5b85d53ce3f78397a",
            "5479b264afa7468dbde3e23cf8328391",
            "c1669361698848cabb6111a7857404b4",
            "2f51b21525674641bba16c5cdc08ed0d",
            "54e9388f66d947219b798e2aee0a55ed",
            "ec451c0e51c046c484c6a97ec71eb74b",
            "5c29603e772a41e4afc4dcbbc0be538a",
            "38f40d63a8534e7a8964ccdde867cf4e",
            "673ad74686e14f1f98ae3a0859e7b63f",
            "4c552c3bfb884d1c861d89121b1b61d4",
            "fcc8601c3e1945038c2f9f0a28ceb152",
            "a2f77aa9a5ba495f92ec76a98e03234a",
            "646651aafdbe489b95b5bd9c01190558",
            "3e6fa7c9ae674a668c37ca65edeac1e6",
            "b1daf4fe5256485ea39eea467845db0c",
            "ba09bf6b008443a1908805b6fbbf53a7",
            "43c1f2cf101f4fab9fa76572493177eb",
            "92d19ea3294c4423a8f735df654289e2",
            "891a499b3bf44858b32bc5d00665d63c",
            "d675a13ec766459fb91a996255286c44",
            "8e4d2034dbaa40c8938e16abdf7a1996",
            "b9929f8c9ce4450e928db3e36ebd3f01",
            "2be89dbf2c5e4d3db9c7704dbf1c0622",
            "d0ddbe1949de446db1bf7b3d530d3bc8",
            "dd94b1da344d4c5ca0d4a73255b4248a",
            "ffd2a8b21c3b4064953f455c48921a89",
            "87d33c74305d49dfa60c5572ab854250",
            "aa294b25a7764c2ba51999975c8fedb2",
            "da8ed6bcfbe445fa9fd88b2affb253ab"
          ]
        },
        "outputId": "ff45831c-eae9-4ae8-fbc0-7e604204b81e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15b88a733a554e0b850bfa4d076f8641"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c776dc0ece364e1c9103a99f1e8bf591"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09a97a4cb8964628b66b5da51d55a802"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de26e861cfee49e296393519522db28d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd74a4e1c4da42bc854e197a6d0f45c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38f40d63a8534e7a8964ccdde867cf4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "891a499b3bf44858b32bc5d00665d63c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "MODEL_NAME = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Create a configuration for text generation based on the specified model name\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set the maximum number of new tokens in the generated text to 1024.\n",
        "# This limits the length of the generated output to 1024 tokens.\n",
        "generation_config.max_new_tokens = 1024\n",
        "\n",
        "# Set the temperature for text generation. Lower values (e.g., 0.0001) make output more deterministic, following likely predictions.\n",
        "# Higher values make the output more random.\n",
        "generation_config.temperature = 0.0001\n",
        "\n",
        "# Set the top-p sampling value. A value of 0.95 means focusing on the most likely words that make up 95% of the probability distribution.\n",
        "generation_config.top_p = 0.95\n",
        "\n",
        "# Enable text sampling. When set to True, the model randomly selects words based on their probabilities, introducing randomness.\n",
        "generation_config.do_sample = True\n",
        "\n",
        "# Set the repetition penalty. A value of 1.15 discourages the model from repeating the same words or phrases too frequently in the output.\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "\n",
        "# Create a text generation pipeline using the initialized model, tokenizer, and generation configuration\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Create a LangChain pipeline that wraps the text generation pipeline and set a specific temperature for generation\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTQ has been shown to be able to quantize GPTs down to 4-bit weights with minimal loss of accuracy. This means that GPTQs can be run on much smaller and cheaper hardware, such as smartphones and laptops.\n",
        "\n",
        "GPTQ is a promising new technology that could make LLMs more accessible to a wider range of users.\n",
        "\n",
        "Here are some of the benefits of using GPTQ:\n",
        "\n",
        "> - Smaller model size: GPTQ can reduce the model size by up to 90%, without sacrificing too much accuracy. This makes it possible to deploy GPTs on smaller and cheaper hardware.\n",
        "- Faster inference: GPTQ can also speed up inference by up to 4x. This makes it possible to use GPTs in more real-time applications.\n",
        "- Lower power consumption: GPTQ can also reduce power consumption by up to 80%. This makes it possible to use GPTs on battery-powered devices."
      ],
      "metadata": {
        "id": "wX33SZLwWYQs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EdDMkKO0sD4"
      },
      "source": [
        "Good thing is that the transformers library supports loading models in GPTQ format using the AutoGPTQ library. Let's try out our LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2FPyRZsrFE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7765517f-45e6-409c-e992-00df6ae1e127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ChatGPT is an AI-powered chatbot developed by Meta AI that can understand and respond to user input in a conversational manner. Open source LLMs, on the other hand, are language models that are available for anyone to use and modify, with some examples including BERT, RoBERTa, and XLNet. While both types of models have their own strengths and weaknesses, they differ in terms of their architecture, training data, and licensing restrictions.\n"
          ]
        }
      ],
      "source": [
        "result = llm(\n",
        "    \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1:\n",
        "\n",
        "Check the results of different settings for a prompt. You can change temperature, top_p, do_sample, and repetition_penalty in the model configuration and compare the results."
      ],
      "metadata": {
        "id": "9xYlQ4rtbQ20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a configuration for text generation based on the specified model name\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set the maximum number of new tokens in the generated text to 1024.\n",
        "# This limits the length of the generated output to 1024 tokens.\n",
        "generation_config.max_new_tokens = 1024\n",
        "\n",
        "# Set the temperature for text generation. Lower values (e.g., 0.0001) make output more deterministic, following likely predictions.\n",
        "# Higher values make the output more random.\n",
        "generation_config.temperature = 0.1\n",
        "\n",
        "# Set the top-p sampling value. A value of 0.95 means focusing on the most likely words that make up 95% of the probability distribution.\n",
        "generation_config.top_p = 0.95\n",
        "\n",
        "# Enable text sampling. When set to True, the model randomly selects words based on their probabilities, introducing randomness.\n",
        "generation_config.do_sample = True\n",
        "\n",
        "# Set the repetition penalty. A value of 1.15 discourages the model from repeating the same words or phrases too frequently in the output.\n",
        "generation_config.repetition_penalty = 0.15\n",
        "\n",
        "\n",
        "# Create a text generation pipeline using the initialized model, tokenizer, and generation configuration\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Create a LangChain pipeline that wraps the text generation pipeline and set a specific temperature for generation\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "id": "fefWE2QAc9Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm(\n",
        "    \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "5zNQYK2udcij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnFu5nIK3QV1"
      },
      "source": [
        "## Prompts and Prompt Templates\n",
        "\n",
        "One of the most useful features of LangChain is the ability to create prompt templates. A prompt template is a string that contains a placeholder for input variable(s). Let's see how we can use them:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "MODEL_NAME = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Create a configuration for text generation based on the specified model name\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set the maximum number of new tokens in the generated text to 1024.\n",
        "# This limits the length of the generated output to 1024 tokens.\n",
        "generation_config.max_new_tokens = 1024\n",
        "\n",
        "# Set the temperature for text generation. Lower values (e.g., 0.0001) make output more deterministic, following likely predictions.\n",
        "# Higher values make the output more random.\n",
        "generation_config.temperature = 0.0001\n",
        "\n",
        "# Set the top-p sampling value. A value of 0.95 means focusing on the most likely words that make up 95% of the probability distribution.\n",
        "generation_config.top_p = 0.95\n",
        "\n",
        "# Enable text sampling. When set to True, the model randomly selects words based on their probabilities, introducing randomness.\n",
        "generation_config.do_sample = True\n",
        "\n",
        "# Set the repetition penalty. A value of 1.15 discourages the model from repeating the same words or phrases too frequently in the output.\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "\n",
        "# Create a text generation pipeline using the initialized model, tokenizer, and generation configuration\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Create a LangChain pipeline that wraps the text generation pipeline and set a specific temperature for generation\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})\n"
      ],
      "metadata": {
        "id": "EdRtDidUe1wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define the template for generating prompts\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Behave as a teacher and provide an explanation for the following query:\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the PromptTemplate with the specified variables and template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],  # Specify the variables to be included in the prompt\n",
        "    template=template,  # Define the template structure\n",
        ")"
      ],
      "metadata": {
        "id": "d43v3tW6i-8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How does attention mechanism work? Let's think step by step\""
      ],
      "metadata": {
        "id": "fb3RejM3jXdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(text=text))"
      ],
      "metadata": {
        "id": "6s8keQGkjZML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnUHNqifjc2I",
        "outputId": "8bd0bf0b-7b5a-4ac9-9bd8-7ffe2c8f6af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S:  [INST: \n",
            "S: \n",
            "S: \n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Basic Prompt Formatting for Sum Calculation\n",
        "\n",
        "Define a PromptTemplate acting as a calculator that takes two input values and formats a prompt to calculate their sum."
      ],
      "metadata": {
        "id": "XOpXU964jfgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Define the template for calculating the sum\n",
        "calculator_template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Calculate the sum of the following two numbers.\n",
        "<</SYS>>\n",
        "\n",
        "The sum of {number1} and {number2} is: [/INST]\n",
        "\"\"\"\n",
        "\n",
        "# Create the PromptTemplate instance\n",
        "calculator_prompt = PromptTemplate(\n",
        "    input_variables=[\"number1\", \"number2\"],  # Define the two input variables\n",
        "    template=calculator_template,\n",
        ")\n",
        "\n",
        "# User input: two numbers for which the sum needs to be calculated\n",
        "number1 = \"5\"\n",
        "number2 = \"7\"\n",
        "\n",
        "# Format the prompt with the two input numbers\n",
        "formatted_prompt = calculator_prompt.format(number1=number1, number2=number2)\n",
        "print(formatted_prompt)\n",
        "\n",
        "# The output will be a prompt ready to calculate the sum of 5 and 7.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-VVnUBmPpoQ",
        "outputId": "00dbe82a-0b9c-46f0-9e42-24384e6acf12"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<s>[INST] <<SYS>>\n",
            "Calculate the sum of the following two numbers.\n",
            "<</SYS>>\n",
            "\n",
            "The sum of 5 and 7 is: [/INST]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG-bSJGPPyCj",
        "outputId": "8e72dd5c-ed71-40c4-dd2e-eec4f2e3bb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S:  [INST: \n",
            "S: \n",
            "S: \n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n",
            "\n",
            "S: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm(calculator_prompt.format(number1=number1, number2=number2))\n",
        "print(result)"
      ],
      "metadata": {
        "id": "lUxRp0UyQRK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3:\n",
        "\n",
        "Modify the prompt or question to explore how we can improve the model's performance."
      ],
      "metadata": {
        "id": "R65w6GODPmSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaBjhuJh8cXh"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Imagine three different experts are answering this question.\n",
        "All experts will write down 1 step of their thinking,\n",
        "then share it with the group.\n",
        "Then all experts will go on to the next step, etc.\n",
        "If any expert realises they're wrong at any point then they leave.\n",
        "The question is...\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBGjrqJr8jAT"
      },
      "outputs": [],
      "source": [
        "text = \"How does attention mechanism work? Let's think step by step\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcsiH6J_8qEk"
      },
      "outputs": [],
      "source": [
        "print(prompt.format(text=text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_40GwsV8wjp"
      },
      "outputs": [],
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofLPTJDat4UM"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act as a Machine Learning engineer who is teaching high school students.\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hylEBPA23Tn6"
      },
      "source": [
        "The variable must be surrounded by {}. The input_variables argument is a list of variable names that will be used to format the template. Let's see how we can use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8mxfp8TuDE8"
      },
      "outputs": [],
      "source": [
        "text = \"Explain what are Deep Neural Networks in 2-3 sentences\"\n",
        "print(prompt.format(text=text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EkGBnY73Wd_"
      },
      "source": [
        "You just have to use the format method of the PromptTemplate instance. The format method returns a string that can be used as input to the LLM. Let's see how we can use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWync3332POR"
      },
      "outputs": [],
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfcew-I-uelc"
      },
      "source": [
        "## Chain\n",
        "\n",
        "Probably the most important component of LangChain is the Chain class. It's a wrapper around the LLM that allows you to create a chain of actions. Here's how you can use the simplest chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP018xXHuT92"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "result = chain.run(text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoVl1RPOuotZ"
      },
      "source": [
        "The arguments to the LLMChain class are the LLM instance and the prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Use the LLMChain for Direct Response Generation\n",
        "Task: Create a new PromptTemplate for a fitness coach explaining the benefits of regular exercise and use LLMChain to generate a response."
      ],
      "metadata": {
        "id": "5XmD8tmzRCKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Define the template\n",
        "fitness_template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act as a fitness coach explaining the benefits of regular exercise to beginners.\n",
        "<</SYS>>\n",
        "\n",
        "{benefits_description} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "# Create the PromptTemplate instance\n",
        "fitness_prompt = PromptTemplate(\n",
        "    input_variables=[\"benefits_description\"],\n",
        "    template=fitness_template,\n",
        ")\n",
        "\n",
        "# Assuming `llm` is a predefined language model function\n",
        "chain = LLMChain(llm=llm, prompt=fitness_prompt)\n",
        "\n",
        "# Generate a response\n",
        "response = chain.run(\"What are the benefits of regular exercise?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "f72PgYIYRMX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcoh94ATwaLZ"
      },
      "source": [
        "#### Chaining Chains\n",
        "\n",
        "The LLMChain is not that different from using the LLM directly. Let's see how we can chain multiple chains together. We'll create a chain that will first explain what are Deep Neural Networks and then give a few examples of practical applications. Let's start by creating the second chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ3DzjryukM5"
      },
      "outputs": [],
      "source": [
        "template = \"<s>[INST] Use the summary {summary} and give 3 examples of practical applications with 1 sentence explaining each [/INST]\"\n",
        "\n",
        "examples_prompt = PromptTemplate(\n",
        "    input_variables=[\"summary\"],\n",
        "    template=template,\n",
        ")\n",
        "examples_chain = LLMChain(llm=llm, prompt=examples_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0GVll2fwfFM"
      },
      "source": [
        "Now we can reuse our first chain along with the examples_chain and combine them into a single chain using the SimpleSequentialChain class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bxlvPFUwdLB"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# Create an instance of 'SimpleSequentialChain'. This chain will execute two other chains\n",
        "# sequentially. The 'chains' parameter is a list of these chains - 'chain' and 'examples_chain'.\n",
        "multi_chain = SimpleSequentialChain(chains=[chain, examples_chain], verbose=True)\n",
        "\n",
        "# The 'run' method executes the chains in the order they are listed, passing the output\n",
        "# of one chain as the input to the next. The final output is then stored in the variable 'result'.\n",
        "result = multi_chain.run(text)\n",
        "\n",
        "print(result.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: Chaining Multiple Chains Together\n",
        "Task: Explain a scientific concept and then provide real-world applications."
      ],
      "metadata": {
        "id": "cKRDfx18SVZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "\n",
        "# First chain: Explain photosynthesis\n",
        "explanation_template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act as a data scientist explaining text embeddings to high school students.\n",
        "<</SYS>>\n",
        "\n",
        "{concept} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "explanation_prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=explanation_template,\n",
        ")\n",
        "\n",
        "explanation_chain = LLMChain(llm=llm, prompt=explanation_prompt)\n",
        "\n",
        "# Second chain: Provide real-world applications\n",
        "applications_template = \"\"\"\n",
        "<s>[INST] Use the explanation {explanation} and give 3 real-world applications of text embeddings [/INST]\n",
        "\"\"\"\n",
        "\n",
        "applications_prompt = PromptTemplate(\n",
        "    input_variables=[\"explanation\"],\n",
        "    template=applications_template,\n",
        ")\n",
        "\n",
        "applications_chain = LLMChain(llm=llm, prompt=applications_prompt)\n",
        "\n",
        "# Combine chains\n",
        "multi_chain = SimpleSequentialChain(chains=[explanation_chain, applications_chain], verbose=True)\n",
        "\n",
        "# Execute chains\n",
        "result = multi_chain.run(\"Explain text embeddings.\")\n",
        "print(result.strip())"
      ],
      "metadata": {
        "id": "VKHSYNyGSU4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2TKHUIew3w6"
      },
      "source": [
        "## Chatbot\n",
        "\n",
        "LangChain makes it easy to create chatbots. Let's see how we can create a simple chatbot that will answer questions about Deep Neural Networks. We'll use the ChatPromptTemplate class to create a template for the chatbot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmhfr6-nwgdi"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n",
        "template = \"Act as an experienced high school teacher that teaches {subject}. Always give examples and analogies\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(template),\n",
        "        HumanMessage(content=\"Hello teacher!\"),\n",
        "        AIMessage(content=\"Welcome everyone!\"),\n",
        "        HumanMessagePromptTemplate.from_template(human_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_prompt.format_messages(\n",
        "    subject=\"Artificial Intelligence\", text=\"What is the most powerful AI model?\"\n",
        ")\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGzTP5oBw-m4"
      },
      "source": [
        "We start by creating a system message that will be used to initialize the chatbot. Then we create a human message that will be used to start the conversation. Next, we create an AI message that will be used to respond to the human message. Finally, we create a human message that will be used to ask the question. We can use the format_messages method to format the messages.\n",
        "\n",
        "To use our LLM with the messages, we'll pass them to the predict_messages method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4dbCGSuw6oY"
      },
      "outputs": [],
      "source": [
        "result = llm.predict_messages(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming necessary imports and initializations have been done...\n",
        "\n",
        "# Define the initial template for the AI acting as a high school teacher.\n",
        "teacher_template = \"Act as an experienced high school teacher specializing in {subject}. Respond to the student's questions with informative answers, examples, and analogies.\"\n",
        "\n",
        "# Set the subject that the teacher specializes in.\n",
        "subject = \"Artificial Intelligence\"\n",
        "\n",
        "# The loop for the interactive conversation.\n",
        "while True:\n",
        "    # Get user input.\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for a quit condition.\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Construct the complete prompt for the AI model.\n",
        "    # This includes the role description (teacher_template) and the user's question.\n",
        "    complete_prompt = teacher_template.format(subject=subject) + \"\\nStudent asks: \" + user_input + \"\\nTeacher:\"\n",
        "\n",
        "    # Use the language model to generate a response.\n",
        "    # Ensure that 'llm.predict' is the correct method for your setup.\n",
        "    # This method should take the prompt as input and return the AI's response.\n",
        "    ai_response = llm.predict(complete_prompt)\n",
        "\n",
        "    # Print the AI's response.\n",
        "    # Make sure that 'ai_response' is being correctly extracted from the model's output.\n",
        "    print(\"Teacher:\", ai_response)\n",
        "\n",
        "# End the conversation loop.\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "id": "hvDhNPSHsKug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhy1WQo6zLNz"
      },
      "source": [
        "## Agents\n",
        "\n",
        "Agents are the most powerful feature of LangChain. They allow you to combine LLMs with external data and tools. Let's see how we can create a simple agent that will use the Python REPL to calculate the square root of a number and divide it by 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdUQ4plazI5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8f6705-d15e-4983-a228-8dd67c0aa095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Let me use the pow() function in python to calculate the square root of a number\n",
            "Action: $python pow(x, 0.5)\n",
            "Action Input: x = 16\u001b[0m\n",
            "Observation: $python pow(x, 0.5) is not a valid tool, try one of [Python_REPL].\n",
            "Thought:\u001b[32;1m\u001b[1;3m Oh, my bad. Let me try using the math.sqrt() function instead.\n",
            "Action: $python math.sqrt(x) / 2\n",
            "Action Input: x = 16\u001b[0m\n",
            "Observation: $python math.sqrt(x) / 2 is not a valid tool, try one of [Python_REPL].\n",
            "Thought:\u001b[32;1m\u001b[1;3m My apologies for that mistake too. Let me try again.\n",
            "Final Answer: The final answer is 4.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "\n",
        "agent = create_python_agent(llm=llm, tool=PythonREPLTool(), verbose=True)\n",
        "\n",
        "result = agent.run(\"Calculate the square root of a number and divide it by 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python REPL stands for \"Read-Eval-Print Loop.\" It's an interactive environment where you can write Python code and execute it immediately."
      ],
      "metadata": {
        "id": "kTtiWlo61oKU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XhPBoX6zSDg"
      },
      "source": [
        "Here's the final answer from our agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCtARjW3zOrV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3030ca6f-71d6-46c8-f428-fbe8e8784427"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The final answer is 4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhN6ZKOOzWjo"
      },
      "source": [
        "Let's run the code from the agent in a Python REPL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15UIrCtUzT7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ffbc5a-b75e-4c90-9982-e71e767abc4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from math import sqrt\n",
        "\n",
        "x = 16\n",
        "y = sqrt(x)\n",
        "z = y / 2\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QsiYJh5znhj"
      },
      "source": [
        "So, our agent works but made a mistake in the calculations. This is important, you might hear great things about AI, but it's still not perfect. Maybe another, more powerful LLM, will get this right. Try it out and let me know.\n",
        "\n",
        "Here's the response to the same prompt but using ChatGPT:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-T0AZyzyTB"
      },
      "source": [
        "     Enter a number: 16\n",
        "     The square root of 16.0 divided by 2 is: 2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AR4uYAZlSuZ",
        "outputId": "4d1f68e6-078e-4b8b-d0e4-1a7bbd7021b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=2c6ff9c1b6c247c4be147b1f43f06fb5ceecaefc25842c8b5bda6989956fdc85\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "class WikipediaAgent:\n",
        "    def search(self, query):\n",
        "        # Search Wikipedia and return the summary of the first result.\n",
        "        try:\n",
        "            # Get the page summary for the query\n",
        "            summary = wikipedia.summary(query)\n",
        "            return summary\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            # If there's a disambiguation issue, return the options.\n",
        "            return \"Disambiguation Error: \" + '; '.join(e.options)\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            # If the page is not found, inform the user.\n",
        "            return \"Page not found for the query.\"\n",
        "\n",
        "# Create an instance of the WikipediaAgent\n",
        "wiki_agent = WikipediaAgent()\n",
        "\n",
        "# Example use of the agent to search for a term\n",
        "result = wiki_agent.search(\"Artificial Intelligence\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "KtDdQq-o5GNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83502c63-f6c2-439c-b76d-13f1f6f7ad8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of other living beings, primarily of humans. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs.\n",
            "AI technology is widely used throughout industry, government, and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), interacting via human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).Alan Turing was the first person to conduct substantial research in the field that he called Machine Intelligence. Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding. Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture. This led to the AI spring of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to complete any task performable by a human) is among the field's long-term goals.To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and other fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ji6U-ewWlQxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15b88a733a554e0b850bfa4d076f8641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9b1880c7a6947bb9ebdc73eb74b1b06",
              "IPY_MODEL_27db62eaf9b34b8eb4c643d9f11c07d3",
              "IPY_MODEL_64e2cd923610449bae1e683ebaaac631"
            ],
            "layout": "IPY_MODEL_8b241c5621a64a1395bd368245f50dfe"
          }
        },
        "a9b1880c7a6947bb9ebdc73eb74b1b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b505fef3d2844e9bf6e0868b6fa2720",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4959cccd73324be8a8ca92693f0e51a2",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "27db62eaf9b34b8eb4c643d9f11c07d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24c895718c0f450da6be33717053e620",
            "max": 727,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bbb025b5041415eba8a5a64ed728898",
            "value": 727
          }
        },
        "64e2cd923610449bae1e683ebaaac631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_446d91f3847a493396799bc69bc22872",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8d9f93becd6040509ac57915b13c30fb",
            "value": " 727/727 [00:00&lt;00:00, 64.3kB/s]"
          }
        },
        "8b241c5621a64a1395bd368245f50dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b505fef3d2844e9bf6e0868b6fa2720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4959cccd73324be8a8ca92693f0e51a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24c895718c0f450da6be33717053e620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bbb025b5041415eba8a5a64ed728898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "446d91f3847a493396799bc69bc22872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9f93becd6040509ac57915b13c30fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c776dc0ece364e1c9103a99f1e8bf591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23c3e97ef5cb4257a29d1f7def0cdc44",
              "IPY_MODEL_024385c1620f402a95d9babe28d06e9e",
              "IPY_MODEL_c2dcd1598a72483aa2cfdf10b9560e12"
            ],
            "layout": "IPY_MODEL_3a1ffba80173451f8ffe306d901ce685"
          }
        },
        "23c3e97ef5cb4257a29d1f7def0cdc44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1dae59aa2b347dea02b1f830f0a50c1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b8c3786bfe664a1f85999b81b73f9a8d",
            "value": "tokenizer.model: 100%"
          }
        },
        "024385c1620f402a95d9babe28d06e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b8035fe5a9b425ca8986b561be9f237",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_829711b54aed41949a5f3e70e7072fb9",
            "value": 499723
          }
        },
        "c2dcd1598a72483aa2cfdf10b9560e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_650c97600d22422abc62ca7c8014f8c8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_993bc2970f7749429abbce002addb123",
            "value": " 500k/500k [00:00&lt;00:00, 26.2MB/s]"
          }
        },
        "3a1ffba80173451f8ffe306d901ce685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1dae59aa2b347dea02b1f830f0a50c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c3786bfe664a1f85999b81b73f9a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b8035fe5a9b425ca8986b561be9f237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829711b54aed41949a5f3e70e7072fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "650c97600d22422abc62ca7c8014f8c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "993bc2970f7749429abbce002addb123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09a97a4cb8964628b66b5da51d55a802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9bbe68bc95c4151b47063db13719721",
              "IPY_MODEL_1b847efdcc13489391f73173c65d64ed",
              "IPY_MODEL_c4e908bb27a144e39cbcc11e2417f50f"
            ],
            "layout": "IPY_MODEL_f13ebf2b85cd48f5acd974b9614d0e41"
          }
        },
        "b9bbe68bc95c4151b47063db13719721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66d8316a83d544df96b173a7f841f53f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6c1fd60ce19348ab87ef8622a70d283d",
            "value": "tokenizer.json: 100%"
          }
        },
        "1b847efdcc13489391f73173c65d64ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77a38082a7fc470ca638030004d3e4aa",
            "max": 1842764,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ceb94508cd64e95ac767c0f69f8f291",
            "value": 1842764
          }
        },
        "c4e908bb27a144e39cbcc11e2417f50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42ba4fbbc34e4a35b53ba94f20b133dc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3ce9999e2c1b4353a7eb5ad9fdd461a3",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 7.16MB/s]"
          }
        },
        "f13ebf2b85cd48f5acd974b9614d0e41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66d8316a83d544df96b173a7f841f53f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c1fd60ce19348ab87ef8622a70d283d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77a38082a7fc470ca638030004d3e4aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ceb94508cd64e95ac767c0f69f8f291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42ba4fbbc34e4a35b53ba94f20b133dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce9999e2c1b4353a7eb5ad9fdd461a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de26e861cfee49e296393519522db28d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed54e99907d5448da7aaa9771e069fcd",
              "IPY_MODEL_c8f18f6111aa48058472e6cced391528",
              "IPY_MODEL_4d9dc4b5f44c49d18fe5902df7b5b592"
            ],
            "layout": "IPY_MODEL_9ffe717a554447328bd96dea518d162d"
          }
        },
        "ed54e99907d5448da7aaa9771e069fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4589dd45a854cfc99125bbcd6278b62",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ebb889a1549346299d73bb3bda3546fd",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "c8f18f6111aa48058472e6cced391528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc3403ddcc3a462090c310eed6f7af2a",
            "max": 411,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75c03cb5cd3544dfb01bd90eafdede2a",
            "value": 411
          }
        },
        "4d9dc4b5f44c49d18fe5902df7b5b592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b848fcec75a43da8af57e53b2dcfe61",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_da05dd86b7f3493e843f7c5b9bfac17c",
            "value": " 411/411 [00:00&lt;00:00, 36.0kB/s]"
          }
        },
        "9ffe717a554447328bd96dea518d162d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4589dd45a854cfc99125bbcd6278b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebb889a1549346299d73bb3bda3546fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc3403ddcc3a462090c310eed6f7af2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75c03cb5cd3544dfb01bd90eafdede2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b848fcec75a43da8af57e53b2dcfe61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da05dd86b7f3493e843f7c5b9bfac17c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd74a4e1c4da42bc854e197a6d0f45c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1b58266ee3c41f68257fc1563daf5bf",
              "IPY_MODEL_ad6d78bfac3d4b77a32c60b40e0373bd",
              "IPY_MODEL_635d64f22c264f5488e964cbcd0a713d"
            ],
            "layout": "IPY_MODEL_1590cb2f699441d5b85d53ce3f78397a"
          }
        },
        "e1b58266ee3c41f68257fc1563daf5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5479b264afa7468dbde3e23cf8328391",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c1669361698848cabb6111a7857404b4",
            "value": "config.json: 100%"
          }
        },
        "ad6d78bfac3d4b77a32c60b40e0373bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f51b21525674641bba16c5cdc08ed0d",
            "max": 789,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54e9388f66d947219b798e2aee0a55ed",
            "value": 789
          }
        },
        "635d64f22c264f5488e964cbcd0a713d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec451c0e51c046c484c6a97ec71eb74b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5c29603e772a41e4afc4dcbbc0be538a",
            "value": " 789/789 [00:00&lt;00:00, 59.9kB/s]"
          }
        },
        "1590cb2f699441d5b85d53ce3f78397a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5479b264afa7468dbde3e23cf8328391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1669361698848cabb6111a7857404b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f51b21525674641bba16c5cdc08ed0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54e9388f66d947219b798e2aee0a55ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec451c0e51c046c484c6a97ec71eb74b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c29603e772a41e4afc4dcbbc0be538a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38f40d63a8534e7a8964ccdde867cf4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_673ad74686e14f1f98ae3a0859e7b63f",
              "IPY_MODEL_4c552c3bfb884d1c861d89121b1b61d4",
              "IPY_MODEL_fcc8601c3e1945038c2f9f0a28ceb152"
            ],
            "layout": "IPY_MODEL_a2f77aa9a5ba495f92ec76a98e03234a"
          }
        },
        "673ad74686e14f1f98ae3a0859e7b63f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_646651aafdbe489b95b5bd9c01190558",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3e6fa7c9ae674a668c37ca65edeac1e6",
            "value": "model.safetensors: 100%"
          }
        },
        "4c552c3bfb884d1c861d89121b1b61d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1daf4fe5256485ea39eea467845db0c",
            "max": 3896726136,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba09bf6b008443a1908805b6fbbf53a7",
            "value": 3896726136
          }
        },
        "fcc8601c3e1945038c2f9f0a28ceb152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43c1f2cf101f4fab9fa76572493177eb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_92d19ea3294c4423a8f735df654289e2",
            "value": " 3.90G/3.90G [00:11&lt;00:00, 398MB/s]"
          }
        },
        "a2f77aa9a5ba495f92ec76a98e03234a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "646651aafdbe489b95b5bd9c01190558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e6fa7c9ae674a668c37ca65edeac1e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1daf4fe5256485ea39eea467845db0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba09bf6b008443a1908805b6fbbf53a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43c1f2cf101f4fab9fa76572493177eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92d19ea3294c4423a8f735df654289e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "891a499b3bf44858b32bc5d00665d63c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d675a13ec766459fb91a996255286c44",
              "IPY_MODEL_8e4d2034dbaa40c8938e16abdf7a1996",
              "IPY_MODEL_b9929f8c9ce4450e928db3e36ebd3f01"
            ],
            "layout": "IPY_MODEL_2be89dbf2c5e4d3db9c7704dbf1c0622"
          }
        },
        "d675a13ec766459fb91a996255286c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0ddbe1949de446db1bf7b3d530d3bc8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dd94b1da344d4c5ca0d4a73255b4248a",
            "value": "generation_config.json: 100%"
          }
        },
        "8e4d2034dbaa40c8938e16abdf7a1996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffd2a8b21c3b4064953f455c48921a89",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87d33c74305d49dfa60c5572ab854250",
            "value": 137
          }
        },
        "b9929f8c9ce4450e928db3e36ebd3f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa294b25a7764c2ba51999975c8fedb2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_da8ed6bcfbe445fa9fd88b2affb253ab",
            "value": " 137/137 [00:00&lt;00:00, 12.8kB/s]"
          }
        },
        "2be89dbf2c5e4d3db9c7704dbf1c0622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0ddbe1949de446db1bf7b3d530d3bc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd94b1da344d4c5ca0d4a73255b4248a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffd2a8b21c3b4064953f455c48921a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87d33c74305d49dfa60c5572ab854250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa294b25a7764c2ba51999975c8fedb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da8ed6bcfbe445fa9fd88b2affb253ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}