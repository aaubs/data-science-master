{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oZGzukpx_EqX",
        "DMG0JTOF_YIY",
        "dDZhQ10zx24j",
        "Iv98-8WzNMdR",
        "4hvgIWMgShwH",
        "ViPcf9z8YmJv",
        "bbR-mqPfnZ6J",
        "rQo3nfQNoJA4",
        "Jmhc5zbpuvSp",
        "cBm6hH8Zr7kC",
        "R6THpolQDuhC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment introduction"
      ],
      "metadata": {
        "id": "nnV6o_KO_AMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time to predict some stuff!** <br> For this assignment, we will try to predict house prices based on the features of the house in question. This means we need to do the following:"
      ],
      "metadata": {
        "id": "NDrIiBDR_DtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load in the data (as per usual)\n",
        "2. Perform some EDA to get a better understanding of the data\n",
        "3. Clean up the data\n",
        "4. Perform feature engineering and choose our feature dimensions\n",
        "5. Create the feature and target matrix (our X's and y's)\n",
        "6. Create, fit a model and evaluate performance\n",
        "7. Set up a data prediction pipeline"
      ],
      "metadata": {
        "id": "LMfuU8DwwC8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment was very much made to give you a lot of creative freedom in how you want to approach your data engineering and model creation. Creating good SML models takes a lot of thinking, effort and testing. You can expect to go back and forth in the notebook a lot to change earlier data engineering steps, so keep your assigned variable names consistent!<br><br>\n",
        "\n",
        "(**Note:** There is no shame in not creating a super well-performing model, as long as you try out the different methods involved in SML. As the picture below illustrates, sometimes it just goes wrong) <br><br>\n",
        "\n",
        "The order in which you perform the different steps is more or less up to you, as long as you end up with some sort of trained model that is suitable for this type of prediction. <br>\n",
        "The way this notebook is laid out, is just to give you a general direction guide in terms of the overarching concepts; data filtering, data engineering, model fitting, testing and evaluation as well as setting up a data pipeline. If it's easier, you're welcome to create a separate notebook instead of working in this one, as long as you cover the tasks included."
      ],
      "metadata": {
        "id": "vh6pEtllwMJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://aaubs.github.io/ds-master/media/ML_Daddy.png)"
      ],
      "metadata": {
        "id": "-te0-Nj61pjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not an exhaustive list, but for this assignment you **will** need the following:\n",
        "1. From Scikitlearn:\n",
        "- Some sort of encoding library of your own choice\n",
        "- train_test_split (unless you want to do it manually for whichever reason)\n",
        "- Some sort of data scaler\n",
        "2. A visualization library:\n",
        "- I recommend matplotlib.pyplot and seaborn, but you can try to use altair if you want to\n",
        "3. Pandas (not the bamboo eating kind)\n",
        "- Because, duh, we need them dataframes!\n",
        "4. Joblib or Pickle\n",
        "- To save each component of the entire process"
      ],
      "metadata": {
        "id": "uCz3z4H4xCvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing modules and loading in the data"
      ],
      "metadata": {
        "id": "oZGzukpx_EqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link to the Data:** <br> https://www.kaggle.com/datasets/harishkumardatalab/housing-price-prediction?resource=download"
      ],
      "metadata": {
        "id": "MlRkalZ9wGyi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnQr2Kff-hPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mr2F5eQl-hN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tTIcCJF-hMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "DMG0JTOF_YIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First things first** <br> It's always a good idea to start out with checking for missing values, and dealing with them if there are any. <br> Do we have any missing values that we need to take care of? If so, find a way to handle the missing data (ie removing or replacing/filling them)"
      ],
      "metadata": {
        "id": "Yy0_nNDP_tNn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCmuvR3h-hJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets visualize some of the dimensions to get a better idea of our data. <br> Create a plot that shows the distribution of the price dimension. What can we see?**"
      ],
      "metadata": {
        "id": "bUsyrsiBHHu1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMqr9Zp9FrGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlations impact our predictions through statistical inference. Thus understanding which correlations we may be dealing with, is a good tool for choosing our dimensions for the feature matrix. Often these can be quite logical with some understanding of what the data represents <br> Check the correlations of the dimensions**"
      ],
      "metadata": {
        "id": "IdhK4G7LH0A1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2HVVvFlH0zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pGEjlcWzNPT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocessing"
      ],
      "metadata": {
        "id": "dDZhQ10zx24j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data cleaning"
      ],
      "metadata": {
        "id": "Iv98-8WzNMdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do we have any outliers in our data that may affect our prediction? If so, remove them if you think they could cause issues**"
      ],
      "metadata": {
        "id": "bOEOZRLHBNs0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7A1zqdcbDeyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z9ZBwUWFEV5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_BvlMWH-hAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAONbxbQE9L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is there any other cleaning that needs to be done? If you believe so, then perform the remaining cleaning and then proceed to the next step**"
      ],
      "metadata": {
        "id": "JAbfvZEUSSEI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xFQ3vCstcfJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature engineering"
      ],
      "metadata": {
        "id": "4hvgIWMgShwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So far we've gotten an idea of how our data is correlated, and we may already have an idea of which features we wish to use for our feature matrix. We can however do more than simply check correlations between data points. We can check what's known as feature importances. In order to do this we need to engineer the data in a way so that we can feed it to the SML model we choose.**"
      ],
      "metadata": {
        "id": "g4OhndqRWP-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BONUS TASK:** <br>\n",
        "It may be possible to define new features, for example as a combination of two existing ones in order to increase predictability. If you believe you can create new features, this is the time to do it <br>\n",
        "(Note: This step is not necessarily needed, but more if you're feeling adventurous, or if your model is not performing as well as you had hoped)"
      ],
      "metadata": {
        "id": "1Q2CyJhvY2zf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CIgg5byHY3Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In order to fit a model to our data, we need to encode the categorical values for prediction. <br> Encode the categorical features in the dataframe**"
      ],
      "metadata": {
        "id": "YXsyOq3WSqVf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cRvvN-zSUDNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3ejoHj0UalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzgOcmEtjTbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yy0Vks3CHH-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ezZ2kosxTULv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating target and feature + final evaluation of dimensions"
      ],
      "metadata": {
        "id": "ViPcf9z8YmJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Separate the target feature from the rest**"
      ],
      "metadata": {
        "id": "gWFMPWXBXdF3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vun4-JVKTdRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next up:**<br> I want you to evaluate whats known as the feature importances of the data. (Hint: Most SML models has a class attribute for this) <br>\n",
        "**Reflect:**<br> Do we evaluate feature importances *before* or *after* we do the train_test_split? What are some possible issues/benefits with either approach? <br>\n",
        "**Task:** Evaluate the feature importances of your data."
      ],
      "metadata": {
        "id": "wiIjkEG7a8xX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution:** I do this *BEFORE* the feature importances, as I wish to increase the models ability to generalize to new data. If we base our selection on feature importances of all the data, it may lead to overly optimistic model evaluation."
      ],
      "metadata": {
        "id": "1KNtj7vPXTjo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sB17oTphV1GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Us5PN7RyX6Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwSZCfeBX-5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56vujE_7YBT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now that you have selected your target features through whichever method you preferred, you need to remove the excess features that you don't need.** <br>\n",
        "*Note: If you've already performed the train_test_split, remember to remove them from both the test and training data*"
      ],
      "metadata": {
        "id": "TDBniFXseg93"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ktLH7oQXitpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3wnst3cNYDb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SyXsHfZmh6qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cuda01Y4h733"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fitting, testing and evaluating the model"
      ],
      "metadata": {
        "id": "bbR-mqPfnZ6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Okay! Now we got our feature and target matrix sorted (for now), we finally get to actually create our machine learning model.** <br><br>\n",
        "**Task:** Select a suitable model for the type of prediction we are trying to make, fit it to your data and evaluate the performance using appropriate metrics. <br><br>\n",
        "**Reflect**: Is the model performing well? If not, how can we increase the performance?"
      ],
      "metadata": {
        "id": "2rTFHTNwnhWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2s0Deol-kFcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bOc7R2QRk85Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TfHOS__xk83A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_dEi_ylk__4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDMDRR9olCSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**BONUS TASK - Hyperparameter tuning using GridSearchCV:**"
      ],
      "metadata": {
        "id": "rQo3nfQNoJA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to increase model performance is to perform what's known as a grid search of the hyperparameters of the model. Basically we try out different combinations of these hyperparameters in order to find the most optimal setup based on some form of scoring metric. Give it a try!  <br><br>\n",
        "**NOTE:** These can take a while to run, so whilst it is a good approach, it ***can*** also cost a lot of time, and as you may have experienced, Colab tends to time out after a while. A way to think of it is the following;<br><br>"
      ],
      "metadata": {
        "id": "h4njdToiqzna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text {Total models tested} = \\text {(Number of variables for parameter 1)} \\space \\times \\text {(Number of variables for parameter 2)} \\space \\times \\text {(Number of variables for parameter 3)} \\space \\text {...}\\space \\times \\text {(Number of variables for parameter n)}\n",
        "$$ <br>\n",
        "\n",
        "**So if you're testing out the following param_grid;**\n",
        "<br>3 variations of parameter 1,\n",
        "<br>4 variations of parameter 2,\n",
        "<br> 2 variations of parameter 3,\n",
        "<br> 2 variations of parameter 5,\n",
        "<br>5 variations of parameter 6\n",
        "<br><br>**You get the following:** <br><br>\n",
        "\n",
        "$$\n",
        "\\text {Total models tested} = 3 \\times 4 \\times 2 \\times 2 \\times 5 = 240\n",
        "$$ <br>\n",
        "As you can see it goes up quick, as we are already testing 240 versions of the model with different parameters. Whilst testing upwards of even 100 variations doesn't necessarily take that long (which I tried), you should still be careful to not just increase it to try out all possible combinations there are"
      ],
      "metadata": {
        "id": "3iGieqC-rlwa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIVIl4CYpIwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3npooQOpIuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aHQnniBXpIr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FtUnclspIpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sbtgSPnhpIdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZc2La5ApS7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12rSo0-gpWHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_aHkGjMpagp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dw1KUke4lEch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save the model components**"
      ],
      "metadata": {
        "id": "Jmhc5zbpuvSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When we have created our model, we want to able to use it outside of our development notebook, for this purpose we need to save each component we used for preprocessing, as well as the model itself**<br>\n",
        "**Task:** <br>\n",
        "Save the model components (the prediction model itself, the scaler and the label encoder)"
      ],
      "metadata": {
        "id": "1mjb2lz6vMGa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSxf29J0ww9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c3IGVoAnuvvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tHaJ-c__vYF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3irRXZMw3L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZiyqW0PxG9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the data pipeline"
      ],
      "metadata": {
        "id": "cBm6hH8Zr7kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finally we want to streamline our preprocessing and prediction methods for new data points. To do this, we create what's known as a data pipeline. It's basically a function that performs the *same* preprocessing steps as what we did earlier on a new observation** <br><br>\n",
        "**Task 1:** <br> Load in your components, and create a data pipeline function that will perform the preprocessing steps you did earlier all in one. Apply it to a new observation. <br><br>\n",
        "**Task 2:** <br> Load in your model, and create a prediction function that will predict an outcome based on this new observation"
      ],
      "metadata": {
        "id": "ln8vK9JT0TXz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k1-0dCS5sl7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yJ5ZjJic6jai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJuCYj_36wNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "os3Bj0sIBgxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_vNFv5MYBnv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus task: Create an interface to interact with your model"
      ],
      "metadata": {
        "id": "R6THpolQDuhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can either create a simple gradio interface, or alternatively create a streamlit application."
      ],
      "metadata": {
        "id": "NjFFFRdkD0nS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HAIpQWGWD9jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GaNez5GwD9f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuBAtn_aD9XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aiT69klGD9QT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}