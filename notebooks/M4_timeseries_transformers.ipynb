{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMLyd58zJxWZMZQ9zo7tkqG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M4_timeseries_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install darts"
      ],
      "metadata": {
        "id": "L6DB6ADKTfsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install yfinance"
      ],
      "metadata": {
        "id": "6rwW_JuPWfRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF2VPA-Axx2O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import seaborn as sns\n",
        "import altair as alt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# before starting, we define some constants\n",
        "figsize = (9, 6)\n",
        "lowest_q, low_q, high_q, highest_q = 0.01, 0.1, 0.9, 0.99\n",
        "label_q_outer = f\"{int(lowest_q * 100)}-{int(highest_q * 100)}th percentiles\"\n",
        "label_q_inner = f\"{int(low_q * 100)}-{int(high_q * 100)}th percentiles\""
      ],
      "metadata": {
        "id": "zzSLdmQpHGKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to deep learning (transformer-based) Timeseries Forecast"
      ],
      "metadata": {
        "id": "OSOqg2C-Gx5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* NeuralProphet (somewhat)\n",
        "* **[N-BEATS (ElementAI)](https://arxiv.org/abs/1905.10437):** Essentially, N-BEATS is a pure deep learning architecture based on a deep stack of ensembled feed forward networks that are also stacked by interconnecting backcast and forecast links.\n",
        "  * Easy to use: The model is simple to understand and has a modular structure (blocks and stacks). \n",
        "  * Multiple time-series: The model has the ability to generalize on many time-series.\n",
        "* **[N-HITS (ElementAI)]():** XXXXXX\n",
        "  * XXX\n",
        "* **[DeepAR (Amazon)](https://arxiv.org/abs/1704.04110?context=stat.ML):** A novel time series model that combines both deep-learning and autoregressive characteristics. \n",
        "  * Multiple time series: DeepAR works really well with multiple time series: A global model is built by using multiple time series with slightly different distributions.\n",
        "  * Rich set of inputs: Apart from historical data, DeepAR also allows the use of known future time sequences (a characteristic of auto-regressive models) and extra static attributes for series.\n",
        "  * Automatic scaling: In DeepAR, there is no need to do that manually since the model under the hood scales the autoregressive input.\n",
        "* **[Spacetimeformer:](https://arxiv.org/abs/2109.12218)** Considers both temporal and spatial relationships.\n",
        "  * Interesting when dealing with geospatial data, but I have little experience there.\n",
        "* **[Temporal Fusion Transformer](https://arxiv.org/abs/1912.09363):** Temporal Fusion Transformer (TFT) is a transformer-based time series forecasting model published by Google.\n",
        "  * Multiple time series: Like the aforementioned models, TFT supports building a model on multiple, heterogeneous time series.\n",
        "  * Rich number of features: TFT supports 3 types of features: i) time-dependent data with known inputs into the future ii) time-dependent data known only up to the present and iii) categorical/static variables, also known as time-invariant features. \n",
        "  * Interpretability: TFT gives much emphasis on interpretability. Specifically, by taking advantage of the Variable Selection component, the model can successfully measure the impact of each feature.\n",
        "  * Prediction Intervals: Similar to DeepAR, TFT outputs a prediction interval along with the predicted values, by using quantile regression."
      ],
      "metadata": {
        "id": "0pTvjTw8nYor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Fusion Transformers"
      ],
      "metadata": {
        "id": "oXFgP1uoNGug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Temporal Fusion Transformer (TFT) is an **attention-based Deep Neural Network**, optimized for great performance and interpretability. \n",
        "\n",
        "**Advantages and novelties:**\n",
        "\n",
        "* Rich features: \n",
        "  1. temporal data with known inputs into the future \n",
        "  2. temporal data known only up to the present and \n",
        "  3. exogenous categorical/static variables, also known as time-invariant features.\n",
        "* Heterogeneous time series: Supports training on multiple time series, splits processing into 2 parts: local processing which focuses on the characteristics of specific events and global processing which captures the collective characteristics of all time series.\n",
        "* Multi-horizon forecasting: Supports multi-step predictions. Apart from the actual prediction, TFT also outputs prediction intervals, by using the quantile loss function.\n",
        "* Interpretability: At its core, TFT is a transformer-based architecture. By taking advantage of self-attention, this model presents a novel Muti Head attention mechanism which when analyzed, provides extra insight on feature importances. "
      ],
      "metadata": {
        "id": "L8iSEax1ORax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timesieries implementation (DARTS)\n",
        "\n",
        "[Darts](https://unit8co.github.io/darts/) is a Python library for easy manipulation and forecasting of time series. It contains a variety of models, from classics such as ARIMA to deep neural networks. The models can all be used in the same way, using fit() and predict() functions, similar to scikit-learn. The library also makes it easy to backtest models, combine the predictions of several models, and take external data into account. \n",
        "\n",
        "Darts supports both univariate and multivariate time series and models. The ML-based models can be trained on potentially large datasets containing multiple time series, and some of the models offer a rich support for probabilistic forecasting.\n",
        "\n",
        "While there is also a standalone version of TFT (eg. [standalone pytorch implementation](https://pypi.org/project/tft-torch/)), we will for this example use the Darts implementation, since it eases the integration of TFT in your traditional forecasting pipeline."
      ],
      "metadata": {
        "id": "9TF87dp1cNCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts import TimeSeries, concatenate\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from darts.models import TFTModel\n",
        "from darts.metrics import mape, rmse\n",
        "\n",
        "from darts.utils.statistics import check_seasonality, plot_acf\n",
        "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
        "from darts.utils.likelihood_models import QuantileRegression\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "sOUbckxkq5WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Darts’ TFTModel incorporates the following main components from the original Temporal Fusion Transformer (TFT) architecture:\n",
        "\n",
        "*gating mechanisms: skip over unused components of the model architecture\n",
        "* variable selection networks: select relevant input variables at each time step.\n",
        "* temporal processing of past and future input with LSTMs (long short-term memory)\n",
        "* multi-head attention: captures long-term temporal dependencies\n",
        "* prediction intervals: per default, produces quantile forecasts instead of deterministic values"
      ],
      "metadata": {
        "id": "ri_OK8zJcLe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "TFTModel can be trained with past and future covariates. It is trained sequentially on fixed-size chunks consisting of an encoder and a decoder part:\n",
        "\n",
        "* encoder: past input with input_chunk_length\n",
        "  * past target: mandatory\n",
        "  * past covariates: optional\n",
        "* decoder: future known input with output_chunk_length\n",
        "  * future covariates: mandatory (if none are available, consider TFTModel’s optional arguments add_encoders or add_relative_index from here)\n",
        "\n",
        "In each iteration, the model produces a quantile prediction of shape (output_chunk_length, n_quantiles) on the decoder part."
      ],
      "metadata": {
        "id": "nVb3GBVjrHLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forecast\n",
        "\n",
        "Per default, TFTModel produces probabilistic quantile forecasts using QuantileRegression. This gives the range of likely target values at each prediction step. Most deep learning models in Darts’ - including TFTModel - support QuantileRegression and 16 other likelihoods to produce probabilistic forecasts by setting likelihood=MyLikelihood() at model creation."
      ],
      "metadata": {
        "id": "IMpsYb3ErnjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toy example (Air Passangers)\n",
        "\n",
        "Adopted from the [DARTS pakage tutorial](https://unit8co.github.io/darts/examples/13-TFT-examples.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "eDCfU0H7qeGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data set that is highly dependent on covariates. Knowing the month tells us a lot about the seasonal component, whereas the year determines the effect of the trend component.\n",
        "\n",
        "Additionally, let’s convert the time index to integer values and use them as covariates as well.\n",
        "\n",
        "All of the three covariates are known in the future, and can be used as future_covariates with the TFTModel."
      ],
      "metadata": {
        "id": "nWfNzvjvrZo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "from darts.datasets import AirPassengersDataset\n",
        "\n",
        "series = AirPassengersDataset().load()"
      ],
      "metadata": {
        "id": "x59sstycMDww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series.head()"
      ],
      "metadata": {
        "id": "yRodLz0uaqXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we convert monthly number of passengers to average daily number of passengers per month\n",
        "series = series / TimeSeries.from_series(series.time_index.days_in_month)\n",
        "series = series.astype(np.float32)"
      ],
      "metadata": {
        "id": "sjGTWStca2Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation sets:\n",
        "training_cutoff = pd.Timestamp(\"19571201\")\n",
        "train, val = series.split_after(training_cutoff)"
      ],
      "metadata": {
        "id": "2Jbc0q8Ma5jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the time series (note: we avoid fitting the transformer on the validation set)\n",
        "transformer = Scaler()\n",
        "train_transformed = transformer.fit_transform(train)\n",
        "val_transformed = transformer.transform(val)\n",
        "series_transformed = transformer.transform(series)"
      ],
      "metadata": {
        "id": "XKnI5IyMa-9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create year, month and integer index covariate series\n",
        "covariates = datetime_attribute_timeseries(series, attribute=\"year\", one_hot=False)"
      ],
      "metadata": {
        "id": "M7RbIfeybEKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covariates = covariates.stack(datetime_attribute_timeseries(series, attribute=\"month\", one_hot=False))"
      ],
      "metadata": {
        "id": "CIcY6-eMbPiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covariates = covariates.stack(\n",
        "    TimeSeries.from_times_and_values(\n",
        "        times=series.time_index,\n",
        "        values=np.arange(len(series)),\n",
        "        columns=[\"linear_increase\"],\n",
        "    )\n",
        ")\n",
        "\n",
        "covariates = covariates.astype(np.float32)"
      ],
      "metadata": {
        "id": "06KqSxxobTt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cov_train, cov_val = covariates.split_after(training_cutoff)"
      ],
      "metadata": {
        "id": "pgXzx74Qbise"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform covariates (note: we fit the transformer on train split and can then transform the entire covariates series)\n",
        "scaler_covs = Scaler()\n",
        "scaler_covs.fit(cov_train)\n",
        "covariates_transformed = scaler_covs.transform(covariates)"
      ],
      "metadata": {
        "id": "zkrj3fje6liS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TFTModel can only be used if some future input is given. Optional parameters add_encoders and add_relative_index can be useful, especially if we don’t have any future input available. They generate endoded temporal data is used as future covariates.\n",
        "\n",
        "Since we already have future covariates defined in our example they are commented out."
      ],
      "metadata": {
        "id": "EeHOKKY7sWYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 200\n",
        "input_chunk_length = 24\n",
        "forecast_horizon = 12"
      ],
      "metadata": {
        "id": "AnMc0Tr37HA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = TFTModel(\n",
        "    input_chunk_length=input_chunk_length,\n",
        "    output_chunk_length=forecast_horizon,\n",
        "    hidden_size=64,\n",
        "    lstm_layers=1,\n",
        "    num_attention_heads=4,\n",
        "    dropout=0.1,\n",
        "    batch_size=16,\n",
        "    n_epochs=300,\n",
        "    add_relative_index=False,\n",
        "    add_encoders=None,\n",
        "    likelihood=QuantileRegression(\n",
        "        # quantiles= [ 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
        "    ),  # QuantileRegression is set per default\n",
        "    # loss_fn=MSELoss(),\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "ku5pnZ8L7Nal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In what follows, we can just provide the whole covariates series as future_covariates argument to the model; the model will slice these covariates and use only what it needs in order to train on forecasting the target train_transformed:"
      ],
      "metadata": {
        "id": "7fHejuSQsj06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.fit(train_transformed, future_covariates=covariates_transformed, verbose=True)"
      ],
      "metadata": {
        "id": "7Oo9tYK37Sxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform a one-shot prediction of 24 months using the “current” model - i.e., the model at the end of the training procedure:"
      ],
      "metadata": {
        "id": "b2dBxj_Rsscc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, n, actual_series, val_series):\n",
        "    pred_series = model.predict(n=n, num_samples=num_samples)\n",
        "\n",
        "    # plot actual series\n",
        "    plt.figure(figsize=figsize)\n",
        "    actual_series[: pred_series.end_time()].plot(label=\"actual\")\n",
        "\n",
        "    # plot prediction with quantile ranges\n",
        "    pred_series.plot(\n",
        "        low_quantile=lowest_q, high_quantile=highest_q, label=label_q_outer\n",
        "    )\n",
        "    pred_series.plot(low_quantile=low_q, high_quantile=high_q, label=label_q_inner)\n",
        "\n",
        "    plt.title(\"MAPE: {:.2f}%\".format(mape(val_series, pred_series)))\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "HGpnsOFl7haz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(my_model, 24, series_transformed, val_transformed)"
      ],
      "metadata": {
        "id": "-OAUz7T37m9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s backtest our TFTModel model, to see how it performs with a forecast horizon of 12 months over the last 3 years:"
      ],
      "metadata": {
        "id": "VTOLk8SPsyAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backtest_series = my_model.historical_forecasts(\n",
        "    series_transformed,\n",
        "    future_covariates=covariates_transformed,\n",
        "    start=train.end_time() + train.freq,\n",
        "    num_samples=num_samples,\n",
        "    forecast_horizon=forecast_horizon,\n",
        "    stride=forecast_horizon,\n",
        "    last_points_only=False,\n",
        "    retrain=False,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "e3fskAmPs33Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_backtest(backtest_series, actual_series, horizon, start, transformer):\n",
        "    plt.figure(figsize=figsize)\n",
        "    actual_series.plot(label=\"actual\")\n",
        "    backtest_series.plot(\n",
        "        low_quantile=lowest_q, high_quantile=highest_q, label=label_q_outer\n",
        "    )\n",
        "    backtest_series.plot(low_quantile=low_q, high_quantile=high_q, label=label_q_inner)\n",
        "    plt.legend()\n",
        "    plt.title(f\"Backtest, starting {start}, {horizon}-months horizon\")\n",
        "    print(\n",
        "        \"MAPE: {:.2f}%\".format(\n",
        "            mape(\n",
        "                transformer.inverse_transform(actual_series),\n",
        "                transformer.inverse_transform(backtest_series),\n",
        "            )\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "oS8JNCS8s89h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_backtest(\n",
        "    backtest_series=concatenate(backtest_series),\n",
        "    actual_series=series_transformed,\n",
        "    horizon=forecast_horizon,\n",
        "    start=training_cutoff,\n",
        "    transformer=transformer,\n",
        ")"
      ],
      "metadata": {
        "id": "8WlzMutZtAlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting stocks price with TFT (and make $$)"
      ],
      "metadata": {
        "id": "iQuw9tq3ugRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.utils.timeseries_generation import datetime_attribute_timeseries"
      ],
      "metadata": {
        "id": "_4Ie20Cgz3gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting data"
      ],
      "metadata": {
        "id": "zpO1Ps1PepKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install yfinance"
      ],
      "metadata": {
        "id": "My0C9TKf1MR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf"
      ],
      "metadata": {
        "id": "Ri6A4fqKrBGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stocks = yf.download(tickers=['GOOGL'], period='10y', interval='1d') # , 'AAPL', 'GOOGL'"
      ],
      "metadata": {
        "id": "J5qqOZRb4L1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stocks.head()"
      ],
      "metadata": {
        "id": "Ayrm_S5WLiEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stocks.dtypes"
      ],
      "metadata": {
        "id": "QMqQrXjZZT73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stocks = df_stocks.drop('Volume', axis=1)"
      ],
      "metadata": {
        "id": "zE-cNAr_ZpHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt.Chart(data = df_stocks.reset_index()).mark_line().encode(\n",
        "    x='Date:T',\n",
        "    y='Close:Q'\n",
        ")"
      ],
      "metadata": {
        "id": "HP603oTuLXUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create \n",
        "\n"
      ],
      "metadata": {
        "id": "zvOq4uqszhyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create time series object for target variable\n",
        "ts_P = TimeSeries.from_series(df_stocks[\"Close\"], fill_missing_dates=True, freq='D') "
      ],
      "metadata": {
        "id": "rolepWwvzlae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_P"
      ],
      "metadata": {
        "id": "d5jgFcsqbrBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check attributes of the time series\n",
        "print(\"components:\", ts_P.components)\n",
        "print(\"duration:\",ts_P.duration)\n",
        "print(\"frequency:\",ts_P.freq)\n",
        "print(\"frequency:\",ts_P.freq_str)\n",
        "print(\"has date time index? (or else, it must have an integer index):\",ts_P.has_datetime_index)\n",
        "print(\"deterministic:\",ts_P.is_deterministic)\n",
        "print(\"univariate:\",ts_P.is_univariate)"
      ],
      "metadata": {
        "id": "ljBFPgjH3SHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create time series object for the feature columns\n",
        "df_covF = df_stocks.loc[:, df_stocks.columns != \"Close\"]\n",
        "ts_covF = TimeSeries.from_dataframe(df_covF, fill_missing_dates=True, freq='D')"
      ],
      "metadata": {
        "id": "GXGCV6z03cbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check attributes of the time series\n",
        "print(\"components (columns) of feature time series:\", ts_covF.components)\n",
        "print(\"duration:\",ts_covF.duration)\n",
        "print(\"frequency:\",ts_covF.freq)\n",
        "print(\"frequency:\",ts_covF.freq_str)\n",
        "print(\"has date time index? (or else, it must have an integer index):\",ts_covF.has_datetime_index)\n",
        "print(\"deterministic:\",ts_covF.is_deterministic)\n",
        "print(\"univariate:\",ts_covF.is_univariate)"
      ],
      "metadata": {
        "id": "MHkn5gzyb2MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all the hyperparameters\n",
        "\n",
        "LOAD = False         # True = load previously saved model from disk?  False = (re)train the model\n",
        "SAVE = \"\\_TFT_model_02.pth.tar\"   # file name to save the model under\n",
        "\n",
        "EPOCHS = 200\n",
        "INLEN = 32          # input size\n",
        "HIDDEN = 64         # hidden layers    \n",
        "LSTMLAYERS = 2      # recurrent layers\n",
        "ATTH = 4            # attention heads\n",
        "BATCH = 32          # batch size\n",
        "LEARN = 1e-3        # learning rate\n",
        "DROPOUT = 0.1       # dropout rate\n",
        "VALWAIT = 1         # epochs to wait before evaluating the loss on the test/validation set\n",
        "N_FC = 1            # output size\n",
        "\n",
        "RAND = 1337           # random seed\n",
        "N_SAMPLES = 100     # number of times a prediction is sampled from a probabilistic model\n",
        "N_JOBS = 3          # parallel processors to use;  -1 = all processors\n",
        "\n",
        "# default quantiles for QuantileRegression\n",
        "QUANTILES = [0.01, 0.1, 0.2, 0.5, 0.8, 0.9, 0.99]\n",
        "\n",
        "SPLIT = 0.9         # train/test %\n",
        "\n",
        "FIGSIZE = (9, 6)\n",
        "\n",
        "\n",
        "qL1, qL2 = 0.01, 0.10        # percentiles of predictions: lower bounds\n",
        "qU1, qU2 = 1-qL1, 1-qL2,     # upper bounds derived from lower bounds\n",
        "label_q1 = f'{int(qU1 * 100)} / {int(qL1 * 100)} percentile band'\n",
        "label_q2 = f'{int(qU2 * 100)} / {int(qL2 * 100)} percentile band'\n",
        "\n",
        "mpath = os.path.abspath(os.getcwd()) + SAVE     # path and file name to save the mode"
      ],
      "metadata": {
        "id": "HFmi_r3D4_Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/test split and scaling of target variable\n",
        "ts_train, ts_test = ts_P.split_after(SPLIT)"
      ],
      "metadata": {
        "id": "-U4bUuF7447Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training start:\", ts_train.start_time())\n",
        "print(\"training end:\", ts_train.end_time())\n",
        "print(\"training duration:\",ts_train.duration)\n",
        "print(\"test start:\", ts_test.start_time())\n",
        "print(\"test end:\", ts_test.end_time())\n",
        "print(\"test duration:\", ts_test.duration)"
      ],
      "metadata": {
        "id": "VFH2qmnicKqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale\n",
        "scalerP = Scaler()\n",
        "scalerP.fit_transform(ts_train)\n",
        "\n",
        "ts_ttrain = scalerP.transform(ts_train)\n",
        "ts_ttest = scalerP.transform(ts_test)    \n",
        "ts_t = scalerP.transform(ts_P)"
      ],
      "metadata": {
        "id": "Du3Y_oNeW8cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure data are of type float\n",
        "ts_t = ts_t.astype(np.float32)\n",
        "ts_ttrain = ts_ttrain.astype(np.float32)\n",
        "ts_ttest = ts_ttest.astype(np.float32)"
      ],
      "metadata": {
        "id": "fIzviB-OXDVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHeck first and last row\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "ts_t.pd_dataframe().iloc[[0,-1]]"
      ],
      "metadata": {
        "id": "n2ERw1aXXHm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/test split and scaling of feature covariates\n",
        "covF_train, covF_test = ts_covF.split_after(SPLIT)"
      ],
      "metadata": {
        "id": "yldW9bNWXQhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaler\n",
        "scalerF = Scaler()\n",
        "scalerF.fit_transform(covF_train)\n",
        "\n",
        "covF_ttrain = scalerF.transform(covF_train) \n",
        "covF_ttest = scalerF.transform(covF_test)   \n",
        "covF_t = scalerF.transform(ts_covF) "
      ],
      "metadata": {
        "id": "AoqZFKrDXQst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure data are of type float\n",
        "covF_ttrain = covF_ttrain.astype(np.float32)\n",
        "covF_ttest = covF_ttest.astype(np.float32)\n",
        "covF_t = covF_t.astype(np.float32)"
      ],
      "metadata": {
        "id": "e_lp3uChXQ6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first and last row of scaled feature covariates\n",
        "covF_t.pd_dataframe().iloc[[0,-1]]"
      ],
      "metadata": {
        "id": "-NUqvCPH5bzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering - create time covariates: hour, weekday, month, year, country-specific holidays\n",
        "covT = datetime_attribute_timeseries( ts_P.time_index, attribute=\"day\", add_length=7 )   # 48 hours beyond end of test set to prepare for out-of-sample forecasting\n",
        "covT = covT.stack(  datetime_attribute_timeseries(covT.time_index, attribute=\"day_of_week\")  )\n",
        "covT = covT.stack(  datetime_attribute_timeseries(covT.time_index, attribute=\"month\")  )\n",
        "covT = covT.stack(  datetime_attribute_timeseries(covT.time_index, attribute=\"year\")  )\n",
        "\n",
        "covT = covT.add_holidays(country_code=\"US\")\n",
        "covT = covT.astype(np.float32)"
      ],
      "metadata": {
        "id": "qb23uOlaXhjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/test split\n",
        "covT_train, covT_test = covT.split_after(ts_train.end_time())"
      ],
      "metadata": {
        "id": "dpci1S6PX55R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rescale the covariates: fitting on the training set\n",
        "scalerT = Scaler()\n",
        "scalerT.fit(covT_train)"
      ],
      "metadata": {
        "id": "DmzcTJWOX8xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covT_ttrain = scalerT.transform(covT_train)\n",
        "covT_ttest = scalerT.transform(covT_test)\n",
        "covT_t = scalerT.transform(covT)"
      ],
      "metadata": {
        "id": "tcX5L15Yc9A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covT_t = covT_t.astype(np.float32)\n",
        "covT_ttrain = covT_ttrain.astype(np.float32)\n",
        "covT_ttest = covT_test.astype(np.float32)"
      ],
      "metadata": {
        "id": "50RQfoQLc5FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first and last row of unscaled time covariates:\n",
        "covT.pd_dataframe().iloc[[0,-1]]"
      ],
      "metadata": {
        "id": "eedsDpwx5e6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine feature and time covariates along component dimension: axis=1\n",
        "ts_cov = ts_covF.concatenate( covT.slice_intersect(ts_covF), axis=1 )                      # unscaled F+T\n",
        "cov_t = covF_t.concatenate( covT_t.slice_intersect(covF_t), axis=1 )                       # scaled F+T\n",
        "cov_ttrain = covF_ttrain.concatenate( covT_ttrain.slice_intersect(covF_ttrain), axis=1 )   # scaled F+T training set\n",
        "cov_ttest = covF_ttest.concatenate( covT_ttest.slice_intersect(covF_ttest), axis=1 )       # scaled F+T test set"
      ],
      "metadata": {
        "id": "OCt2Ia0VYE67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first and last row of unscaled covariates\n",
        "ts_cov.pd_dataframe().iloc[[0,-1]]"
      ],
      "metadata": {
        "id": "m2coWn0T5nU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFTModel(   input_chunk_length=INLEN,\n",
        "                    output_chunk_length=N_FC,\n",
        "                    hidden_size=HIDDEN,\n",
        "                    lstm_layers=LSTMLAYERS,\n",
        "                    num_attention_heads=ATTH,\n",
        "                    dropout=DROPOUT,\n",
        "                    batch_size=BATCH,\n",
        "                    n_epochs= 5, # EPOCHS,                        \n",
        "                    nr_epochs_val_period=VALWAIT, \n",
        "                    likelihood=QuantileRegression(QUANTILES), \n",
        "                    optimizer_kwargs={\"lr\": LEARN}, \n",
        "                    model_name=\"TFT_stocks\",\n",
        "                    log_tensorboard=True,\n",
        "                    random_state=RAND,\n",
        "                    force_reset=True,\n",
        "                    save_checkpoints=True\n",
        "                )"
      ],
      "metadata": {
        "id": "ELNA9EkS51pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training: load a saved model or (re)train\n",
        "if LOAD:\n",
        "    print(\"have loaded a previously saved model from disk:\" + mpath)\n",
        "    model = TFTModel.load_model(mpath)                            # load previously model from disk \n",
        "else:\n",
        "    model.fit(  series=ts_ttrain, \n",
        "                future_covariates=cov_t, \n",
        "                val_series=ts_ttest, \n",
        "                val_future_covariates=cov_t, \n",
        "                verbose=True)\n",
        "    print(\"have saved the model after training:\", mpath)\n",
        "    model.save(path = mpath)"
      ],
      "metadata": {
        "id": "TPSIm_Cw553i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing: generate predictions\n",
        "ts_tpred = model.predict(   n=len(ts_ttest), \n",
        "                            num_samples=N_SAMPLES,   \n",
        "                            n_jobs=N_JOBS, \n",
        "                            verbose=True)"
      ],
      "metadata": {
        "id": "6z-7dMkT6Gon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve forecast series for chosen quantiles, \n",
        "# inverse-transform each series,\n",
        "# insert them as columns in a new dataframe dfY\n",
        "q50_RMSE = np.inf\n",
        "q50_MAPE = np.inf\n",
        "ts_q50 = None\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "dfY = pd.DataFrame()\n",
        "dfY[\"Actual\"] = TimeSeries.pd_series(ts_test)"
      ],
      "metadata": {
        "id": "OfM7H2856PRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function: get forecast values for selected quantile q and insert them in dataframe dfY\n",
        "def predQ(ts_t, q):\n",
        "    ts_tq = ts_t.quantile_timeseries(q)\n",
        "    ts_q = scalerP.inverse_transform(ts_tq)\n",
        "    s = TimeSeries.pd_series(ts_q)\n",
        "    header = \"Q\" + format(int(q*100), \"02d\")\n",
        "    dfY[header] = s\n",
        "    if q==0.5:\n",
        "        ts_q50 = ts_q\n",
        "        q50_RMSE = rmse(ts_q50, ts_test)\n",
        "        q50_MAPE = mape(ts_q50, ts_test) \n",
        "        print(\"RMSE:\", f'{q50_RMSE:.2f}')\n",
        "        print(\"MAPE:\", f'{q50_MAPE:.2f}')"
      ],
      "metadata": {
        "id": "-9_K7RVUjz0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call helper function predQ, once for every quantile\n",
        "_ = [predQ(ts_tpred, q) for q in QUANTILES]"
      ],
      "metadata": {
        "id": "950hCTO4j4NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move Q50 column to the left of the Actual column\n",
        "col = dfY.pop(\"Q50\")\n",
        "dfY.insert(1, col.name, col)\n",
        "dfY.iloc[np.r_[0:2, -2:0]]"
      ],
      "metadata": {
        "id": "LcEmygsoj6RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the forecast\n",
        "plt.figure(100, figsize=(20, 7))\n",
        "sns.set(font_scale=1.3)\n",
        "p = sns.lineplot(x = \"time\", y = \"Q50\", data = dfY, palette=\"coolwarm\")\n",
        "sns.lineplot(x = \"time\", y = \"Actual\", data = dfY, palette=\"coolwarm\")\n",
        "plt.legend(labels=[\"forecast median price Q50\", \"actual price\"])\n",
        "p.set_ylabel(\"price\")\n",
        "p.set_xlabel(\"\")\n",
        "p.set_title(\"stock price (test set)\");"
      ],
      "metadata": {
        "id": "dbnDAOEq6QLC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}