{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tyJ_uRTJTvs"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial, we'll explore how to analyze relationships between companies and technologies using network analysis. We'll work with a dataset of company-technology relationships extracted from news articles and create visualizations to understand the connections between different companies based on their technology focuses. The creation of the network will be based on LLM-extraction.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoSSCsC6JTvu"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install ollama pandas networkx matplotlib tqdm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hsaiJETJTvu"
      },
      "source": [
        "## Data Extraction\n",
        "\n",
        "### Setting up Ollama\n",
        "\n",
        "We'll use Ollama, an open-source large language model framework, to extract relationships from our text data. First, we need to set up the Ollama server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1IV0I0pKTSS"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Tt-PFcJTvv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "\n",
        "def start_ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=start_ollama)\n",
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make sure to download a model\n",
        "# ollama run qwen2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4GLrI7QKubh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import ollama\n",
        "import networkx as nx\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd5ohyP4JTvv"
      },
      "source": [
        "### Defining the Extraction Schema\n",
        "\n",
        "We'll use a structured schema to extract relationships between companies and technologies. Each relationship will include:\n",
        "\n",
        "- Source company (`from`)\n",
        "- Technology (`to`)\n",
        "- Relationship type (`type`: owns/develops/implements)\n",
        "- Technology category (`tech_type`)\n",
        "\n",
        "Here's our system prompt that defines the extraction rules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXKGHrfUJTvv"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"Extract relationships between companies and technologies from the given text. Focus only on relationships where a company owns, develops, or implements a specific technology. Provide output in this JSON format:\n",
        "{\n",
        " \"edges\": [\n",
        " {\"from\": \"Company Name\", \"to\": \"Technology Name\", \"type\": \"relationship_type\", \"tech_type\": \"Technology Category\"}\n",
        " ]\n",
        "}\n",
        "The \"type\" field should be \"owns\", \"develops\", or \"implements\".\n",
        "The \"tech_type\" field should categorize the technology into one of these types:\n",
        "1. Customer Service and Support AI\n",
        "2. AI Infrastructure and Operations\n",
        "3. Robotics and Autonomous Systems\n",
        "4. Construction and Manufacturing AI\n",
        "5. Healthcare AI Applications\n",
        "6. Business Process and Workflow Automation\n",
        "7. Extended Reality (AR/VR) and Immersive Technologies\n",
        "8. AI in Mobile and Imaging\n",
        "9. AI Audio and Video Generation\n",
        "10. Search and Information Retrieval AI\n",
        "11. Financial Technology (FinTech) and Financial AI\n",
        "12. Smart Home and IoT AI\n",
        "13. E-Commerce AI Solutions\n",
        "14. Cybersecurity AI Solutions\n",
        "15. Recruitment and Human Resources (HR) AI\n",
        "16. Media and Content Personalization AI\n",
        "17. Data Analytics and Business Intelligence\n",
        "18. Software Development and DevOps AI Tools\n",
        "19. Generative and Multimodal AI\n",
        "20. Educational and Training AI\n",
        "\n",
        "Ensure a valid JSON object with an 'edges' array, even if empty. English output only.\n",
        "\n",
        "Examples based on the input articles:\n",
        "1. {\"from\": \"Google\", \"to\": \"AI-powered conversational chatbot\", \"type\": \"develops\", \"tech_type\": \"Customer Service and Support AI\"}\n",
        "2. {\"from\": \"OpenAI\", \"to\": \"ChatGPT desktop app for macOS\", \"type\": \"develops\", \"tech_type\": \"AI Infrastructure and Operations\"}\n",
        "3. {\"from\": \"YouTube\", \"to\": \"AI chatbot for Premium subscribers\", \"type\": \"implements\", \"tech_type\": \"Customer Service and Support AI\"}\n",
        "4. {\"from\": \"Apple\", \"to\": \"AI training curriculum for Developer Academy\", \"type\": \"develops\", \"tech_type\": \"Educational and Training AI\"}\n",
        "5. {\"from\": \"Adobe\", \"to\": \"Firefly AI for text-to-video generation\", \"type\": \"develops\", \"tech_type\": \"AI Audio and Video Generation\"}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-9fWLMiJTvv"
      },
      "source": [
        "### Extracting Relationships\n",
        "\n",
        "We'll create a function to process each article and extract the relationships:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn2rPg-uJTvv"
      },
      "outputs": [],
      "source": [
        "def extract_relationships(article):\n",
        "    prompt = f\"\"\"\n",
        "    Extract key relationships between companies and technologies from this text:\n",
        "    Title: {article['title']}\n",
        "    Text: {article['text']}\n",
        "    Focus on relationships where a company owns, develops, or implements a specific technology.\n",
        "    Categorize each technology according to the tech_type categories provided.\n",
        "    \"\"\"\n",
        "    response = ollama.chat(\n",
        "        model='qwen2.5',\n",
        "        messages=[\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': prompt},\n",
        "        ],\n",
        "        format='json',\n",
        "        options={\"temperature\":0.1}\n",
        "    )\n",
        "    return response['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract for the first 50 articles in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/aaubs/ds-master/refs/heads/main/data/paraphrased_articles.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPKfMXbOL2Ba"
      },
      "outputs": [],
      "source": [
        "# Read input articles\n",
        "with open('paraphrased_articles.jsonl', 'r', encoding='utf-8') as f:\n",
        "    articles = [json.loads(line) for line in f][:50]\n",
        "\n",
        "# Process articles\n",
        "results = []\n",
        "for article in tqdm(articles, desc=\"Processing articles\"):\n",
        "    try:\n",
        "        extracted_data = json.loads(extract_relationships(article))\n",
        "        results.append({\n",
        "            'title': article['title'],\n",
        "            'extracted_data': extracted_data\n",
        "        })\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error processing article '{article['title']}': {str(e)}\")\n",
        "\n",
        "# Display first 5 results\n",
        "for i, result in enumerate(results[:5]):\n",
        "    print(f\"Article {i+1}: {result['title']}\")\n",
        "    print(json.dumps(result['extracted_data'], indent=2))\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Save all results\n",
        "with open('company_technology_relationships.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Results saved to company_technology_relationships.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RLXvHd6JTvv"
      },
      "source": [
        "## Network Analysis\n",
        "\n",
        "### Creating the Bipartite Graph\n",
        "\n",
        "We'll first create a bipartite graph where one set of nodes represents companies and the other represents technology types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-ffdQqaJTvw"
      },
      "outputs": [],
      "source": [
        "# Create a bipartite graph from the extracted relationships\n",
        "B = nx.Graph()\n",
        "companies = set()\n",
        "tech_types = set()\n",
        "\n",
        "print(\"Building bipartite graph...\")\n",
        "for result in tqdm(results, desc=\"Processing Results\"):\n",
        "    for edge in result['extracted_data']['edges']:\n",
        "        company = edge.get('from')\n",
        "        tech_type = edge.get('tech_type')\n",
        "        if not company or not tech_type:\n",
        "            continue\n",
        "        companies.add(company)\n",
        "        tech_types.add(tech_type)\n",
        "        B.add_edge(company, tech_type)\n",
        "\n",
        "print(f\"\\nNumber of companies: {len(companies)}\")\n",
        "print(f\"Number of technology types: {len(tech_types)}\")\n",
        "print(f\"Number of edges in the bipartite graph: {B.number_of_edges()}\")\n",
        "\n",
        "# Project onto the company layer using weighted_projected_graph\n",
        "print(\"\\nProjecting bipartite graph onto company layer...\")\n",
        "company_graph = nx.bipartite.weighted_projected_graph(B, companies)\n",
        "print(f\"Number of nodes in company graph: {company_graph.number_of_nodes()}\")\n",
        "print(f\"Number of edges in company graph: {company_graph.number_of_edges()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVMP-IAKJTvw"
      },
      "source": [
        "### Graph Projection and Edge Trimming\n",
        "\n",
        "To analyze relationships between companies, we'll project the bipartite graph onto the company layer and trim weak connections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBWFpJ4jJTvw"
      },
      "outputs": [],
      "source": [
        "def trim_edges(graph, percentile=20):\n",
        "    \"\"\"\n",
        "    Removes edges from the graph that have weights below the specified percentile.\n",
        "\n",
        "    Parameters:\n",
        "    - graph (networkx.Graph): The input graph with weighted edges.\n",
        "    - percentile (float): The percentile threshold below which edges will be removed.\n",
        "\n",
        "    Returns:\n",
        "    - networkx.Graph: The trimmed graph.\n",
        "    \"\"\"\n",
        "    if not graph.edges(data=True):\n",
        "        print(\"The graph has no edges to trim.\")\n",
        "        return graph\n",
        "\n",
        "    weights = [data['weight'] for u, v, data in graph.edges(data=True)]\n",
        "    threshold = np.percentile(weights, percentile)\n",
        "    print(f\"Trimming edges with weights below the {percentile}th percentile (threshold: {threshold:.4f})\")\n",
        "\n",
        "    trimmed_graph = nx.Graph()\n",
        "    for u, v, data in graph.edges(data=True):\n",
        "        if data['weight'] >= threshold:\n",
        "            trimmed_graph.add_edge(u, v, **data)\n",
        "\n",
        "    trimmed_graph.add_nodes_from(graph.nodes(data=True))\n",
        "    return trimmed_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTD9IbyxNv-b"
      },
      "outputs": [],
      "source": [
        "# Trim edges with lowest weights before plotting\n",
        "percentile = 10  # Adjust this value as needed (e.g., remove bottom 10% of edges)\n",
        "print(\"\\nTrimming low-weight edges...\")\n",
        "trimmed_company_graph = trim_edges(company_graph, percentile=percentile)\n",
        "print(f\"Number of nodes after trimming: {trimmed_company_graph.number_of_nodes()}\")\n",
        "print(f\"Number of edges after trimming: {trimmed_company_graph.number_of_edges()}\")\n",
        "\n",
        "# Calculate centralities on the trimmed graph\n",
        "print(\"\\nCalculating centrality measures...\")\n",
        "degree_centrality = nx.degree_centrality(trimmed_company_graph)\n",
        "betweenness_centrality = nx.betweenness_centrality(trimmed_company_graph)\n",
        "eigenvector_centrality = nx.eigenvector_centrality(trimmed_company_graph, max_iter=1000)\n",
        "\n",
        "# Combine centralities\n",
        "combined_centrality = {\n",
        "    node: (degree_centrality.get(node, 0) +\n",
        "           betweenness_centrality.get(node, 0) +\n",
        "           eigenvector_centrality.get(node, 0)) / 3\n",
        "    for node in trimmed_company_graph.nodes()\n",
        "}\n",
        "\n",
        "# Sort companies by combined centrality\n",
        "sorted_companies = sorted(combined_centrality.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Select top N companies for visualization\n",
        "N = 75  # Adjust as needed\n",
        "top_companies = [company for company, _ in sorted_companies[:N]]\n",
        "\n",
        "# Create a subgraph with only the top companies\n",
        "subgraph = trimmed_company_graph.subgraph(top_companies).copy()\n",
        "print(f\"\\nNumber of nodes in subgraph: {subgraph.number_of_nodes()}\")\n",
        "print(f\"Number of edges in subgraph: {subgraph.number_of_edges()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b99BpdCTJTvw"
      },
      "source": [
        "### Calculating Centrality Measures\n",
        "\n",
        "We'll use multiple centrality measures to identify important companies in the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q72Arkd1JTvw"
      },
      "outputs": [],
      "source": [
        "def calculate_centralities(graph):\n",
        "    return {\n",
        "        'degree': nx.degree_centrality(graph),\n",
        "        'betweenness': nx.betweenness_centrality(graph),\n",
        "        'eigenvector': nx.eigenvector_centrality(graph, max_iter=1000)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeJd7p0UJTvw"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Finally, we'll create a network visualization of the most central companies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5es6gAjJTvw"
      },
      "outputs": [],
      "source": [
        "# Prepare for visualization\n",
        "print(\"\\nPreparing visualization...\")\n",
        "pos = nx.spring_layout(subgraph, k=0.3, iterations=50)  # Adjust layout parameters as needed\n",
        "sizes = [combined_centrality[node] * 5000 for node in subgraph.nodes()]  # Scale node sizes\n",
        "labels = {node: node for node in subgraph.nodes()}\n",
        "\n",
        "# Visualize the network\n",
        "plt.figure(figsize=(15, 20))  # Adjust figure size as needed\n",
        "nx.draw_networkx_nodes(subgraph, pos, node_size=sizes, alpha=0.7, node_color='skyblue')\n",
        "nx.draw_networkx_edges(\n",
        "    subgraph, pos, alpha=0.2,\n",
        "    width=[data['weight']/10 for _, _, data in subgraph.edges(data=True)],\n",
        "    edge_color='gray'\n",
        ")\n",
        "nx.draw_networkx_labels(subgraph, pos, labels, font_size=10)\n",
        "\n",
        "plt.title(\"Company Relationships based on Shared Technology Types\", fontsize=20)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('company_network.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print out some results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7kpDXeHN-b7"
      },
      "outputs": [],
      "source": [
        "# Print top N companies and their centralities\n",
        "print(f\"\\nTop {N} Companies by Combined Centrality:\")\n",
        "for company, centrality in sorted_companies[:N]:\n",
        "    print(f\"{company}: {centrality:.4f}\")\n",
        "\n",
        "# Calculate and print technology type distribution for top companies\n",
        "tech_type_distribution = defaultdict(lambda: defaultdict(int))\n",
        "for company in top_companies:\n",
        "    for tech_type in B.neighbors(company):\n",
        "        tech_type_distribution[company][tech_type] += 1\n",
        "\n",
        "print(f\"\\nTechnology Type Distribution for Top {N} Companies:\")\n",
        "for company in top_companies:\n",
        "    print(f\"\\n{company}:\")\n",
        "    total = sum(tech_type_distribution[company].values())\n",
        "    sorted_tech = sorted(tech_type_distribution[company].items(), key=lambda x: x[1], reverse=True)\n",
        "    for tech_type, count in sorted_tech:\n",
        "        percentage = (count / total) * 100\n",
        "        print(f\"  {tech_type}: {percentage:.2f}%\")\n",
        "\n",
        "# Save centrality data\n",
        "centrality_data = {\n",
        "    'degree': degree_centrality,\n",
        "    'betweenness': betweenness_centrality,\n",
        "    'eigenvector': eigenvector_centrality,\n",
        "    'combined': combined_centrality\n",
        "}\n",
        "\n",
        "with open('company_centralities.json', 'w') as f:\n",
        "    json.dump(centrality_data, f, indent=2)\n",
        "\n",
        "print(\"\\nCentrality data saved to company_centralities.json\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
