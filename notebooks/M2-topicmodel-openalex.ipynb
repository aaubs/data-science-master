{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M2-topicmodel-openalex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Gensim and PyLDAvis\n",
        "!pip install -qq -U gensim\n",
        "!pip install -qq pyLDAvis\n",
        "!pip install -qq --upgrade numpy"
      ],
      "metadata": {
        "id": "a7ClxNmEW8-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm #progress bar\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "\n",
        "import spacy #spacy for quick language prepro\n",
        "nlp = spacy.load('en_core_web_sm') #instantiating English module\n",
        "\n",
        "# sampling, splitting\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# loading ML libraries\n",
        "from sklearn.pipeline import make_pipeline #pipeline creation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
        "from sklearn.linear_model import LogisticRegression #Logit model\n",
        "from sklearn.metrics import classification_report #that's self explanatory\n",
        "from sklearn.decomposition import TruncatedSVD #dimensionality reduction\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import altair as alt #viz\n",
        "\n",
        "# topic modeling\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary # Import the dictionary builder\n",
        "from gensim.models import LdaMulticore # we'll use the faster multicore version of LDA\n",
        "\n",
        "\n",
        "# Import pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "%matplotlib inline\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "FWtRTWgSBerP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "QXxHRq6zE4j4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3-rBov_TJuV"
      },
      "source": [
        "# Load remote file - dataframe of 1072 publications records on NLP research from Openalex\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/AI-Growth-Lab/SciNerTopic/main/data/nlp_openalex.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Data from OpenAlex\n",
        "\n",
        "#@markdown You can check out the list of concepts witht heir IDs [here](https://docs.google.com/spreadsheets/d/1LBFHjPt4rj_9r0t0TTAlT68NwOtNH8Z21lBMsJDMoZg/edit#gid=575855905), e.g., NLP c204321447 \n",
        "# specify endpoint\n",
        "endpoint = 'works'\n",
        "concept = \"'C548445021'\" #@param {type:\"string\"}\n",
        "oa = True #@param {type:\"boolean\"}\n",
        "nDocs = 996 #@param {type:\"slider\", min:200, max:3000, step:1}\n",
        "from_pub_date = \"2015-01-01\" #@param {type:\"date\"}\n",
        "#@markdown Enter your email for API call to OpenAlex. It is not stored but just used for the API call to OpenAlex.\n",
        "email = 'roman@business.aau.dk'#@param {type:\"string\"} \n",
        "\n",
        "\n",
        "def OA(oa):\n",
        "  if True:\n",
        "    return 'true'\n",
        "  else:\n",
        "    return 'false'\n",
        "\n",
        "\n",
        "\n",
        "oa_str = OA(oa)\n",
        "\n",
        "# build the 'filter' parameter\n",
        "filters = \",\".join((\n",
        "    f'concepts.id:{concept}',\n",
        "    'is_paratext:false', \n",
        "    f'from_publication_date:{from_pub_date}',\n",
        "    f'is_oa:{oa_str}'\n",
        "))\n",
        "\n",
        "# put the URL together\n",
        "filtered_works_url = f'https://api.openalex.org/{endpoint}?mailto={email}&filter={filters}'\n",
        "print(f'complete URL with filters:\\n{filtered_works_url}')\n",
        "\n",
        "\n",
        "paging_param = 'per-page=100&cursor=*'\n",
        "\n",
        "works_query = f'{filtered_works_url}&{paging_param}'\n",
        "\n",
        "response = requests.get(works_query)\n",
        "meta = json.loads(response.text)['meta']\n",
        "next_cursor = meta['next_cursor']\n",
        "results_alx = json.loads(response.text)['results']\n",
        "\n",
        "\n",
        "cycles = math.floor((meta['count'] - 100) / meta['per_page'])+1\n",
        "if cycles > 30:\n",
        "  cycles = int(nDocs/100)\n",
        "\n",
        "df_input = []\n",
        "\n",
        "for result in results_alx:\n",
        "  if result['abstract_inverted_index']:\n",
        "    abs = ' '.join(result['abstract_inverted_index'].keys())\n",
        "    df_input.append((result['id'], result['doi'],result['title'],result['publication_year'],abs))\n",
        "\n",
        "for cycle in range(cycles):\n",
        "  cycle_query = f'{works_query[:-1]}{next_cursor}'\n",
        "  response = requests.get(cycle_query)\n",
        "  meta = json.loads(response.text)['meta']\n",
        "  next_cursor = meta['next_cursor']\n",
        "  results_alx = json.loads(response.text)['results']\n",
        "  for result in results_alx:\n",
        "    if result['abstract_inverted_index']:\n",
        "      abs = ' '.join(result['abstract_inverted_index'].keys())\n",
        "      df_input.append((result['id'], result['doi'],result['title'],result['publication_year'],abs))\n",
        "\n",
        "\n",
        "data = pd.DataFrame(df_input, columns=['id','doi','title','publication_year','abstract'])\n",
        "\n",
        "print(f'Downloaded {str(len(data))} documents')\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Iip8qxOtCHnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "CUkRDA7gELnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "zPS_VOisiXs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rP5da69_nv1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data ['text'] = data['title'] + '. ' + data['abstract']"
      ],
      "metadata": {
        "id": "3aB3k2CWi8Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijwe4ks92cTn"
      },
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm.tqdm(total=len(data['text']),position=0, leave=True)\n",
        "\n",
        "for text in nlp.pipe(data['text'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text \n",
        "         if token.is_alpha \n",
        "         and not token.is_stop \n",
        "         and not token.is_punct]\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdAqGWwL12Y0"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_clean'] = clean_text"
      ],
      "metadata": {
        "id": "fHpD7QnSFiTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts (we need tokens)\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(data['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "q1IjMnsdlQg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tokens'] = tokens"
      ],
      "metadata": {
        "id": "QIFmN_6tlSt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(data['tokens'])\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data['tokens']]"
      ],
      "metadata": {
        "id": "uxBNVSFwgrKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=5, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "WR-I1RThhBSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to visualize\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "81KTmWw7hCqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Let's Visualize\n",
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "dfGSNcH5hl_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel"
      ],
      "metadata": {
        "id": "IgpHVDk8He7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = CoherenceModel(model=lda_model, texts = data['tokens'], coherence='c_v')\n",
        "coherence = cm.get_coherence()  # get coherence value"
      ],
      "metadata": {
        "id": "pBzQmhs_WNSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coherence"
      ],
      "metadata": {
        "id": "cDRyAIzMHg06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oy26H0GgZKkz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}