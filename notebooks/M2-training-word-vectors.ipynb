{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVNr+GlyomFSdbdRGIMnBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M2-training-word-vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training customized word embeddings\n",
        "\n",
        "Word embeddings became big around 2013 and are linked to [this paper](https://arxiv.org/abs/1301.3781) with the beautiful title \n",
        "*Efficient Estimation of Word Representations in Vector Space* by Tomas Mokolov et al. coming out of Google. This was the foundation of Word2Vec.\n",
        "\n",
        "The idea behind it is easiest summarized by the following quote: \n",
        "\n",
        "\n",
        "> *You shall know a word by the company it keeps (Firth, J. R. 1957:11)*\n",
        "\n",
        "![](https://ruder.io/content/images/size/w2000/2016/04/word_embeddings_colah.png)\n",
        "\n",
        "Let me start with a fascinating example of word embeddings in practice. Below, you can see a figure from the paper: \n",
        "*Dynamic Word Embeddings for Evolving Semantic Discovery*. Here (in simple terms) the researchers estimated word vectors for from textual inputs in different time-frames. They picked out some terms and person that obviously changed *their company* over the years. Then they look at the relative position of these terms compared to terms that did not change much (anchors). If you are interested in this kind of research, check out [this blog](https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/) that describes the paper briefly or the [original paper](https://arxiv.org/abs/1703.00607).\n",
        "\n",
        "![alt text](https://adriancolyer.files.wordpress.com/2018/02/evolving-word-embeddings-fig-1.jpeg)\n",
        "\n",
        "Word embeddings allow us to create term representations that \"learn\" meaning from semantic and syntactic features. These models take a sequence of sentences as an input and scan for all individual terms that appear in the whole corpus and all their occurrences. Such contextual learning seems to be able to pick up non-trivial conceptual details and it is this class of models that today enable technologies such as chatbots, machine translation and much more.\n",
        "\n",
        "The early word embedding models were Word2Vec and [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
        "In December 2017 Facebook presented [fastText](https://fasttext.cc/) (by the way - by 2017 Tomas Mikolov was working for Facebook and is one of the authors of the [paper](https://arxiv.org/abs/1607.04606) that introduces the research behind fastText). This model extends the idea of Word2Vec, enriching these vectors by information from sub-word elements. What does that mean? Words are not only defined by surrounding words but in addition also by the various syllables that make up the word. Why should that be a good idea? Well, now words such as *apple* and *apples* do not only get similar vectors due to them often sharing context but also because they are composed of the same sub-word elements. This comes in particularly handy when we are dealing with language that have a rich morphology such as Turkish or Russian.  This is also great when working with web-text, which is often messy and misspelt.\n",
        "\n",
        "The current state-of-the-art transformer models go even further and implement context-specificity (a word may change meaning depending on the context in which it occurs)\n",
        "\n",
        "Now the good news: You will find pre-trained vectors from all mentioned models online. They will do great in most cases. However, when working with specific tasks: Some obscure languages and/or specific technical jargon (specific scientific field or industry e.g. finance, insurance), it is nice to know how to train such word-vectors.\n",
        "\n",
        "\n",
        "In this tutorial we will train the \"classic\" Word2Vec model, considering bi-grams. We will also look a bit into data-engineering issues in sequence-training. Finally, we will look at how we can use such models for text representation beyond individual words.\n",
        "\n",
        "## Data\n",
        "\n",
        "The data used here are 10k cooking related posts from Reddit. They come in JSON-lines format and can be either downloaded first or opened via requests.\n",
        "\n",
        "## Plan of attack\n",
        "In this tutorial we will not be using Spacy, as it is not fast enough for use in training of large language models.\n",
        "The intent is to understand training from disk - where the file is not opened (with e.g. pandas) and an object in memory but streamed from disk."
      ],
      "metadata": {
        "id": "VLbsQbzzYlwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6qffJDvqFVq"
      },
      "outputs": [],
      "source": [
        "# download data (optional when training from memory)\n",
        "!wget https://raw.githubusercontent.com/aaubs/ds-master/main/data/reddit_r_cooking_sample.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installs\n",
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "k_iGYh4etyYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# we will use nltk for sentence tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# we will be using gensim for training\n",
        "import gensim\n",
        "from gensim import utils\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
        "\n",
        "\n",
        "# Logging settings\n",
        "import logging\n",
        "\n",
        "for handler in logging.root.handlers[:]:\n",
        "   logging.root.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "metadata": {
        "id": "maHvkJm-qQqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple In-memory training\n",
        "\n",
        "To better understand the training itself we start with simple model training out of memory. All the data will be loaded with pandas.\n",
        "Preprocessing results will also be stored in the dataframe. This is a viable approache up a certain data-size. When going beyond 5M texts (depending on the hardware) that's probably not a good idea.."
      ],
      "metadata": {
        "id": "p-vGObWWc8ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "data = pd.read_json('https://raw.githubusercontent.com/aaubs/ds-master/main/data/reddit_r_cooking_sample.jsonl', lines=True)"
      ],
      "metadata": {
        "id": "dGaq3bGZqUJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "_BQE_vtjqX-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec uses sentences to train, not paragraphs. Therefore we will need to sentence-tokenize."
      ],
      "metadata": {
        "id": "CDlgZNWsdq0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK tokenizer:\n",
        "sent_tokenize('this is a sentence. also that one.')"
      ],
      "metadata": {
        "id": "WZ3F19r5z3H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's apply that to all texts\n",
        "sentences = []\n",
        "for i in data['text']:\n",
        "  sentences.extend(sent_tokenize(i))"
      ],
      "metadata": {
        "id": "2ZRvFTU2zp4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "id": "iZCwa5lH0Mq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim has efficient simple preprocessing as part of the utility functions. That works well for most latin-letter texts. Check out [Gensim docos](https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html) for more into."
      ],
      "metadata": {
        "id": "ll1MhWVfeCG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple prepro (tokenization, lowercase, de-accent (otional))\n",
        "sentences_prepro = [utils.simple_preprocess(line) for line in sentences]"
      ],
      "metadata": {
        "id": "hPSpWs_quIO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are not removing stopwords for Word2Vec, as the model actually cares about syntax. One thing that we can do is identifying n-grams (phrases)."
      ],
      "metadata": {
        "id": "sJpPII06ehW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainig a model to identify n-grams\n",
        "phrase_model = Phrases(sentences_prepro, min_count=1, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)"
      ],
      "metadata": {
        "id": "zII18gcct_d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the model\n",
        "sentences_phrased = [phrase_model[line] for line in sentences_prepro]"
      ],
      "metadata": {
        "id": "osOmQ11HuUtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick check\n",
        "sentences_phrased[:5]"
      ],
      "metadata": {
        "id": "M_AxSeeQum2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "obviousely, some hyperparameter tuning is needed"
      ],
      "metadata": {
        "id": "_A9-WXqCe5AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusting min_count and threshold (that's a value calculated within the model - read docus)\n",
        "phrase_model = Phrases(sentences_prepro, min_count=25, threshold=20, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
        "sentences_phrased = [phrase_model[line] for line in sentences_prepro]\n",
        "sentences_phrased[:5]"
      ],
      "metadata": {
        "id": "TqStLJB5urRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# did we actually find anything?\n",
        "for phrase, score in phrase_model.find_phrases(sentences_prepro).items():\n",
        "    print(phrase, score)"
      ],
      "metadata": {
        "id": "N0JdwoIpve-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once sentences are pre-processed (tokenized, list of lists) we can train the model."
      ],
      "metadata": {
        "id": "NRkgZneFfM2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(sentences=sentences_phrased, \n",
        "                               vector_size=300, \n",
        "                               window=5, \n",
        "                               min_count=5, \n",
        "                               workers=4, \n",
        "                               epochs=15)"
      ],
      "metadata": {
        "id": "qWLP480uw2bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check most similar terms\n",
        "model.wv.most_similar('dutch_oven')"
      ],
      "metadata": {
        "id": "Hwirw6wKxSrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can call the vector of each word\n",
        "model.wv['kettle']"
      ],
      "metadata": {
        "id": "ox6_ykEZk3Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.vectors.shape"
      ],
      "metadata": {
        "id": "-a6llx8p2CkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from here you can ennter key-word dicts for mapping\n",
        "model.wv.key_to_index"
      ],
      "metadata": {
        "id": "fRwJ5SGQ2gNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Word2Vec from disk\n",
        "\n",
        "Let's assume you want to train a word-embeddding model from disk. You downloaded all of Wikipedia or one of the large (multi GB datasets from Huggingface)"
      ],
      "metadata": {
        "id": "FsCIdl8xlBS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open file (not read yet) from disk\n",
        "texts_reddit = open('/content/reddit_r_cooking_sample.jsonl','r')"
      ],
      "metadata": {
        "id": "3gXWAQ-I3HN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read single line (this will iterate over the lines)\n",
        "texts_reddit.readline()"
      ],
      "metadata": {
        "id": "2MdG_3wLl_eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode JSON\n",
        "json.loads(texts_reddit.readline())"
      ],
      "metadata": {
        "id": "AnFNpnG1mC9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to turn our comments into sentences (tokenize) and preprocess. No need to do on-the-fly preprocessing 15 times\n",
        "For that we create a new file `sentences.txt`, we tokenize our texts and write all sentences as lines into the new file. Using 1-sentence-per-line in TXTs is a common approach."
      ],
      "metadata": {
        "id": "t9etkw1XlcL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need re-open to start from top\n",
        "texts_reddit = open('/content/reddit_r_cooking_sample.jsonl','r')"
      ],
      "metadata": {
        "id": "s2NwEkMAmgGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open file\n",
        "with open('sentances.txt','w') as f:\n",
        "  for line in texts_reddit: # iterate over the json-lines with comments (alternative to readline())\n",
        "    line = json.loads(line) # decode json\n",
        "    for sent in sent_tokenize(line['text']): # sent-tokenize\n",
        "      f.write(sent) # write sents into the new file\n",
        "      f.write('\\n')\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "cCiWGQUH4SwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is not easy but important and your first step to writing \"real code\".\n",
        "We need to define something that allows us to retrieve our sentences from the stored file one by one (and start from the beginning after the last one).\n",
        "\n",
        "A class with an `__iter__` function can help here. This becomes an iterator that yields them one by one. `yield` is different from `return`. The latter ends an execution and returns the \"overall\" result of a function. `yield` is called repeatedly."
      ],
      "metadata": {
        "id": "nkKp51UjmmSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/sentances.txt\""
      ],
      "metadata": {
        "id": "10YTGlGW8unx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "    def __iter__(self):\n",
        "        for line in open(path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ],
      "metadata": {
        "id": "RxpYOJwS5t3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try out how that works"
      ],
      "metadata": {
        "id": "TKymLmULoHuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a corpus object\n",
        "sentences_disk = MyCorpus()"
      ],
      "metadata": {
        "id": "XXFJhLbz6tXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a generator (similar to list comprehension but on \"stand-by\")\n",
        "test_gen = (a for a in sentences_disk)"
      ],
      "metadata": {
        "id": "ODb8_s_Rnzmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# every time we call next, it runs one iteration\n",
        "next(test_gen)"
      ],
      "metadata": {
        "id": "CocqDbqrn_YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train our Phrases model from the disk-corpus"
      ],
      "metadata": {
        "id": "VTEy_juLoexu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_disk = MyCorpus()"
      ],
      "metadata": {
        "id": "9k1uK2uWoc4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_model = Phrases(sentences_disk, min_count=25, threshold=20, connector_words=ENGLISH_CONNECTOR_WORDS)"
      ],
      "metadata": {
        "id": "kt2v8Gm17j13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for phrase, score in phrase_model.find_phrases(sentences_disk).items():\n",
        "    print(phrase, score)"
      ],
      "metadata": {
        "id": "gNor6m0Z7upC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚀🚀🚀\n",
        "**Efficiency** is key when working from disk.\n",
        "Let's preprocess the inputs using simple-prepro and the phrases model.\n",
        "Since we preprocess our sentences into lists we need to store them using json such that we can load them into python objects, not strings"
      ],
      "metadata": {
        "id": "LBMdBRFOu7ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_disk = MyCorpus()"
      ],
      "metadata": {
        "id": "18xzuBc2vSQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open new file (txt file with json-input)\n",
        "with open('sentances_phrases.txt','w') as f:\n",
        "  for sent in sentences_disk: # iterate over the json-lines with comments (alternative to readline())\n",
        "    f.write(json.dumps(phrase_model[sent])) # write sents into the new file\n",
        "    f.write('\\n')\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "7_pN3cEjvMBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/sentances_phrases.txt'"
      ],
      "metadata": {
        "id": "hnsk-h8Lxc5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCorpus_processed:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "    def __iter__(self):\n",
        "        for line in open(path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield json.loads(line)"
      ],
      "metadata": {
        "id": "Lf4mls_KwirG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_disk = MyCorpus_processed()"
      ],
      "metadata": {
        "id": "GNIcwrU0x3KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or we just add it to the training\n",
        "model = gensim.models.Word2Vec(sentences=sentences_disk, \n",
        "                               vector_size=300, \n",
        "                               window=5, \n",
        "                               min_count=5, \n",
        "                               workers=4, \n",
        "                               epochs=15)"
      ],
      "metadata": {
        "id": "eqH5Ap63_RKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('coriander')"
      ],
      "metadata": {
        "id": "wkIQ0f-HBQ29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus: Training FastText\n",
        "\n",
        "training of FastText is syntax-wise the same.\n",
        "There are a few other paras that you can tune"
      ],
      "metadata": {
        "id": "_jDW5gJgt7wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fasttext = FastText(sentences = sentences_disk, \n",
        "                          vector_size=300, \n",
        "                          window=8, \n",
        "                          min_count=5, \n",
        "                          workers=4, \n",
        "                          epochs=15)"
      ],
      "metadata": {
        "id": "v4CzDHe3APOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fasttext.wv.most_similar('coriander')"
      ],
      "metadata": {
        "id": "s8hXrlwZAoyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv['powder']"
      ],
      "metadata": {
        "id": "IZtg7Ep3BMm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Word-Vectors\n",
        "\n",
        "now that we have our Word-vectors we should be able to reduce their dimensionality to explore visually"
      ],
      "metadata": {
        "id": "jPPdBczQywEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn -q"
      ],
      "metadata": {
        "id": "vp5ja48Oy26Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import umap\n",
        "import altair as alt"
      ],
      "metadata": {
        "id": "QQrstZdjy4rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# picking 2000 random vectors from the W2V model\n",
        "idx = random.sample(range(len(model.wv.vectors)), 2000)"
      ],
      "metadata": {
        "id": "vvkLXXK2MMtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating 2D reduction\n",
        "umap_reducer = umap.UMAP(random_state=42, n_components=2)\n",
        "embeddings = umap_reducer.fit_transform(model.wv.vectors[idx])"
      ],
      "metadata": {
        "id": "8Xhd-NA7Kwrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df for plot\n",
        "df_plot = pd.DataFrame(embeddings, columns=['x','y'])"
      ],
      "metadata": {
        "id": "cmrRGZ3ZMsgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vector-labels\n",
        "labels = [model.wv.index_to_key[ix] for ix in idx]"
      ],
      "metadata": {
        "id": "QAboNRJD0_b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_plot['labels'] = labels"
      ],
      "metadata": {
        "id": "bWGbN-FyMvPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot\n",
        "alt.Chart(df_plot).mark_circle(size=60).encode(\n",
        "    x='x',\n",
        "    y='y',\n",
        "    tooltip=['labels']\n",
        ").properties(\n",
        "    width=800,\n",
        "    height=600\n",
        ").interactive()"
      ],
      "metadata": {
        "id": "miYjcUAhM_tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create sentence embeddings from our W2V model\n",
        "\n",
        "The final aim is to use the custom W2V embeddings to vectorize sentences\n",
        "We will look at average vectors and tfidf weighted avg. embeddings"
      ],
      "metadata": {
        "id": "l5PqPdQ-1ZGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sents = ['I love chicken super much with soy',\n",
        "              'I enjoy asian food, especially chicken',\n",
        "              'Give me cake', 'mexican food is amazing', \n",
        "              'I enjoy cuisine italian']"
      ],
      "metadata": {
        "id": "NnMrUI9-DV-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average W2V vectors"
      ],
      "metadata": {
        "id": "6rk45Tw82I7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "tokens = phrase_model[utils.simple_preprocess(test_sents[0])]"
      ],
      "metadata": {
        "id": "iH7vW_CJEhCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out only those words that are part of the vocab\n",
        "tokens = [t for t in tokens if t in model.wv.key_to_index.keys()]"
      ],
      "metadata": {
        "id": "49ZKmF_1EwnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create average-vectors\n",
        "avg_vec = np.average([model.wv[t] for t in tokens], axis=0)"
      ],
      "metadata": {
        "id": "OjQy6-_8ExU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's package this process up into a vectorizer-function"
      ],
      "metadata": {
        "id": "eIr84l6s15lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def w2v_vectorize(text):\n",
        "  tokens = phrase_model[utils.simple_preprocess(text)] # preprocess just as model inputs\n",
        "  tokens = [t for t in tokens if t in model.wv.key_to_index.keys()] # filter only tokens that are in vocab\n",
        "  return np.average([model.wv[t] for t in tokens], axis=0) # calculate avg vector"
      ],
      "metadata": {
        "id": "AgqwnSESFr__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it's a goof idea to stack them using numpy into a matrix\n",
        "vecs = np.vstack([w2v_vectorize(s) for s in test_sents])"
      ],
      "metadata": {
        "id": "5oBT2PDuGba4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick explaininng of the vectors (not really part of the code)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(vecs)"
      ],
      "metadata": {
        "id": "vZrrwfXWGOl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFIDF weighted W2V Embeddings\n",
        "\n",
        "Very similar to avg-embeddings, however here we will use sklearn TfidfVectorizer (that one we already know) to weight our vecs\n",
        "The approach is a bit \"hacky\" but efficient"
      ],
      "metadata": {
        "id": "_ijrfvUc2Ggg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "CqPGDRipIirt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that does absolutely nothing...\n",
        "# cause we do prepro and tokenization in one using gensim, we will define it for prepro\n",
        "def dummy_fun(doc):\n",
        "    return doc"
      ],
      "metadata": {
        "id": "AJhijzyt_3D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[phrase_model[utils.simple_preprocess(text)] for text in test_sents]"
      ],
      "metadata": {
        "id": "WTWxNqKdKAzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we define a preprocessing function to pass into the TfidfVectorizer\n",
        "def gensim_prepro(doc):\n",
        "  return phrase_model[utils.simple_preprocess(doc)]"
      ],
      "metadata": {
        "id": "BRhTvA39Jkzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we turn of any preprocessing and align vocabulary with the one\n",
        "# used by our embeddings\n",
        "# that will allow us to use TFIDF vectors to weight the embeddings\n",
        "\n",
        "tfidf_new_text = TfidfVectorizer(\n",
        "    vocabulary=model.wv.key_to_index.keys(), # here using the W2V vocab\n",
        "    tokenizer=dummy_fun,\n",
        "    preprocessor=gensim_prepro,\n",
        "    token_pattern=None)  "
      ],
      "metadata": {
        "id": "4Nr9PLLr_5ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create TFIDF matrix (we could also just use that one for search)\n",
        "new_tfidf = tfidf_new_text.fit_transform(test_sents)"
      ],
      "metadata": {
        "id": "klAEuO17KK36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_tfidf"
      ],
      "metadata": {
        "id": "isLYA0oAKNuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This here is a cool little trick: Since N-columns for the TFIDF is the same as n-rows for our word-embeddings we can simply take a dot-product here.\n",
        "Another cool feature: this can be done sequentially for large datasets (when no space in ram)"
      ],
      "metadata": {
        "id": "h4VM3Bwh24qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating TFIDF-weighted avg. embeddings\n",
        "test_w2v_tfidf = new_tfidf @ model.wv.vectors"
      ],
      "metadata": {
        "id": "6mcA211_KfbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(test_w2v_tfidf)"
      ],
      "metadata": {
        "id": "1a1Iz7K2KsIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using these embeddings for semantic search\n",
        "We can use such embeddings (and others) for semantic search (similarity maximization) and also downstream in unsuprvised/supervised tasks."
      ],
      "metadata": {
        "id": "55WP1y523Uzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create TFIDF matrix for all\n",
        "tfidf_all = tfidf_new_text.fit_transform(data['text'])"
      ],
      "metadata": {
        "id": "qnH5ZGW5Rdtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get vecs by dot-product\n",
        "tfidf_w2v_all = tfidf_all @ model.wv.vectors"
      ],
      "metadata": {
        "id": "fAlFvbJ6RsKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make query and transform it into same vector-space\n",
        "\n",
        "query = 'Steak egg'\n",
        "\n",
        "tfidf_q = tfidf_new_text.transform([query]) \n",
        "tfidf_w2v_q = tfidf_q @ model.wv.vectors"
      ],
      "metadata": {
        "id": "FpiMq9GQRw0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate cos-sim between the query and all vecs\n",
        "\n",
        "distances = cosine_similarity(tfidf_w2v_q,tfidf_w2v_all)"
      ],
      "metadata": {
        "id": "I9cwoccNUyz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get corresponding texts\n",
        "ids = np.flip(np.argsort(distances))[0]\n",
        "ids"
      ],
      "metadata": {
        "id": "P4Pw36GKUuUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print\n",
        "for ix in ids[:10]:\n",
        "  print(data['text'].values[ix])"
      ],
      "metadata": {
        "id": "w0zuI_BQUuKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Serialization\n",
        "\n",
        "Gensim models can be (ans should be) saved to disk after training."
      ],
      "metadata": {
        "id": "4dNpccay4Dzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_model.save('bigram_model.m')"
      ],
      "metadata": {
        "id": "24HXP-SDQgP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('w2v_food.m')"
      ],
      "metadata": {
        "id": "h8aZ260jGoK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = Word2Vec.load('/content/w2v_food.m')"
      ],
      "metadata": {
        "id": "7mVoMzqGHhIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g.wv.most_similar('garlic')"
      ],
      "metadata": {
        "id": "oPJDNFiMHthR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}